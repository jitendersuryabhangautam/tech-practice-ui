{
  "id": "statefulsets",
  "title": "StatefulSets",
  "category": "Kubernetes Workloads",
  "description": "Managing stateful applications with stable identities, ordered deployment, and persistent storage.",
  "explanation": "A StatefulSet manages stateful applications that require stable network identities, persistent storage, and ordered deployment/scaling.\n\nKey guarantees:\n- Stable, unique pod names: <statefulset-name>-<ordinal> (web-0, web-1, web-2). Names persist across rescheduling.\n- Stable DNS: Each pod gets a DNS record: <pod-name>.<headless-service>.<namespace>.svc.cluster.local.\n- Ordered deployment: Pods are created sequentially (0, 1, 2...). Pod N+1 waits until Pod N is Running and Ready.\n- Ordered deletion: Pods are terminated in reverse order (2, 1, 0) during scale-down.\n- Ordered rolling updates: Pods are updated in reverse ordinal order.\n\nComponents:\n- Headless Service (clusterIP: None): Required for pod DNS records. Does not load-balance — clients connect to specific pods.\n- volumeClaimTemplates: Each pod gets its own PVC (web-0-data, web-1-data). PVCs survive pod deletion and rescheduling.\n\nUse cases:\n- Databases: MySQL, PostgreSQL (primary-replica with stable IDs).\n- Distributed systems: Kafka, ZooKeeper, etcd (need stable network identity for cluster membership).\n- Caches: Redis Cluster (needs consistent hashing with stable endpoints).\n\nPod Management Policies:\n- OrderedReady (default): Sequential create/delete, wait for Ready between pods.\n- Parallel: Create/delete all pods simultaneously. Use when ordering doesn't matter but stable identity does.\n\nUpdate Strategies:\n- RollingUpdate (default): Updates in reverse ordinal order. partition field allows canary updates (only pods >= partition ordinal are updated).\n- OnDelete: Pods only updated when manually deleted.\n\nStatefulSet vs Deployment:\n- Deployment: Stateless, random pod names, shared PVC, parallel scaling.\n- StatefulSet: Stateful, predictable pod names, per-pod PVCs, ordered operations.\n- Use Deployment unless you specifically need stable identity or per-pod storage.",
  "code": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  serviceName: web-headless\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.25\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: data\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      resources:\n        requests:\n          storage: 1Gi",
  "command": "# Create StatefulSet\nkubectl apply -f statefulset.yaml\n\n# Watch ordered pod creation\nkubectl get pods -w -l app=web\n\n# Check pod DNS\nkubectl run -it dns-test --image=busybox --rm -- nslookup web-0.web-headless\n\n# Scale StatefulSet\nkubectl scale statefulset web --replicas=5\n\n# Check PVCs (one per pod)\nkubectl get pvc\n\n# Partition update (canary)\nkubectl patch statefulset web -p '{\"spec\":{\"updateStrategy\":{\"rollingUpdate\":{\"partition\":2}}}}'",
  "example": "# Headless Service for StatefulSet\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-headless\nspec:\n  clusterIP: None\n  selector:\n    app: web\n  ports:\n  - port: 80\n---\n# MySQL StatefulSet with init container for replication setup\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  serviceName: mysql-headless\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: init-mysql\n        image: mysql:8.0\n        command: ['bash', '-c', 'echo server-id=$(($(hostname | grep -o \"[0-9]*$\") + 1)) > /mnt/conf.d/server-id.cnf']\n        volumeMounts:\n        - name: conf\n          mountPath: /mnt/conf.d\n      containers:\n      - name: mysql\n        image: mysql:8.0\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql-secret\n              key: password\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/mysql\n        - name: conf\n          mountPath: /etc/mysql/conf.d\n      volumes:\n      - name: conf\n        emptyDir: {}\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      resources:\n        requests:\n          storage: 10Gi",
  "useCase": "Databases, distributed systems (Kafka, ZooKeeper), caches (Redis Cluster), any app needing stable identity or per-pod storage",
  "interviewQuestions": [
    {
      "question": "What is a StatefulSet and when would you use one?",
      "answer": "StatefulSet manages stateful applications needing stable pod names, persistent per-pod storage, and ordered deployment. Use for databases (MySQL, PostgreSQL), distributed systems (Kafka, etcd, ZooKeeper), and any workload requiring stable network identity. Pods get predictable names (web-0, web-1) and dedicated PVCs."
    },
    {
      "question": "How does a StatefulSet differ from a Deployment?",
      "answer": "Deployment: random pod names, shared PVC, parallel scaling, for stateless apps. StatefulSet: predictable ordinal names (app-0, app-1), per-pod PVC via volumeClaimTemplates, ordered create/delete, requires headless Service. Use Deployment unless you need stable identity or individual storage."
    },
    {
      "question": "Why does a StatefulSet require a Headless Service?",
      "answer": "Headless Service (clusterIP: None) creates individual DNS records for each pod: <pod>.<service>.<namespace>.svc.cluster.local. This enables stable network identity — other pods can connect to specific StatefulSet members by DNS name (mysql-0.mysql-headless) even after rescheduling."
    },
    {
      "question": "What happens to PVCs when a StatefulSet pod is deleted?",
      "answer": "PVCs are NOT deleted when pods are deleted or StatefulSet is scaled down. This preserves data. When scaled back up, the same PVC is reattached to the pod with the same ordinal. Must manually delete PVCs to reclaim storage. persistentVolumeClaimRetentionPolicy (1.27+) can automate this."
    },
    {
      "question": "Explain the partition feature in StatefulSet rolling updates.",
      "answer": "partition in updateStrategy.rollingUpdate specifies an ordinal: only pods with ordinal >= partition are updated. Example: partition=2 with 5 replicas → only pods 2,3,4 get updated; pods 0,1 keep old version. Enables canary deployments. Set partition=0 to complete the rollout."
    },
    {
      "question": "How do you handle StatefulSet scaling in production?",
      "answer": "Scale up: new pods created in order, each gets new PVC. Scale down: pods removed in reverse order (highest ordinal first), PVCs retained. Considerations: check application cluster membership before scaling down (e.g., Kafka partition reassignment), ensure quorum is maintained."
    },
    {
      "question": "What are pod management policies in StatefulSet?",
      "answer": "OrderedReady (default): Pods created/deleted sequentially, waits for Ready between each. Parallel: All pods created/deleted simultaneously. Use Parallel when ordering doesn't matter but stable identity does (e.g., cache cluster where all nodes are equal)."
    },
    {
      "question": "How do you perform a rolling restart of a StatefulSet?",
      "answer": "kubectl rollout restart statefulset <name>. Pods restart in reverse ordinal order (highest first). Unlike Deployment, no surge — each pod terminates before next starts. Use kubectl rollout status statefulset <name> to monitor. Can also use OnDelete strategy for manual control."
    },
    {
      "question": "What are the challenges of running databases on Kubernetes with StatefulSets?",
      "answer": "Challenges: Storage performance (cloud PVs may be slow), backup/restore complexity, replication setup (init containers or operators), failover handling, connection management. Solutions: Use database operators (MySQL Operator, PostgreSQL Operator) that automate these concerns."
    },
    {
      "question": "How does DNS work for StatefulSet pods?",
      "answer": "Each pod gets DNS: <pod-name>.<headless-service>.<namespace>.svc.cluster.local. Example: mysql-0.mysql-headless.default.svc.cluster.local. SRV records also created for port discovery. DNS updates when pod is rescheduled to new node. Application can use short name within same namespace: mysql-0.mysql-headless."
    }
  ],
  "exercises": [
    {
      "type": "write",
      "question": "Write a StatefulSet manifest for a 3-replica Redis cluster with 5Gi persistent storage per pod.",
      "answer": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis\nspec:\n  serviceName: redis-headless\n  replicas: 3\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis:7\n        ports:\n        - containerPort: 6379\n        volumeMounts:\n        - name: data\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      resources:\n        requests:\n          storage: 5Gi"
    },
    {
      "type": "command",
      "question": "Scale a StatefulSet named 'db' from 3 to 5 replicas and verify ordered creation.",
      "answer": "kubectl scale statefulset db --replicas=5\nkubectl get pods -w -l app=db  # Watch db-3 then db-4 created in order"
    },
    {
      "type": "explain",
      "question": "Why are StatefulSet PVCs not deleted on scale-down?",
      "answer": "Data safety: PVCs contain application state (database data). Accidental scale-down shouldn't cause data loss. When scaled back up, the same PVC reattaches to the pod with the same ordinal. Must manually delete PVCs (kubectl delete pvc data-web-2) to reclaim storage."
    },
    {
      "type": "troubleshoot",
      "question": "StatefulSet pod web-1 is stuck in Pending. web-0 is Running. What to check?",
      "answer": "Check: (1) kubectl describe pod web-1 for events, (2) PVC status — StorageClass may have no available PVs, (3) Node resources (CPU/memory insufficient), (4) Node affinity/tolerations, (5) kubectl get pvc to see if PVC is Bound or Pending."
    },
    {
      "type": "scenario",
      "question": "You need to update the container image for a 5-pod StatefulSet but test on one pod first.",
      "answer": "Use partition update: kubectl patch statefulset web -p '{\"spec\":{\"updateStrategy\":{\"rollingUpdate\":{\"partition\":4}}}}' — only web-4 gets updated. Test web-4. If OK, lower partition to 0: kubectl patch statefulset web -p '{\"spec\":{\"updateStrategy\":{\"rollingUpdate\":{\"partition\":0}}}}' to update all."
    },
    {
      "type": "write",
      "question": "Write a Headless Service for a StatefulSet with app=postgres selector.",
      "answer": "apiVersion: v1\nkind: Service\nmetadata:\n  name: postgres-headless\nspec:\n  clusterIP: None\n  selector:\n    app: postgres\n  ports:\n  - port: 5432\n    targetPort: 5432"
    },
    {
      "type": "command",
      "question": "Check the DNS record of a specific StatefulSet pod.",
      "answer": "kubectl run -it dns-test --image=busybox:1.36 --rm --restart=Never -- nslookup web-0.web-headless.default.svc.cluster.local"
    },
    {
      "type": "scenario",
      "question": "A StatefulSet pod is deleted but its PVC still exists. What happens when the pod is recreated?",
      "answer": "The controller recreates the pod with the same ordinal (e.g., web-2). The existing PVC (data-web-2) is automatically reattached. Data from the previous pod is preserved. This is the core value of StatefulSets — data survives pod restarts and rescheduling."
    },
    {
      "type": "explain",
      "question": "When should you use Parallel pod management policy?",
      "answer": "Use Parallel when pods don't depend on each other during startup but still need stable identity and per-pod storage. Example: cache nodes that auto-discover peers, stateless workers with persistent scratch space. Faster scaling since all pods start simultaneously."
    },
    {
      "type": "command",
      "question": "Perform a rolling restart of a StatefulSet and watch the progress.",
      "answer": "kubectl rollout restart statefulset web\nkubectl rollout status statefulset web\nkubectl get pods -w -l app=web  # Watch pods restart in reverse ordinal order"
    }
  ],
  "programExercises": [
    {
      "type": "program",
      "question": "Program 1: Create a headless service and StatefulSet",
      "code": "cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-headless\nspec:\n  clusterIP: None\n  selector:\n    app: web\n  ports:\n  - port: 80\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  serviceName: web-headless\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n  template:\n    metadata:\n      labels:\n        app: web\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.25-alpine\n        ports:\n        - containerPort: 80\nEOF\nkubectl get pods -l app=web",
      "output": "web-0   1/1   Running   web-1   1/1   Running   web-2   1/1   Running"
    },
    {
      "type": "program",
      "question": "Program 2: Verify ordered pod creation",
      "code": "kubectl delete statefulset web\nkubectl apply -f statefulset.yaml\nkubectl get pods -w -l app=web --output-watch-events",
      "output": "Events show web-0 created first, then web-1 after web-0 is Ready, then web-2"
    },
    {
      "type": "program",
      "question": "Program 3: Verify stable DNS for each pod",
      "code": "kubectl run -it dns-check --image=busybox:1.36 --rm --restart=Never -- sh -c '\n  nslookup web-0.web-headless\n  nslookup web-1.web-headless\n  nslookup web-2.web-headless\n'",
      "output": "Each pod resolves to its individual IP address via headless service DNS"
    },
    {
      "type": "program",
      "question": "Program 4: StatefulSet with volumeClaimTemplates",
      "code": "cat <<EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: db\nspec:\n  serviceName: db-headless\n  replicas: 2\n  selector:\n    matchLabels:\n      app: db\n  template:\n    metadata:\n      labels:\n        app: db\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:15-alpine\n        env:\n        - name: POSTGRES_PASSWORD\n          value: secret\n        volumeMounts:\n        - name: pgdata\n          mountPath: /var/lib/postgresql/data\n  volumeClaimTemplates:\n  - metadata:\n      name: pgdata\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      resources:\n        requests:\n          storage: 2Gi\nEOF\nkubectl get pvc",
      "output": "pgdata-db-0 Bound   pgdata-db-1 Bound (each pod gets its own PVC)"
    },
    {
      "type": "program",
      "question": "Program 5: Canary update with partition",
      "code": "kubectl patch statefulset web -p '{\"spec\":{\"updateStrategy\":{\"rollingUpdate\":{\"partition\":2}}}}'\nkubectl set image statefulset web nginx=nginx:1.26-alpine\nkubectl get pods -l app=web -o custom-columns=NAME:.metadata.name,IMAGE:.spec.containers[0].image",
      "output": "web-0 nginx:1.25-alpine   web-1 nginx:1.25-alpine   web-2 nginx:1.26-alpine (only web-2 updated)"
    },
    {
      "type": "program",
      "question": "Program 6: Scale down and verify PVC retention",
      "code": "kubectl scale statefulset web --replicas=1\nkubectl get pods -l app=web\nkubectl get pvc | grep web",
      "output": "Only web-0 running, but PVCs for web-1 and web-2 still exist (data preserved)"
    },
    {
      "type": "program",
      "question": "Program 7: Use init container to set server-id based on ordinal",
      "code": "cat <<EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mysql\nspec:\n  serviceName: mysql-headless\n  replicas: 3\n  selector:\n    matchLabels:\n      app: mysql\n  template:\n    metadata:\n      labels:\n        app: mysql\n    spec:\n      initContainers:\n      - name: set-id\n        image: busybox\n        command: ['sh', '-c', 'echo server-id=$(($(hostname | grep -o \"[0-9]*$\") + 1)) > /config/server-id.cnf && cat /config/server-id.cnf']\n        volumeMounts:\n        - name: config\n          mountPath: /config\n      containers:\n      - name: mysql\n        image: mysql:8.0\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: secret\n        volumeMounts:\n        - name: config\n          mountPath: /etc/mysql/conf.d\n      volumes:\n      - name: config\n        emptyDir: {}\nEOF\nkubectl logs mysql-0 -c set-id",
      "output": "server-id=1 (mysql-1 gets server-id=2, mysql-2 gets server-id=3)"
    },
    {
      "type": "program",
      "question": "Program 8: Rolling restart and monitoring",
      "code": "kubectl rollout restart statefulset web\nkubectl rollout status statefulset web",
      "output": "Waiting for partitioned roll out... Waiting for 1 pods to be ready... statefulset rolling update complete 3/3"
    },
    {
      "type": "program",
      "question": "Program 9: Compare pod names in Deployment vs StatefulSet",
      "code": "kubectl create deployment test-deploy --image=nginx --replicas=3\nkubectl get pods -l app=test-deploy -o name\nkubectl get pods -l app=web -o name",
      "output": "Deployment: pod/test-deploy-6d4f5b-xk9zm (random suffix). StatefulSet: pod/web-0, pod/web-1, pod/web-2 (predictable ordinal)"
    },
    {
      "type": "program",
      "question": "Program 10: Delete StatefulSet without deleting pods (orphan)",
      "code": "kubectl delete statefulset web --cascade=orphan\nkubectl get pods -l app=web\nkubectl get statefulset web",
      "output": "Pods still running (orphaned), StatefulSet deleted. Pods can be adopted by recreating StatefulSet with same selector."
    }
  ]
}