{
  "id": "rate-limiting",
  "title": "Rate Limiting & Throttling",
  "category": "Foundations",
  "description": "Rate limiting controls the rate of requests a client can make to a service, protecting systems from abuse, overload, and ensuring fair resource distribution across users.",
  "explanation": "Rate limiting is a critical technique for protecting APIs and services by controlling how many requests a client can make within a given time window. It prevents abuse, ensures fair usage, protects backend resources, and maintains service quality under load.\n\nThe Token Bucket algorithm maintains a bucket with a fixed capacity of tokens. Tokens are added at a constant rate. Each request consumes one token. If the bucket is empty, the request is rejected. This allows short bursts up to the bucket capacity while enforcing an average rate.\n\nThe Leaky Bucket algorithm processes requests at a fixed rate, like water leaking from a bucket. Incoming requests fill the bucket; if full, excess requests are dropped. This produces a perfectly smooth output rate, ideal for scenarios requiring constant throughput.\n\nThe Fixed Window Counter algorithm divides time into fixed windows (e.g., 1-minute intervals) and counts requests per window. Simple to implement but suffers from boundary issues — a burst at the end of one window and start of the next can allow 2x the rate.\n\nThe Sliding Window Log keeps timestamps of all requests and counts those within the current sliding window. Accurate but memory-intensive. The Sliding Window Counter is a hybrid: it combines the current window's count with a weighted portion of the previous window's count, offering a good balance of accuracy and efficiency.\n\nDistributed rate limiting uses centralized storage like Redis with atomic operations (INCR, EXPIRE) to maintain counters across multiple application instances. API gateways (Kong, AWS API Gateway) provide built-in rate limiting capabilities.\n\nRate limiting can be applied at multiple layers: per-IP at the network edge, per-API-key at the gateway, per-user at the application, and globally per-service. The Retry-After header tells clients when they can retry after being rate limited (HTTP 429 status).",
  "code": "// Token Bucket Rate Limiter implementation\nclass TokenBucket {\n  constructor(capacity, refillRate) {\n    this.capacity = capacity;       // max tokens\n    this.tokens = capacity;         // current tokens\n    this.refillRate = refillRate;    // tokens per second\n    this.lastRefill = Date.now();\n  }\n\n  refill() {\n    const now = Date.now();\n    const elapsed = (now - this.lastRefill) / 1000;\n    this.tokens = Math.min(this.capacity, this.tokens + elapsed * this.refillRate);\n    this.lastRefill = now;\n  }\n\n  tryConsume(tokens = 1) {\n    this.refill();\n    if (this.tokens >= tokens) {\n      this.tokens -= tokens;\n      return { allowed: true, remaining: Math.floor(this.tokens) };\n    }\n    return { allowed: false, retryAfter: Math.ceil((tokens - this.tokens) / this.refillRate) };\n  }\n}\n\n// Sliding Window Counter Rate Limiter\nclass SlidingWindowCounter {\n  constructor(windowMs, maxRequests) {\n    this.windowMs = windowMs;\n    this.maxRequests = maxRequests;\n    this.prevCount = 0;\n    this.currCount = 0;\n    this.windowStart = Date.now();\n  }\n\n  _updateWindow(now) {\n    const elapsed = now - this.windowStart;\n    if (elapsed >= this.windowMs * 2) {\n      this.prevCount = 0;\n      this.currCount = 0;\n      this.windowStart = now;\n    } else if (elapsed >= this.windowMs) {\n      this.prevCount = this.currCount;\n      this.currCount = 0;\n      this.windowStart += this.windowMs;\n    }\n  }\n\n  tryRequest(now = Date.now()) {\n    this._updateWindow(now);\n    const elapsed = now - this.windowStart;\n    const weight = 1 - elapsed / this.windowMs;\n    const estimatedCount = this.prevCount * weight + this.currCount;\n\n    if (estimatedCount < this.maxRequests) {\n      this.currCount++;\n      return { allowed: true, estimated: Math.round(estimatedCount) };\n    }\n    return { allowed: false, estimated: Math.round(estimatedCount) };\n  }\n}\n\n// Fixed Window Counter\nclass FixedWindowCounter {\n  constructor(windowMs, maxRequests) {\n    this.windowMs = windowMs;\n    this.maxRequests = maxRequests;\n    this.count = 0;\n    this.windowStart = Date.now();\n  }\n\n  tryRequest(now = Date.now()) {\n    if (now - this.windowStart >= this.windowMs) {\n      this.count = 0;\n      this.windowStart = now;\n    }\n    if (this.count < this.maxRequests) {\n      this.count++;\n      return { allowed: true, remaining: this.maxRequests - this.count };\n    }\n    return { allowed: false, retryAfter: this.windowMs - (now - this.windowStart) };\n  }\n}",
  "example": "// Using the Token Bucket Rate Limiter\nconst limiter = new TokenBucket(10, 2); // 10 tokens max, 2 tokens/sec refill\n\n// Simulate API requests\nfor (let i = 0; i < 12; i++) {\n  const result = limiter.tryConsume();\n  console.log(`Request ${i + 1}: ${result.allowed ? 'ALLOWED' : 'DENIED'}`);\n}\n\n// Using the Fixed Window Counter\nconst fwLimiter = new FixedWindowCounter(60000, 100); // 100 requests per minute\nconst result = fwLimiter.tryRequest();\nconsole.log(`Allowed: ${result.allowed}, Remaining: ${result.remaining}`);\n\n// Rate limiting middleware example\nfunction rateLimitMiddleware(limits) {\n  const buckets = new Map();\n  return (req) => {\n    const key = req.headers['x-api-key'] || req.ip;\n    if (!buckets.has(key)) {\n      buckets.set(key, new TokenBucket(limits.capacity, limits.rate));\n    }\n    const result = buckets.get(key).tryConsume();\n    if (!result.allowed) {\n      return { status: 429, headers: { 'Retry-After': result.retryAfter } };\n    }\n    return { status: 200, headers: { 'X-RateLimit-Remaining': result.remaining } };\n  };\n}",
  "useCase": "Used to protect APIs from abuse, ensure fair resource allocation, prevent DDoS attacks, enforce billing quotas, and maintain service reliability under high traffic.",
  "interviewQuestions": [
    {
      "question": "Explain the token bucket algorithm for rate limiting.",
      "answer": "The token bucket maintains a bucket of fixed capacity. Tokens are added at a constant rate (refill rate). Each request consumes a token. If tokens are available, the request is allowed. If empty, the request is rejected. It permits short bursts (up to bucket capacity) while enforcing an average rate equal to the refill rate. Parameters: bucket capacity (burst size) and refill rate (sustained rate)."
    },
    {
      "question": "What is the difference between token bucket and leaky bucket algorithms?",
      "answer": "Token Bucket allows bursts up to the bucket capacity and is flexible with traffic patterns — it's rate-limiting with burst tolerance. Leaky Bucket processes requests at a strict constant rate (like a queue draining at fixed speed), producing smooth output but potentially adding latency to bursty traffic. Token bucket is more commonly used in API rate limiting; leaky bucket is preferred where constant processing rate is required."
    },
    {
      "question": "What's the boundary problem with fixed window counters and how does sliding window solve it?",
      "answer": "Fixed window counters reset at window boundaries. A client could send the maximum requests at the end of one window and the maximum again at the start of the next, effectively doubling the rate in a short period. Sliding window log tracks exact timestamps of each request and counts those within the last N seconds. Sliding window counter approximates this by weighting the previous window's count with the current window's count, avoiding the boundary burst problem."
    },
    {
      "question": "How would you implement distributed rate limiting across multiple server instances?",
      "answer": "Use a centralized data store like Redis. For token bucket: store token count and last refill timestamp in Redis using Lua scripts for atomicity. For sliding window: use Redis sorted sets with timestamps as scores. Use MULTI/EXEC or Lua scripts to ensure atomic check-and-increment. Redis Cluster provides horizontal scaling. Consider eventual consistency trade-offs — a small window of over-limit requests is usually acceptable."
    },
    {
      "question": "At which layers should rate limiting be applied?",
      "answer": "Rate limiting should be layered: 1) Network edge/CDN — block obvious DDoS by IP. 2) API Gateway — enforce per-API-key quotas and global limits. 3) Application layer — per-user, per-endpoint fine-grained limits. 4) Database layer — connection pool limits. Each layer provides different protection: edge stops volumetric attacks, gateway enforces business rules, application handles user-specific logic."
    },
    {
      "question": "What HTTP headers are used in rate limiting?",
      "answer": "Standard headers include: X-RateLimit-Limit (max requests in current window), X-RateLimit-Remaining (requests left), X-RateLimit-Reset (when the window resets, Unix timestamp), Retry-After (seconds until the client should retry, used with 429 status). These headers help clients self-regulate and implement backoff strategies. The 429 Too Many Requests status code indicates rate limiting."
    },
    {
      "question": "How would you rate limit a chat application differently from a REST API?",
      "answer": "A chat app needs per-channel message rate limits (e.g., 5 messages/second per user per channel), global message rate per user, and connection-level rate limiting for WebSocket frames. Unlike REST APIs where each request is independent, chat requires stateful rate limiting tied to WebSocket connections. Use sliding window at the connection handler level, with different limits for different message types (text, file upload, typing indicators)."
    },
    {
      "question": "What is the sliding window log algorithm and what are its trade-offs?",
      "answer": "Sliding window log stores the timestamp of every request in a sorted set. To check the rate, remove all entries older than the window size and count remaining entries. Pros: perfectly accurate, no boundary issues. Cons: memory-intensive (stores every request timestamp), cleanup overhead. For high-volume APIs, this is impractical — a user making 10K req/min would store 10K timestamps. The sliding window counter hybrid is preferred for production use."
    },
    {
      "question": "How do API gateways like Kong or AWS API Gateway handle rate limiting?",
      "answer": "API gateways implement rate limiting as a plugin/policy layer: requests are checked before reaching backend services. AWS API Gateway supports usage plans with throttling (steady-state rate and burst) per API key. Kong uses a plugin with configurable policies (local, Redis-backed cluster). They support multiple granularities: per-deployment, per-stage, per-method. Benefits: centralized configuration, no application code changes, consistent enforcement."
    },
    {
      "question": "How would you handle rate limiting for a multi-tenant SaaS application?",
      "answer": "Implement tiered rate limits based on subscription plan (free: 100 req/min, pro: 1000 req/min, enterprise: custom). Use tenant ID as the rate limit key. Store limits in a configuration service. Apply both per-tenant global limits and per-endpoint limits. Use token bucket for burst flexibility. Track usage for billing. Implement graceful degradation — instead of hard rejection, consider queuing or reducing response quality for over-limit tenants."
    }
  ],
  "exercises": [
    {
      "type": "design",
      "question": "Design a distributed rate limiter for a global API serving requests from multiple data centers. How do you handle consistency across regions?",
      "answer": "Use a local rate limiter per data center for immediate decisions with eventual consistency via a central Redis cluster. Each DC maintains local counters and periodically syncs with global counters. Accept slight over-limiting risk. Use sticky sessions or geo-routing to minimize cross-DC coordination. For strict limits, use a single Redis cluster with read replicas. Trade-off: strict consistency adds latency; eventual consistency may briefly exceed limits."
    },
    {
      "type": "scenario",
      "question": "Your API gateway rate limiter is using fixed window counters. During a flash sale, you observe that the system allows 2x the configured rate. What happened and how do you fix it?",
      "answer": "This is the fixed window boundary problem. If the flash sale started at the window boundary, users sent max requests at the end of window N and again at the start of window N+1. Fix: switch to sliding window counter algorithm. Short-term mitigation: reduce the window size (e.g., from 1 minute to 10 seconds with proportionally smaller limits) to reduce the burst window."
    },
    {
      "type": "estimation",
      "question": "A public API allows 1000 requests per minute per API key. You have 10,000 active API keys. Estimate the memory needed for a Redis-based sliding window log implementation.",
      "answer": "Worst case: all 10K keys at max rate = 10M request timestamps per minute. Each timestamp in a Redis sorted set: ~50 bytes (member + score). Total: 10M * 50 = 500MB. With 1-minute TTL, memory stays bounded. However, in practice not all keys hit max rate — at 20% average utilization: ~100MB. Using fixed window counters instead: 10K keys * ~100 bytes = 1MB, dramatically less."
    },
    {
      "type": "debug",
      "question": "Your token bucket rate limiter allows 100 requests per minute, but users report they can only send about 90 requests. What could be wrong?",
      "answer": "Possible causes: 1) Refill calculation uses integer math, losing fractional tokens. 2) Clock drift between refill checks causes lost time. 3) The refill rate calculation has an off-by-one or rounding error. 4) Health check or monitoring requests consume tokens from the same bucket. 5) Concurrent requests create a race condition on the token count. Fix: use floating-point for token count, atomic operations, and exclude internal requests."
    },
    {
      "type": "tricky",
      "question": "Can a token bucket rate limiter with capacity 10 and refill rate 10/sec ever allow more than 10 requests in a one-second window?",
      "answer": "Yes! If the bucket was full (10 tokens) and not recently used, the first 10 requests drain it instantly. Then if 0.5 seconds pass, 5 tokens are refilled. Those 5 requests plus the initial 10 means 15 requests in 1.5 seconds — more than 10 per second average. Over any 1-second sliding window, you can see up to capacity + refillRate = 20 requests. The token bucket controls average rate and burst size, not strict per-second limits."
    },
    {
      "type": "design",
      "question": "Design a rate limiting system that handles different limits per endpoint (e.g., login: 5/min, search: 100/min, data export: 1/hour) using a single system.",
      "answer": "Use composite rate limit keys: 'user:{id}:endpoint:{path}'. Store limits in a config map keyed by endpoint pattern. Each request generates the composite key, looks up the endpoint's rate limit config, and checks against the appropriate limiter. Use a single Redis instance with different TTLs per endpoint. Implement as middleware that matches the request path to a config entry. Use regex patterns for endpoint matching."
    },
    {
      "type": "explain",
      "question": "Explain the difference between rate limiting and throttling. Are they the same thing?",
      "answer": "They're related but different. Rate limiting rejects requests that exceed a defined rate — it's a hard boundary (HTTP 429). Throttling slows down or queues excess requests rather than rejecting them — it's a soft control that introduces delay. Example: rate limiting drops request 101 after 100/min; throttling queues request 101 and processes it when capacity is available. Throttling is better for internal services; rate limiting for public APIs."
    },
    {
      "type": "scenario",
      "question": "You notice your Redis-based rate limiter is adding 20ms latency to every API call. How do you reduce this overhead?",
      "answer": "Solutions: 1) Use Redis pipelining to batch the check-increment into one round trip. 2) Use Lua scripts for atomic operations in a single call. 3) Implement a local in-memory cache with periodic sync to Redis. 4) Use a two-tier approach: fast local counter for rough limits, Redis for accurate accounting. 5) Use Redis read replicas for check operations. 6) Consider rate limiting asynchronously — allow the request and post-check."
    },
    {
      "type": "estimation",
      "question": "Design rate limiting for a social media API that serves 50,000 requests per second globally. How many Redis operations per second are needed?",
      "answer": "Each rate limit check requires at least 2 Redis operations (GET/INCR + EXPIRE, or a Lua script as 1 op). At 50K req/s with Lua scripts: 50K Redis ops/s. A single Redis instance handles ~100K ops/s, so one instance suffices. With sliding window log: 50K ZADD + 50K ZRANGEBYSCORE = 100K ops/s — at the limit. For safety, use Redis Cluster with 3 shards. With local caching tier: reduce to ~5K Redis ops/s."
    },
    {
      "type": "debug",
      "question": "After deploying a new rate limiter, you discover that some users are bypassing it by rotating their API keys. How do you prevent this?",
      "answer": "Rate limit by multiple dimensions simultaneously: per API key AND per IP AND per user account. Implement key rotation detection: if the same IP uses many keys in a short period, flag and rate limit at IP level. Add request fingerprinting (user-agent, headers pattern). Implement hierarchical rate limits: user account → API key → IP. Monitor for suspicious patterns and alert. Consider requiring API key binding to specific IPs or verified accounts."
    }
  ],
  "programExercises": [
    {
      "question": "Program 1: Implement a Token Bucket rate limiter",
      "code": "class TokenBucket {\n  constructor(capacity, refillRate) {\n    this.capacity = capacity;\n    this.tokens = capacity;\n    this.refillRate = refillRate;\n    this.lastRefill = 0;\n  }\n\n  _refill(now) {\n    const elapsed = now - this.lastRefill;\n    this.tokens = Math.min(this.capacity, this.tokens + elapsed * this.refillRate);\n    this.lastRefill = now;\n  }\n\n  allow(now = 0) {\n    this._refill(now);\n    if (this.tokens >= 1) {\n      this.tokens -= 1;\n      return true;\n    }\n    return false;\n  }\n}\n\nconst bucket = new TokenBucket(5, 1); // 5 capacity, 1 token/s\nconst results = [];\n\n// Burst of 7 requests at time 0\nfor (let i = 0; i < 7; i++) {\n  results.push(bucket.allow(0));\n}\n\n// After 3 seconds, 3 tokens refilled\nfor (let i = 0; i < 4; i++) {\n  results.push(bucket.allow(3));\n}\n\nconsole.log(results);",
      "output": "[\n  true,  true,  true,\n  true,  true,  false,\n  false, true,  true,\n  true,  false\n]"
    },
    {
      "question": "Program 2: Implement a Fixed Window Counter",
      "code": "class FixedWindow {\n  constructor(windowSize, limit) {\n    this.windowSize = windowSize;\n    this.limit = limit;\n    this.windows = new Map();\n  }\n\n  _getWindowKey(timestamp) {\n    return Math.floor(timestamp / this.windowSize);\n  }\n\n  allow(timestamp) {\n    const key = this._getWindowKey(timestamp);\n    const count = this.windows.get(key) || 0;\n    if (count < this.limit) {\n      this.windows.set(key, count + 1);\n      return true;\n    }\n    return false;\n  }\n\n  getCount(timestamp) {\n    const key = this._getWindowKey(timestamp);\n    return this.windows.get(key) || 0;\n  }\n}\n\nconst fw = new FixedWindow(10, 3); // 10-second window, 3 requests max\n\nconsole.log('t=1:', fw.allow(1));   // request 1\nconsole.log('t=3:', fw.allow(3));   // request 2\nconsole.log('t=7:', fw.allow(7));   // request 3\nconsole.log('t=9:', fw.allow(9));   // over limit\nconsole.log('t=11:', fw.allow(11)); // new window\nconsole.log('Window 0 count:', fw.getCount(5));",
      "output": "t=1: true\nt=3: true\nt=7: true\nt=9: false\nt=11: true\nWindow 0 count: 3"
    },
    {
      "question": "Program 3: Implement a Sliding Window Log",
      "code": "class SlidingWindowLog {\n  constructor(windowMs, maxRequests) {\n    this.windowMs = windowMs;\n    this.maxRequests = maxRequests;\n    this.logs = [];\n  }\n\n  allow(timestamp) {\n    // Remove expired entries\n    const windowStart = timestamp - this.windowMs;\n    this.logs = this.logs.filter(t => t > windowStart);\n\n    if (this.logs.length < this.maxRequests) {\n      this.logs.push(timestamp);\n      return true;\n    }\n    return false;\n  }\n\n  getLogSize() {\n    return this.logs.length;\n  }\n}\n\nconst swl = new SlidingWindowLog(1000, 3); // 1 second, 3 requests\n\nconsole.log('t=100:', swl.allow(100));\nconsole.log('t=400:', swl.allow(400));\nconsole.log('t=800:', swl.allow(800));\nconsole.log('t=900:', swl.allow(900));   // denied\nconsole.log('t=1200:', swl.allow(1200)); // t=100 expired\nconsole.log('Log size:', swl.getLogSize());",
      "output": "t=100: true\nt=400: true\nt=800: true\nt=900: false\nt=1200: true\nLog size: 3"
    },
    {
      "question": "Program 4: Implement a Leaky Bucket rate limiter",
      "code": "class LeakyBucket {\n  constructor(capacity, leakRate) {\n    this.capacity = capacity;\n    this.leakRate = leakRate; // requests drained per second\n    this.water = 0;\n    this.lastLeak = 0;\n  }\n\n  _leak(now) {\n    const elapsed = now - this.lastLeak;\n    const leaked = elapsed * this.leakRate;\n    this.water = Math.max(0, this.water - leaked);\n    this.lastLeak = now;\n  }\n\n  allow(now) {\n    this._leak(now);\n    if (this.water < this.capacity) {\n      this.water += 1;\n      return true;\n    }\n    return false;\n  }\n}\n\nconst lb = new LeakyBucket(3, 1); // capacity 3, leak 1/sec\n\nconst times = [0, 0, 0, 0, 2, 2, 2, 5];\nconst results = times.map(t => ({ time: t, allowed: lb.allow(t) }));\n\nresults.forEach(r => console.log(`t=${r.time}: ${r.allowed ? 'ALLOWED' : 'DENIED'}`));",
      "output": "t=0: ALLOWED\nt=0: ALLOWED\nt=0: ALLOWED\nt=0: DENIED\nt=2: ALLOWED\nt=2: ALLOWED\nt=2: DENIED\nt=5: ALLOWED"
    },
    {
      "question": "Program 5: Implement a per-user rate limiter",
      "code": "class PerUserRateLimiter {\n  constructor(windowMs, maxRequests) {\n    this.windowMs = windowMs;\n    this.maxRequests = maxRequests;\n    this.users = new Map();\n  }\n\n  allow(userId, timestamp) {\n    if (!this.users.has(userId)) {\n      this.users.set(userId, { count: 0, windowStart: timestamp });\n    }\n    const user = this.users.get(userId);\n\n    if (timestamp - user.windowStart >= this.windowMs) {\n      user.count = 0;\n      user.windowStart = timestamp;\n    }\n\n    if (user.count < this.maxRequests) {\n      user.count++;\n      return { allowed: true, remaining: this.maxRequests - user.count };\n    }\n    return { allowed: false, remaining: 0 };\n  }\n}\n\nconst limiter = new PerUserRateLimiter(60000, 3); // 3 per minute\n\nconst requests = [\n  { user: 'alice', time: 1000 },\n  { user: 'bob',   time: 1500 },\n  { user: 'alice', time: 2000 },\n  { user: 'alice', time: 3000 },\n  { user: 'alice', time: 4000 },  // alice over limit\n  { user: 'bob',   time: 5000 },\n];\n\nrequests.forEach(r => {\n  const result = limiter.allow(r.user, r.time);\n  console.log(`${r.user} t=${r.time}: ${result.allowed ? 'OK' : 'DENIED'} (remaining: ${result.remaining})`);\n});",
      "output": "alice t=1000: OK (remaining: 2)\nbob t=1500: OK (remaining: 2)\nalice t=2000: OK (remaining: 1)\nalice t=3000: OK (remaining: 0)\nalice t=4000: DENIED (remaining: 0)\nbob t=5000: OK (remaining: 1)"
    },
    {
      "question": "Program 6: Implement a sliding window counter (hybrid approach)",
      "code": "class SlidingWindowCounter {\n  constructor(windowMs, limit) {\n    this.windowMs = windowMs;\n    this.limit = limit;\n    this.prevCount = 0;\n    this.currCount = 0;\n    this.currStart = 0;\n  }\n\n  allow(timestamp) {\n    const windowIndex = Math.floor(timestamp / this.windowMs);\n    const currWindowStart = windowIndex * this.windowMs;\n\n    if (currWindowStart !== this.currStart) {\n      if (currWindowStart - this.currStart === this.windowMs) {\n        this.prevCount = this.currCount;\n      } else {\n        this.prevCount = 0;\n      }\n      this.currCount = 0;\n      this.currStart = currWindowStart;\n    }\n\n    const elapsed = timestamp - this.currStart;\n    const weight = 1 - elapsed / this.windowMs;\n    const estimated = this.prevCount * weight + this.currCount;\n\n    if (estimated < this.limit) {\n      this.currCount++;\n      return { allowed: true, estimated: Math.round(estimated) };\n    }\n    return { allowed: false, estimated: Math.round(estimated) };\n  }\n}\n\nconst swc = new SlidingWindowCounter(100, 5); // 100ms window, 5 req limit\n\n// Window 0: 4 requests\nfor (let t = 0; t < 80; t += 20) {\n  console.log(`t=${t}:`, swc.allow(t));\n}\n// Window 1: weighted count = 4*(1-0.5) + 0 = 2, so 3 more allowed\nconsole.log('t=150:', swc.allow(150));\nconsole.log('t=160:', swc.allow(160));\nconsole.log('t=170:', swc.allow(170));\nconsole.log('t=180:', swc.allow(180));",
      "output": "t=0: { allowed: true, estimated: 0 }\nt=20: { allowed: true, estimated: 1 }\nt=40: { allowed: true, estimated: 2 }\nt=60: { allowed: true, estimated: 3 }\nt=150: { allowed: true, estimated: 2 }\nt=160: { allowed: true, estimated: 3 }\nt=170: { allowed: true, estimated: 4 }\nt=180: { allowed: false, estimated: 5 }"
    },
    {
      "question": "Program 7: Implement rate limit middleware with Retry-After header",
      "code": "class RateLimitMiddleware {\n  constructor(limits) {\n    this.limits = limits;\n    this.counters = new Map();\n  }\n\n  handleRequest(apiKey, endpoint, timestamp) {\n    const limit = this.limits[endpoint] || this.limits['default'];\n    const key = `${apiKey}:${endpoint}`;\n\n    if (!this.counters.has(key)) {\n      this.counters.set(key, { count: 0, windowStart: timestamp });\n    }\n\n    const counter = this.counters.get(key);\n    if (timestamp - counter.windowStart >= limit.windowMs) {\n      counter.count = 0;\n      counter.windowStart = timestamp;\n    }\n\n    if (counter.count < limit.max) {\n      counter.count++;\n      return {\n        status: 200,\n        headers: {\n          'X-RateLimit-Limit': limit.max,\n          'X-RateLimit-Remaining': limit.max - counter.count,\n        }\n      };\n    }\n\n    const retryAfter = Math.ceil((limit.windowMs - (timestamp - counter.windowStart)) / 1000);\n    return {\n      status: 429,\n      headers: {\n        'X-RateLimit-Limit': limit.max,\n        'X-RateLimit-Remaining': 0,\n        'Retry-After': retryAfter\n      }\n    };\n  }\n}\n\nconst mw = new RateLimitMiddleware({\n  '/api/login': { max: 3, windowMs: 60000 },\n  '/api/search': { max: 10, windowMs: 60000 },\n  'default': { max: 100, windowMs: 60000 }\n});\n\nconsole.log(mw.handleRequest('key1', '/api/login', 1000));\nconsole.log(mw.handleRequest('key1', '/api/login', 2000));\nconsole.log(mw.handleRequest('key1', '/api/login', 3000));\nconsole.log(mw.handleRequest('key1', '/api/login', 4000)); // 429",
      "output": "{ status: 200, headers: { 'X-RateLimit-Limit': 3, 'X-RateLimit-Remaining': 2 } }\n{ status: 200, headers: { 'X-RateLimit-Limit': 3, 'X-RateLimit-Remaining': 1 } }\n{ status: 200, headers: { 'X-RateLimit-Limit': 3, 'X-RateLimit-Remaining': 0 } }\n{\n  status: 429,\n  headers: { 'X-RateLimit-Limit': 3, 'X-RateLimit-Remaining': 0, 'Retry-After': 57 }\n}"
    },
    {
      "question": "Program 8: Implement a distributed rate limiter simulation with Redis-like store",
      "code": "class RedisSimulator {\n  constructor() {\n    this.store = new Map();\n    this.ttls = new Map();\n  }\n\n  incr(key, now) {\n    if (this.ttls.has(key) && now > this.ttls.get(key)) {\n      this.store.delete(key);\n      this.ttls.delete(key);\n    }\n    const val = (this.store.get(key) || 0) + 1;\n    this.store.set(key, val);\n    return val;\n  }\n\n  expire(key, ttlMs, now) {\n    if (!this.ttls.has(key)) {\n      this.ttls.set(key, now + ttlMs);\n    }\n  }\n\n  get(key, now) {\n    if (this.ttls.has(key) && now > this.ttls.get(key)) {\n      this.store.delete(key);\n      this.ttls.delete(key);\n      return 0;\n    }\n    return this.store.get(key) || 0;\n  }\n}\n\nfunction distributedRateLimit(redis, userId, limit, windowMs, now) {\n  const key = `ratelimit:${userId}:${Math.floor(now / windowMs)}`;\n  const count = redis.incr(key, now);\n  redis.expire(key, windowMs, now);\n  return { allowed: count <= limit, count, limit };\n}\n\nconst redis = new RedisSimulator();\n\n// Simulate requests from user-1\nfor (let i = 0; i < 6; i++) {\n  const result = distributedRateLimit(redis, 'user-1', 5, 10000, 1000 + i * 100);\n  console.log(`Request ${i + 1}: allowed=${result.allowed}, count=${result.count}`);\n}",
      "output": "Request 1: allowed=true, count=1\nRequest 2: allowed=true, count=2\nRequest 3: allowed=true, count=3\nRequest 4: allowed=true, count=4\nRequest 5: allowed=true, count=5\nRequest 6: allowed=false, count=6"
    },
    {
      "question": "Program 9: Implement tiered rate limiting for different subscription plans",
      "code": "class TieredRateLimiter {\n  constructor(tiers) {\n    this.tiers = tiers;\n    this.usage = new Map();\n  }\n\n  getUserTier(userId) {\n    // Simulated tier lookup\n    const tierMap = { 'free-user': 'free', 'pro-user': 'pro', 'enterprise-user': 'enterprise' };\n    return tierMap[userId] || 'free';\n  }\n\n  allow(userId, timestamp) {\n    const tier = this.getUserTier(userId);\n    const limits = this.tiers[tier];\n\n    if (!this.usage.has(userId)) {\n      this.usage.set(userId, { count: 0, windowStart: timestamp });\n    }\n\n    const usage = this.usage.get(userId);\n    if (timestamp - usage.windowStart >= limits.windowMs) {\n      usage.count = 0;\n      usage.windowStart = timestamp;\n    }\n\n    if (usage.count < limits.maxRequests) {\n      usage.count++;\n      return { allowed: true, tier, used: usage.count, limit: limits.maxRequests };\n    }\n    return { allowed: false, tier, used: usage.count, limit: limits.maxRequests };\n  }\n}\n\nconst limiter = new TieredRateLimiter({\n  free: { maxRequests: 2, windowMs: 60000 },\n  pro: { maxRequests: 5, windowMs: 60000 },\n  enterprise: { maxRequests: 100, windowMs: 60000 }\n});\n\nconst requests = ['free-user', 'free-user', 'free-user', 'pro-user', 'pro-user', 'pro-user'];\nrequests.forEach((user, i) => {\n  const r = limiter.allow(user, i * 1000);\n  console.log(`${user}: ${r.allowed ? 'OK' : 'DENIED'} (${r.used}/${r.limit}, tier: ${r.tier})`);\n});",
      "output": "free-user: OK (1/2, tier: free)\nfree-user: OK (2/2, tier: free)\nfree-user: DENIED (2/2, tier: free)\npro-user: OK (1/5, tier: pro)\npro-user: OK (2/5, tier: pro)\npro-user: OK (3/5, tier: pro)"
    },
    {
      "question": "Program 10: Compare throughput of different rate limiting algorithms",
      "code": "function simulateAlgorithm(name, allowFn, requests) {\n  let allowed = 0;\n  let denied = 0;\n  requests.forEach(t => {\n    if (allowFn(t)) allowed++;\n    else denied++;\n  });\n  return { name, allowed, denied, total: requests.length };\n}\n\n// Generate bursty request pattern: 8 at time 0, then 4 at time 500\nconst requests = [\n  ...Array(8).fill(0),\n  ...Array(4).fill(500)\n];\n\n// Fixed Window (window=1000ms, limit=10)\nlet fwCount = new Map();\nconst fwResult = simulateAlgorithm('Fixed Window', (t) => {\n  const key = Math.floor(t / 1000);\n  const c = (fwCount.get(key) || 0) + 1;\n  fwCount.set(key, c);\n  return c <= 10;\n}, requests);\n\n// Sliding Window Log (window=1000ms, limit=10)\nlet swlLog = [];\nconst swlResult = simulateAlgorithm('Sliding Window Log', (t) => {\n  swlLog = swlLog.filter(ts => ts > t - 1000);\n  if (swlLog.length < 10) { swlLog.push(t); return true; }\n  return false;\n}, requests);\n\n// Token Bucket (capacity=10, refill=10/sec)\nlet tokens = 10;\nlet lastRefill = 0;\nconst tbResult = simulateAlgorithm('Token Bucket', (t) => {\n  tokens = Math.min(10, tokens + (t - lastRefill) / 1000 * 10);\n  lastRefill = t;\n  if (tokens >= 1) { tokens--; return true; }\n  return false;\n}, requests);\n\nconsole.log(fwResult);\nconsole.log(swlResult);\nconsole.log(tbResult);",
      "output": "{ name: 'Fixed Window', allowed: 10, denied: 2, total: 12 }\n{ name: 'Sliding Window Log', allowed: 10, denied: 2, total: 12 }\n{ name: 'Token Bucket', allowed: 12, denied: 0, total: 12 }"
    }
  ]
}
