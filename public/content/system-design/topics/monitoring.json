{
  "id": "monitoring",
  "title": "Monitoring & Observability",
  "category": "Foundations",
  "description": "Monitoring and observability enable teams to understand the internal state of distributed systems through metrics, logs, and traces, ensuring reliability, performance, and rapid incident response.",
  "explanation": "Monitoring and observability are foundational pillars of operating reliable distributed systems. While monitoring tells you when something is wrong (reactive), observability lets you ask arbitrary questions about your system's behavior without deploying new code (proactive). The three pillars of observability are metrics (numeric measurements over time like CPU usage or request count), logs (discrete timestamped events describing what happened), and traces (end-to-end request flows across service boundaries). Together, they provide a comprehensive view of system health and behavior.\n\nKey concepts in monitoring include SLIs, SLOs, and SLAs. A Service Level Indicator (SLI) is a quantitative measure of service behavior, such as request latency or error rate. A Service Level Objective (SLO) is a target value or range for an SLI, like 'p99 latency < 200ms' or '99.9% availability.' A Service Level Agreement (SLA) is a formal contract with customers that specifies consequences (usually financial) if SLOs are not met. Error budgets — the allowed amount of unreliability (e.g., 0.1% downtime for a 99.9% SLO) — help teams balance feature velocity with reliability work. Google's four golden signals — latency, traffic, errors, and saturation — provide a minimal but powerful framework for monitoring any service.\n\nPercentile metrics are critical for understanding real user experience. While averages hide outliers, percentiles reveal the distribution: p50 (median) shows what a typical user experiences, p95 shows the experience for most users, and p99 captures tail latency that affects 1 in 100 users. A system with 50ms average but 2s p99 latency has a serious tail latency problem that averages completely mask. Histograms (bucketed counts of observations) are the preferred metric type for tracking latency distributions because they allow computing arbitrary percentiles after the fact.\n\nStructured logging replaces unstructured text logs with machine-parseable key-value pairs (typically JSON), enabling powerful querying and aggregation. Each log entry should include a timestamp, severity level, correlation ID (trace ID), service name, and contextual fields. Distributed tracing follows a request as it traverses multiple services by propagating a trace context (trace ID + span ID) through headers. Each unit of work creates a span with timing data, and spans are linked into a trace tree, revealing exactly where time is spent and where failures occur. Tools like OpenTelemetry provide vendor-neutral instrumentation for all three pillars.\n\nAlerting strategies must balance signal quality with noise reduction. Static threshold alerts (e.g., 'error rate > 5%') are simple but brittle — they don't account for normal traffic variations. Anomaly detection uses statistical methods or ML to detect deviations from learned baselines, reducing false positives. Best practices include alerting on symptoms (user impact) rather than causes, using multi-window burn-rate alerts for SLO-based monitoring, and maintaining runbooks for every alert. The RED method (Rate, Errors, Duration) is ideal for request-driven services, while the USE method (Utilization, Saturation, Errors) is better for infrastructure resources like CPU, memory, and disk. Effective dashboards follow a hierarchy: a top-level overview showing golden signals, drill-down views for individual services, and detailed views for debugging specific issues.",
  "code": "// Metrics Collector: Counters, Histograms (with percentiles), Gauges, Rate\nclass MetricsCollector {\n  constructor() {\n    this.counters = new Map();\n    this.histograms = new Map();\n    this.gauges = new Map();\n    this.rateWindows = new Map();\n  }\n\n  // --- Counters ---\n  incrementCounter(name, value = 1) {\n    const current = this.counters.get(name) || 0;\n    this.counters.set(name, current + value);\n    // Track for rate calculation\n    if (!this.rateWindows.has(name)) {\n      this.rateWindows.set(name, []);\n    }\n    this.rateWindows.get(name).push({ value, timestamp: Date.now() });\n  }\n\n  getCounter(name) {\n    return this.counters.get(name) || 0;\n  }\n\n  // --- Gauges ---\n  setGauge(name, value) {\n    this.gauges.set(name, value);\n  }\n\n  getGauge(name) {\n    return this.gauges.get(name);\n  }\n\n  // --- Histograms ---\n  recordHistogram(name, value) {\n    if (!this.histograms.has(name)) {\n      this.histograms.set(name, []);\n    }\n    this.histograms.get(name).push(value);\n  }\n\n  getPercentile(name, percentile) {\n    const values = this.histograms.get(name);\n    if (!values || values.length === 0) return null;\n    const sorted = [...values].sort((a, b) => a - b);\n    const index = Math.ceil((percentile / 100) * sorted.length) - 1;\n    return sorted[Math.max(0, index)];\n  }\n\n  getHistogramStats(name) {\n    const values = this.histograms.get(name);\n    if (!values || values.length === 0) return null;\n    const sorted = [...values].sort((a, b) => a - b);\n    const sum = sorted.reduce((a, b) => a + b, 0);\n    return {\n      count: sorted.length,\n      min: sorted[0],\n      max: sorted[sorted.length - 1],\n      avg: Math.round((sum / sorted.length) * 100) / 100,\n      p50: this.getPercentile(name, 50),\n      p95: this.getPercentile(name, 95),\n      p99: this.getPercentile(name, 99)\n    };\n  }\n\n  // --- Rate Computation ---\n  getRate(name, windowMs = 60000) {\n    const entries = this.rateWindows.get(name);\n    if (!entries) return 0;\n    const now = Date.now();\n    const windowStart = now - windowMs;\n    const inWindow = entries.filter(e => e.timestamp >= windowStart);\n    const totalValue = inWindow.reduce((sum, e) => sum + e.value, 0);\n    return Math.round((totalValue / (windowMs / 1000)) * 100) / 100;\n  }\n\n  // --- Summary Report ---\n  getReport() {\n    const report = { counters: {}, gauges: {}, histograms: {} };\n    for (const [k, v] of this.counters) report.counters[k] = v;\n    for (const [k, v] of this.gauges) report.gauges[k] = v;\n    for (const [k] of this.histograms) report.histograms[k] = this.getHistogramStats(k);\n    return report;\n  }\n}\n\n// --- Demo ---\nconst metrics = new MetricsCollector();\n\nmetrics.incrementCounter('http_requests_total');\nmetrics.incrementCounter('http_requests_total');\nmetrics.incrementCounter('http_errors_total');\n\nmetrics.setGauge('active_connections', 42);\nmetrics.setGauge('cpu_usage_percent', 68.5);\n\n[12, 15, 18, 22, 30, 45, 50, 120, 200, 500].forEach(v =>\n  metrics.recordHistogram('request_duration_ms', v)\n);\n\nconsole.log('Counter:', metrics.getCounter('http_requests_total'));\nconsole.log('Gauge:', metrics.getGauge('active_connections'));\nconsole.log('p50:', metrics.getPercentile('request_duration_ms', 50));\nconsole.log('p99:', metrics.getPercentile('request_duration_ms', 99));\nconsole.log('Stats:', metrics.getHistogramStats('request_duration_ms'));",
  "example": "// Full Request Monitoring Middleware\n// Captures latency histogram, error rate, request rate, and health report\n\nclass RequestMonitor {\n  constructor() {\n    this.metrics = {\n      totalRequests: 0,\n      totalErrors: 0,\n      latencies: [],\n      statusCodes: {},\n      endpoints: {},\n      startTime: Date.now()\n    };\n  }\n\n  // Middleware: wrap a request handler\n  middleware(endpoint, handler) {\n    return (req) => {\n      const start = Date.now();\n      this.metrics.totalRequests++;\n\n      // Track per-endpoint\n      if (!this.metrics.endpoints[endpoint]) {\n        this.metrics.endpoints[endpoint] = { count: 0, errors: 0, latencies: [] };\n      }\n      this.metrics.endpoints[endpoint].count++;\n\n      try {\n        const result = handler(req);\n        const latency = Date.now() - start;\n        this.metrics.latencies.push(latency);\n        this.metrics.endpoints[endpoint].latencies.push(latency);\n        const status = result.status || 200;\n        this.metrics.statusCodes[status] = (this.metrics.statusCodes[status] || 0) + 1;\n        return { ...result, latency };\n      } catch (err) {\n        const latency = Date.now() - start;\n        this.metrics.totalErrors++;\n        this.metrics.endpoints[endpoint].errors++;\n        this.metrics.latencies.push(latency);\n        this.metrics.statusCodes[500] = (this.metrics.statusCodes[500] || 0) + 1;\n        return { status: 500, error: err.message, latency };\n      }\n    };\n  }\n\n  // Compute percentile from array\n  percentile(arr, p) {\n    if (arr.length === 0) return 0;\n    const sorted = [...arr].sort((a, b) => a - b);\n    const idx = Math.ceil((p / 100) * sorted.length) - 1;\n    return sorted[Math.max(0, idx)];\n  }\n\n  // Error rate as percentage\n  errorRate() {\n    if (this.metrics.totalRequests === 0) return 0;\n    return Math.round((this.metrics.totalErrors / this.metrics.totalRequests) * 10000) / 100;\n  }\n\n  // Request rate per second\n  requestRate() {\n    const elapsed = (Date.now() - this.metrics.startTime) / 1000;\n    if (elapsed === 0) return 0;\n    return Math.round((this.metrics.totalRequests / elapsed) * 100) / 100;\n  }\n\n  // Generate full health report\n  healthReport() {\n    return {\n      uptime: `${Math.round((Date.now() - this.metrics.startTime) / 1000)}s`,\n      totalRequests: this.metrics.totalRequests,\n      errorRate: `${this.errorRate()}%`,\n      requestRate: `${this.requestRate()} req/s`,\n      latency: {\n        p50: `${this.percentile(this.metrics.latencies, 50)}ms`,\n        p95: `${this.percentile(this.metrics.latencies, 95)}ms`,\n        p99: `${this.percentile(this.metrics.latencies, 99)}ms`\n      },\n      statusCodes: this.metrics.statusCodes,\n      endpoints: Object.fromEntries(\n        Object.entries(this.metrics.endpoints).map(([ep, data]) => [\n          ep,\n          {\n            requests: data.count,\n            errors: data.errors,\n            p50: `${this.percentile(data.latencies, 50)}ms`,\n            p99: `${this.percentile(data.latencies, 99)}ms`\n          }\n        ])\n      )\n    };\n  }\n}\n\n// --- Usage ---\nconst monitor = new RequestMonitor();\n\n// Define handlers\nconst getUser = monitor.middleware('/api/user', (req) => {\n  return { status: 200, body: { id: req.userId, name: 'Alice' } };\n});\n\nconst createOrder = monitor.middleware('/api/order', (req) => {\n  if (!req.item) throw new Error('Item required');\n  return { status: 201, body: { orderId: 'ORD-123' } };\n});\n\n// Simulate requests\nconsole.log(getUser({ userId: 1 }));\nconsole.log(getUser({ userId: 2 }));\nconsole.log(createOrder({ item: 'Widget' }));\nconsole.log(createOrder({})); // will error\n\nconsole.log('\\nHealth Report:', JSON.stringify(monitor.healthReport(), null, 2));",
  "useCase": "Monitoring and observability are essential in production systems for detecting and diagnosing failures, SRE practices for maintaining reliability budgets, incident response for rapid root-cause analysis, capacity planning by tracking resource utilization trends, and performance optimization by identifying bottlenecks through latency percentiles and distributed traces.",
  "interviewQuestions": [
    {
      "question": "What is the difference between SLI, SLO, and SLA?",
      "answer": "An SLI (Service Level Indicator) is a quantitative metric that measures a specific aspect of service quality, such as latency (p99 < 200ms) or availability (successful requests / total requests). An SLO (Service Level Objective) is a target value or range for an SLI that the team aims to maintain, like '99.95% availability over a 30-day window.' An SLA (Service Level Agreement) is a formal contract with customers that defines consequences (typically financial credits or penalties) if SLOs are breached. SLIs inform SLOs, which back SLAs. Teams should set internal SLOs tighter than external SLAs to provide a buffer."
    },
    {
      "question": "What are the four golden signals and why are they important?",
      "answer": "Google's four golden signals are: 1) Latency — the time it takes to serve a request (track separately for successful vs failed requests). 2) Traffic — the demand on the system measured in requests per second or concurrent sessions. 3) Errors — the rate of failed requests, whether explicit (HTTP 500), implicit (HTTP 200 with wrong content), or policy-based (responses slower than a threshold). 4) Saturation — how full the service is, measuring resource utilization (CPU, memory, I/O) and predicting when capacity will be exhausted. These four signals cover the essential dimensions of service health and are sufficient for meaningful monitoring of most systems."
    },
    {
      "question": "Why are percentile latencies (p50, p95, p99) more useful than average latency?",
      "answer": "Averages can be misleading because they hide the distribution of latencies. A service with 50ms average could have most requests completing in 10ms but a long tail with some taking 2 seconds. Percentiles reveal the full picture: p50 (median) shows what the typical user experiences, p95 shows what the vast majority experience, and p99 captures tail latency affecting 1% of users. In large-scale systems, even p99 issues affect millions of users daily. High-percentile latencies often indicate resource contention, garbage collection pauses, or downstream dependency issues that averages completely mask."
    },
    {
      "question": "What are the benefits of structured logging over unstructured text logs?",
      "answer": "Structured logs (typically JSON key-value pairs) offer several advantages: 1) Machine parsability — log aggregation tools can automatically index and query fields without regex. 2) Consistent schema — every log entry has standard fields (timestamp, level, service, trace_id) enabling reliable filtering. 3) Contextual correlation — trace IDs and request IDs link related logs across services. 4) Efficient storage — columnar storage can compress structured fields better. 5) Powerful analytics — SQL-like queries, aggregations, and dashboards can be built on structured fields. 6) Reduced alert noise — alerts can target specific field values rather than text pattern matching."
    },
    {
      "question": "How does distributed tracing work and what problems does it solve?",
      "answer": "Distributed tracing tracks a request's journey across multiple services by propagating a trace context (trace ID and span ID) through request headers (e.g., W3C Trace Context or B3 headers). Each service creates spans representing units of work, recording start time, duration, status, and metadata. Spans are linked via parent-child relationships to form a trace tree. This solves: 1) Understanding request flow across microservices. 2) Identifying which service causes latency (critical path analysis). 3) Detecting cascading failures. 4) Debugging specific request failures. Tools like Jaeger, Zipkin, and OpenTelemetry Collector aggregate spans into complete traces for visualization."
    },
    {
      "question": "What causes alerting fatigue and how can it be prevented?",
      "answer": "Alerting fatigue occurs when on-call engineers receive too many alerts, leading to desensitization and missed critical issues. Causes include: overly sensitive thresholds, alerting on causes instead of symptoms, lack of deduplication, and alerts without actionability. Prevention strategies: 1) Alert on symptoms that impact users, not internal causes. 2) Use multi-window, multi-burn-rate alerting for SLO-based monitoring. 3) Every alert must have a runbook and be actionable. 4) Regularly review and prune alerts — delete those that haven't been actionable. 5) Implement severity levels (page vs ticket). 6) Use anomaly detection instead of static thresholds where appropriate."
    },
    {
      "question": "What is the difference between the RED and USE monitoring methods?",
      "answer": "The RED method (Rate, Errors, Duration) is designed for request-driven services like APIs and microservices: Rate is requests per second, Errors is the count of failed requests, and Duration is the distribution of request latencies (histograms). The USE method (Utilization, Saturation, Errors) is designed for infrastructure resources: Utilization is the percentage of time a resource is busy, Saturation is the degree of queued work (backlog), and Errors is the count of error events. Use RED for your services (application layer) and USE for your infrastructure (CPU, memory, disk, network)."
    },
    {
      "question": "What are error budgets and how do they influence engineering decisions?",
      "answer": "An error budget is the inverse of an SLO — it quantifies how much unreliability is acceptable. For a 99.9% availability SLO, the error budget is 0.1%, which equals 43.2 minutes of downtime per 30-day window. When the error budget is healthy (plenty remaining), teams can deploy more aggressively, run experiments, and prioritize features. When the budget is nearly exhausted, teams should freeze risky deployments and focus on reliability improvements. This creates a data-driven negotiation framework between product (velocity) and SRE (reliability) teams. Error budget policies should define actions at different consumption thresholds (e.g., 50%, 75%, 100% consumed)."
    },
    {
      "question": "What are the key challenges of monitoring in a microservices architecture?",
      "answer": "Key challenges include: 1) Volume — many services generate massive amounts of telemetry data, requiring sampling strategies and efficient storage. 2) Correlation — linking metrics, logs, and traces across services requires consistent context propagation (trace IDs, correlation IDs). 3) Dynamic topology — services scale up/down, making static dashboards insufficient; need service discovery integration. 4) Ownership boundaries — each team owns their service's monitoring but cross-team issues require unified observability. 5) Cardinality explosion — per-service, per-endpoint, per-customer labels can create millions of time series. 6) Cascading failures — one failing service may cause symptoms across many others, making root cause identification difficult without distributed tracing."
    },
    {
      "question": "What are best practices for on-call and incident response?",
      "answer": "Best practices include: 1) Define clear severity levels (SEV1-SEV4) with response time expectations. 2) Maintain runbooks for every alert with diagnostic steps and mitigation actions. 3) Implement an incident command structure with roles (Incident Commander, Communications Lead, Subject Matter Experts). 4) Use status pages for external communication. 5) Automate common mitigations like rollbacks and traffic shifting. 6) Conduct blameless postmortems for all significant incidents, focusing on systemic improvements. 7) Track Mean Time to Detect (MTTD), Mean Time to Acknowledge (MTTA), and Mean Time to Resolve (MTTR). 8) Rotate on-call fairly and provide compensation. 9) Follow up on action items from postmortems with deadlines."
    }
  ],
  "exercises": [
    {
      "type": "design",
      "question": "Design a monitoring system for a microservices application with 50 services. How would you collect, store, and visualize metrics, logs, and traces?",
      "answer": "Use OpenTelemetry SDK in each service to emit metrics, logs, and traces. Deploy an OTel Collector as a sidecar or daemonset to receive, process, and export telemetry. Metrics go to Prometheus (or Thanos for long-term storage) with Grafana dashboards. Logs go to Loki or Elasticsearch with structured JSON format. Traces go to Jaeger or Tempo. All three are correlated via trace IDs — Grafana can link from metrics to traces to logs. Use exemplars on metrics to attach trace IDs. Implement sampling (head-based for high-throughput services, tail-based to keep interesting traces). Store hot data for 15 days, cold data for 1 year. Set up alerting via Alertmanager with PagerDuty integration."
    },
    {
      "type": "scenario",
      "question": "Your p99 latency suddenly jumped from 100ms to 2 seconds but p50 remains at 15ms. Average latency increased only slightly. How do you investigate?",
      "answer": "The p99 spike with stable p50 indicates a tail latency issue affecting ~1% of requests. Steps: 1) Check if the spike correlates with deployment, traffic increase, or infrastructure event. 2) Break down p99 by endpoint — is it one endpoint or all? 3) Examine traces for high-latency requests — which service/span is slow? 4) Check resource saturation: CPU throttling, memory pressure, disk I/O, garbage collection pauses. 5) Look for noisy neighbor effects in shared infrastructure. 6) Check downstream dependency latency. 7) Examine if the slow requests share characteristics (specific user, region, payload size). Common causes: GC pauses, connection pool exhaustion, thread starvation, or a slow downstream service."
    },
    {
      "type": "estimation",
      "question": "Your platform generates 100,000 requests per second across 30 services. Estimate the storage needed for traces if you sample at 1% with an average of 8 spans per trace, and each span is 500 bytes.",
      "answer": "At 1% sampling: 100,000 * 0.01 = 1,000 sampled traces per second. Each trace has 8 spans at 500 bytes = 4,000 bytes per trace. Per second: 1,000 * 4,000 = 4 MB/s. Per day: 4 MB * 86,400 = ~345 GB/day. Per 15-day retention: ~5.2 TB. With compression (typically 5-10x for span data): ~500 GB to 1 TB for 15 days. Add indexing overhead (~30%): ~650 GB to 1.3 TB total. This is manageable with Jaeger + Elasticsearch or Grafana Tempo with object storage."
    },
    {
      "type": "debug",
      "question": "Your alerting system fires 200 alerts per day but only 10 are actionable. The on-call engineer is overwhelmed. How do you fix this?",
      "answer": "This is classic alert fatigue. Steps: 1) Audit all 200 daily alerts — categorize as actionable, informational, or false positive. 2) Delete or convert non-actionable alerts to dashboard panels. 3) Increase thresholds or add duration requirements (alert only if condition persists for 5+ minutes). 4) Replace static thresholds with anomaly detection for variable-traffic services. 5) Implement alert grouping and deduplication in Alertmanager. 6) Switch from cause-based to symptom-based alerting — instead of 'CPU > 80%', alert on 'error rate > SLO threshold'. 7) Add SLO-based multi-burn-rate alerts. 8) Establish a weekly alert review process to continuously tune."
    },
    {
      "type": "tricky",
      "question": "Your SLO is 99.9% availability (43 minutes error budget per month). You've used 30 minutes in the first week. Should you freeze deployments?",
      "answer": "Not necessarily — it depends on context. 30 minutes used in week 1 means 13 minutes remain for 3 weeks. The burn rate is 30/10080 minutes = 0.3%, but linear projection suggests you'll exhaust the budget by ~week 2. However, if the 30 minutes came from a single incident that's been resolved and won't recur, the risk may be low. Best approach: 1) Analyze if the budget consumption was from a one-time event or systemic issue. 2) If systemic, freeze risky deployments. 3) If one-time, implement preventive measures and continue cautiously. 4) Increase deployment validation (canary, staged rollouts). Follow your documented error budget policy rather than making ad-hoc decisions."
    },
    {
      "type": "design",
      "question": "Design an SLO monitoring and error budget tracking system. What components are needed and how do you calculate burn rate?",
      "answer": "Components: 1) SLO definition store (SLI metric, target, window). 2) Metrics pipeline (Prometheus recording rules) to compute SLI values continuously. 3) Error budget calculator: budget = 1 - SLO target; remaining = budget - (1 - actual_SLI). 4) Burn rate = error budget consumed / time elapsed. A burn rate of 1 means you'll exactly exhaust the budget by window end. Burn rate > 1 means you're consuming faster than sustainable. 5) Multi-window alerts: fast burn (14.4x over 1 hour) for SEV1, slow burn (3x over 6 hours) for ticket. 6) Dashboard showing budget remaining, burn rate trend, and projected exhaustion date. 7) Automated policy actions (deployment freeze) at configurable thresholds."
    },
    {
      "type": "explain",
      "question": "Explain the difference between black-box and white-box monitoring and when to use each.",
      "answer": "Black-box monitoring tests externally visible behavior — synthetic probes, health checks, uptime monitoring (e.g., pinging /health from an external location). It tells you what users experience. White-box monitoring uses internal instrumentation — application metrics, traces, and logs. It tells you why something is happening. Use black-box for: SLA compliance, detecting outages users would notice, validating external connectivity, cross-region availability. Use white-box for: debugging root causes, capacity planning, performance optimization, detecting degradation before it affects users. Best practice: use both. Black-box catches issues white-box misses (network, DNS, CDN), while white-box provides the detail needed for diagnosis."
    },
    {
      "type": "scenario",
      "question": "After migrating from a monolith to microservices, you notice that total request latency increased by 3x despite each individual service being fast. What's happening and how do you diagnose it?",
      "answer": "This is likely due to serialized network hops (fan-out latency). In the monolith, operations were in-process function calls. In microservices, each call adds network latency (1-5ms per hop), serialization/deserialization overhead, and potential queuing. Diagnosis: 1) Use distributed tracing to visualize the request waterfall — identify sequential calls that could be parallelized. 2) Check for N+1 call patterns (service A calling service B in a loop). 3) Look for missing caches between services. 4) Measure network latency between services. Solutions: parallelize independent service calls, implement caching, use batch APIs, consider a BFF (Backend for Frontend) to reduce round trips, or merge overly granular services."
    },
    {
      "type": "estimation",
      "question": "You need to store metrics for 50 services, each emitting 200 unique time series, scraped every 15 seconds, with 30-day retention. Estimate the storage needed.",
      "answer": "Total time series: 50 * 200 = 10,000. Samples per series per day: 86,400 / 15 = 5,760. Total samples per day: 10,000 * 5,760 = 57.6 million. Prometheus uses ~1-2 bytes per sample with compression. At 1.5 bytes average: 57.6M * 1.5 = 86.4 MB/day. For 30 days: 86.4 * 30 = ~2.6 GB. With index and metadata overhead (~50%): ~3.9 GB. This is very manageable. However, cardinality matters — if labels create 10x more series (per-endpoint, per-status-code), storage grows to ~39 GB. With high cardinality (per-customer labels): could explode to TB-scale."
    },
    {
      "type": "debug",
      "question": "Your Prometheus instance is consuming 50 GB of memory and crashing. Metrics storage on disk is only 10 GB. What's happening?",
      "answer": "Prometheus holds the most recent data (head block, typically 2 hours) in memory, and high cardinality labels are the most common cause of memory explosion. Diagnosis: 1) Check total time series count via `prometheus_tsdb_head_series` metric — if it's millions, you have a cardinality problem. 2) Use `topk` or `count by` queries to find high-cardinality labels (e.g., user_id, request_id used as labels). 3) Check for unbounded label values (IP addresses, UUIDs). Fix: 1) Remove high-cardinality labels from metrics (move to logs/traces). 2) Use recording rules to pre-aggregate. 3) Implement relabeling rules to drop unwanted series. 4) Increase scrape interval for verbose endpoints. 5) Consider sharding Prometheus or using Thanos/Cortex for horizontal scaling."
    }
  ],
  "programExercises": [
    {
      "question": "Program 1: Implement a Metrics Collector with counters, gauges, and histograms",
      "code": "class MetricsCollector {\n  constructor() {\n    this.counters = new Map();\n    this.gauges = new Map();\n    this.histograms = new Map();\n  }\n\n  incCounter(name, val = 1) {\n    this.counters.set(name, (this.counters.get(name) || 0) + val);\n  }\n\n  setGauge(name, val) {\n    this.gauges.set(name, val);\n  }\n\n  observe(name, val) {\n    if (!this.histograms.has(name)) this.histograms.set(name, []);\n    this.histograms.get(name).push(val);\n  }\n\n  snapshot() {\n    const result = {};\n    for (const [k, v] of this.counters) result[`counter:${k}`] = v;\n    for (const [k, v] of this.gauges) result[`gauge:${k}`] = v;\n    for (const [k, v] of this.histograms) {\n      const sorted = [...v].sort((a, b) => a - b);\n      const sum = sorted.reduce((a, b) => a + b, 0);\n      result[`histogram:${k}`] = {\n        count: sorted.length,\n        sum: sum,\n        avg: Math.round((sum / sorted.length) * 100) / 100,\n        min: sorted[0],\n        max: sorted[sorted.length - 1]\n      };\n    }\n    return result;\n  }\n}\n\nconst m = new MetricsCollector();\nm.incCounter('requests', 5);\nm.incCounter('requests', 3);\nm.incCounter('errors', 1);\nm.setGauge('active_conns', 42);\nm.setGauge('cpu_pct', 73.2);\nm.observe('latency_ms', 10);\nm.observe('latency_ms', 25);\nm.observe('latency_ms', 50);\nm.observe('latency_ms', 200);\n\nconsole.log(JSON.stringify(m.snapshot(), null, 2));",
      "output": "{\n  \"counter:requests\": 8,\n  \"counter:errors\": 1,\n  \"gauge:active_conns\": 42,\n  \"gauge:cpu_pct\": 73.2,\n  \"histogram:latency_ms\": {\n    \"count\": 4,\n    \"sum\": 285,\n    \"avg\": 71.25,\n    \"min\": 10,\n    \"max\": 200\n  }\n}"
    },
    {
      "question": "Program 2: Implement a Percentile Calculator with histogram buckets",
      "code": "class PercentileCalculator {\n  constructor() {\n    this.values = [];\n  }\n\n  record(value) {\n    this.values.push(value);\n  }\n\n  percentile(p) {\n    if (this.values.length === 0) return null;\n    const sorted = [...this.values].sort((a, b) => a - b);\n    const idx = Math.ceil((p / 100) * sorted.length) - 1;\n    return sorted[Math.max(0, idx)];\n  }\n\n  distribution(buckets) {\n    const result = {};\n    for (const b of buckets) {\n      result[`le_${b}`] = this.values.filter(v => v <= b).length;\n    }\n    result['le_Inf'] = this.values.length;\n    return result;\n  }\n\n  summary() {\n    return {\n      p50: this.percentile(50),\n      p90: this.percentile(90),\n      p95: this.percentile(95),\n      p99: this.percentile(99),\n      count: this.values.length\n    };\n  }\n}\n\nconst calc = new PercentileCalculator();\nconst latencies = [5, 8, 12, 15, 20, 25, 30, 50, 80, 100, 120, 150, 200, 300, 500, 800, 1000, 1200, 1500, 2000];\nlatencies.forEach(v => calc.record(v));\n\nconsole.log('Summary:', calc.summary());\nconsole.log('Buckets:', calc.distribution([10, 50, 100, 250, 500, 1000]));",
      "output": "Summary: { p50: 100, p90: 1200, p95: 1500, p99: 2000, count: 20 }\nBuckets: {\n  le_10: 2,\n  le_50: 8,\n  le_100: 10,\n  le_250: 14,\n  le_500: 15,\n  le_1000: 17,\n  le_Inf: 20\n}"
    },
    {
      "question": "Program 3: Implement an Alert Evaluator with static thresholds and duration",
      "code": "class AlertEvaluator {\n  constructor(rules) {\n    this.rules = rules; // { name, metric, operator, threshold, forDuration }\n    this.activeAlerts = new Map();\n    this.firedAlerts = [];\n  }\n\n  evaluate(metrics, timestamp) {\n    for (const rule of this.rules) {\n      const value = metrics[rule.metric];\n      if (value === undefined) continue;\n\n      const condition =\n        rule.operator === '>' ? value > rule.threshold :\n        rule.operator === '<' ? value < rule.threshold :\n        rule.operator === '>=' ? value >= rule.threshold :\n        value <= rule.threshold;\n\n      if (condition) {\n        if (!this.activeAlerts.has(rule.name)) {\n          this.activeAlerts.set(rule.name, { since: timestamp, value });\n        }\n        const alert = this.activeAlerts.get(rule.name);\n        const duration = timestamp - alert.since;\n        if (duration >= rule.forDuration) {\n          this.firedAlerts.push({\n            alert: rule.name,\n            value,\n            threshold: rule.threshold,\n            duration: `${duration}s`,\n            firedAt: timestamp\n          });\n          this.activeAlerts.delete(rule.name);\n        }\n      } else {\n        this.activeAlerts.delete(rule.name);\n      }\n    }\n    return this.firedAlerts;\n  }\n}\n\nconst evaluator = new AlertEvaluator([\n  { name: 'HighErrorRate', metric: 'error_rate', operator: '>', threshold: 5, forDuration: 60 },\n  { name: 'HighLatency', metric: 'p99_latency', operator: '>', threshold: 500, forDuration: 30 },\n  { name: 'LowAvailability', metric: 'availability', operator: '<', threshold: 99.9, forDuration: 0 }\n]);\n\n// Simulate evaluation cycles\nevaluator.evaluate({ error_rate: 3, p99_latency: 200, availability: 99.95 }, 0);\nevaluator.evaluate({ error_rate: 7, p99_latency: 600, availability: 99.8 }, 30);\nevaluator.evaluate({ error_rate: 8, p99_latency: 700, availability: 99.7 }, 60);\nconst fired = evaluator.evaluate({ error_rate: 9, p99_latency: 800, availability: 99.5 }, 90);\n\nconsole.log('Fired alerts:');\nfired.forEach(a => console.log(`  ${a.alert}: value=${a.value}, threshold=${a.threshold}, duration=${a.duration}`));",
      "output": "Fired alerts:\n  LowAvailability: value=99.8, threshold=99.9, duration=0s\n  HighLatency: value=700, threshold=500, duration=30s\n  LowAvailability: value=99.7, threshold=99.9, duration=0s\n  HighErrorRate: value=9, threshold=5, duration=60s\n  LowAvailability: value=99.5, threshold=99.9, duration=0s"
    },
    {
      "question": "Program 4: Implement an SLO Tracker with error budget consumption",
      "code": "class SLOTracker {\n  constructor(name, target, windowDays) {\n    this.name = name;\n    this.target = target;               // e.g., 99.9\n    this.windowMs = windowDays * 86400000;\n    this.totalRequests = 0;\n    this.failedRequests = 0;\n    this.startTime = null;\n  }\n\n  record(success, timestamp) {\n    if (!this.startTime) this.startTime = timestamp;\n    this.totalRequests++;\n    if (!success) this.failedRequests++;\n  }\n\n  currentSLI() {\n    if (this.totalRequests === 0) return 100;\n    return ((this.totalRequests - this.failedRequests) / this.totalRequests) * 100;\n  }\n\n  errorBudgetTotal() {\n    return 100 - this.target; // percentage\n  }\n\n  errorBudgetConsumed() {\n    if (this.totalRequests === 0) return 0;\n    const errorRate = (this.failedRequests / this.totalRequests) * 100;\n    return (errorRate / this.errorBudgetTotal()) * 100;\n  }\n\n  errorBudgetRemaining() {\n    return Math.max(0, 100 - this.errorBudgetConsumed());\n  }\n\n  burnRate(currentTimeMs) {\n    if (!this.startTime) return 0;\n    const elapsed = currentTimeMs - this.startTime;\n    const windowFraction = elapsed / this.windowMs;\n    if (windowFraction === 0) return 0;\n    return this.errorBudgetConsumed() / (windowFraction * 100);\n  }\n\n  report(currentTimeMs) {\n    return {\n      slo: `${this.name}: ${this.target}%`,\n      currentSLI: `${this.currentSLI().toFixed(3)}%`,\n      totalRequests: this.totalRequests,\n      failedRequests: this.failedRequests,\n      errorBudgetTotal: `${this.errorBudgetTotal()}%`,\n      errorBudgetConsumed: `${this.errorBudgetConsumed().toFixed(1)}%`,\n      errorBudgetRemaining: `${this.errorBudgetRemaining().toFixed(1)}%`,\n      burnRate: this.burnRate(currentTimeMs).toFixed(2)\n    };\n  }\n}\n\nconst slo = new SLOTracker('API Availability', 99.9, 30);\n\n// Simulate: 10000 requests, 15 failures over 1 day\nconst dayMs = 86400000;\nfor (let i = 0; i < 10000; i++) {\n  slo.record(i % 667 !== 0, dayMs); // ~15 failures\n}\n\nconsole.log(JSON.stringify(slo.report(dayMs), null, 2));",
      "output": "{\n  \"slo\": \"API Availability: 99.9%\",\n  \"currentSLI\": \"99.850%\",\n  \"totalRequests\": 10000,\n  \"failedRequests\": 15,\n  \"errorBudgetTotal\": \"0.1%\",\n  \"errorBudgetConsumed\": \"150.0%\",\n  \"errorBudgetRemaining\": \"0.0%\",\n  \"burnRate\": \"45.00\"\n}"
    },
    {
      "question": "Program 5: Implement a Structured Logger with severity levels and context",
      "code": "class StructuredLogger {\n  constructor(service, defaultContext = {}) {\n    this.service = service;\n    this.defaultContext = defaultContext;\n    this.logs = [];\n  }\n\n  _log(level, message, context = {}) {\n    const entry = {\n      timestamp: new Date().toISOString(),\n      level,\n      service: this.service,\n      message,\n      ...this.defaultContext,\n      ...context\n    };\n    this.logs.push(entry);\n    return entry;\n  }\n\n  debug(msg, ctx) { return this._log('DEBUG', msg, ctx); }\n  info(msg, ctx) { return this._log('INFO', msg, ctx); }\n  warn(msg, ctx) { return this._log('WARN', msg, ctx); }\n  error(msg, ctx) { return this._log('ERROR', msg, ctx); }\n\n  child(context) {\n    return new StructuredLogger(this.service, { ...this.defaultContext, ...context });\n  }\n\n  query(filters) {\n    return this.logs.filter(log => {\n      return Object.entries(filters).every(([key, value]) => log[key] === value);\n    });\n  }\n}\n\nconst logger = new StructuredLogger('api-gateway', { env: 'production' });\n\n// Create child logger with request context\nconst reqLogger = logger.child({ traceId: 'abc-123', requestId: 'req-456' });\n\nreqLogger.info('Request received', { method: 'GET', path: '/api/users' });\nreqLogger.info('Auth validated', { userId: 'user-789' });\nreqLogger.warn('Slow query detected', { duration_ms: 1500, query: 'SELECT * FROM users' });\nreqLogger.error('Downstream timeout', { service: 'user-service', timeout_ms: 5000 });\n\n// Query logs\nconst errors = logger.query({ level: 'ERROR' });\nconsole.log(`Total logs: ${logger.logs.length}`);\nconsole.log(`Errors: ${errors.length}`);\nconsole.log('Error details:', JSON.stringify(errors.map(e => ({\n  message: e.message,\n  traceId: e.traceId,\n  service: e.service\n})), null, 2));",
      "output": "Total logs: 4\nErrors: 1\nError details: [\n  {\n    \"message\": \"Downstream timeout\",\n    \"traceId\": \"abc-123\",\n    \"service\": \"api-gateway\"\n  }\n]"
    },
    {
      "question": "Program 6: Implement a Trace Span Manager for distributed tracing",
      "code": "class SpanManager {\n  constructor() {\n    this.traces = new Map();\n  }\n\n  startTrace(traceId) {\n    this.traces.set(traceId, []);\n    return traceId;\n  }\n\n  startSpan(traceId, spanId, parentSpanId, operation, service) {\n    const span = {\n      traceId,\n      spanId,\n      parentSpanId: parentSpanId || null,\n      operation,\n      service,\n      startTime: Date.now(),\n      endTime: null,\n      duration: null,\n      status: 'IN_PROGRESS',\n      tags: {}\n    };\n    if (!this.traces.has(traceId)) this.traces.set(traceId, []);\n    this.traces.get(traceId).push(span);\n    return span;\n  }\n\n  endSpan(traceId, spanId, status = 'OK') {\n    const spans = this.traces.get(traceId);\n    const span = spans.find(s => s.spanId === spanId);\n    if (span) {\n      span.endTime = Date.now();\n      span.duration = span.endTime - span.startTime;\n      span.status = status;\n    }\n    return span;\n  }\n\n  getTrace(traceId) {\n    const spans = this.traces.get(traceId) || [];\n    const totalDuration = spans.reduce((max, s) => Math.max(max, s.duration || 0), 0);\n    return {\n      traceId,\n      spanCount: spans.length,\n      totalDuration: `${totalDuration}ms`,\n      spans: spans.map(s => ({\n        spanId: s.spanId,\n        parent: s.parentSpanId,\n        operation: s.operation,\n        service: s.service,\n        duration: `${s.duration}ms`,\n        status: s.status\n      }))\n    };\n  }\n}\n\nconst mgr = new SpanManager();\nconst traceId = 'trace-001';\nmgr.startTrace(traceId);\n\n// Simulate a request flow\nmgr.startSpan(traceId, 'span-1', null, 'HTTP GET /order', 'api-gateway');\nmgr.startSpan(traceId, 'span-2', 'span-1', 'getOrder', 'order-service');\nmgr.startSpan(traceId, 'span-3', 'span-2', 'SELECT orders', 'postgres');\n\n// End spans in reverse order\nmgr.endSpan(traceId, 'span-3', 'OK');\nmgr.endSpan(traceId, 'span-2', 'OK');\nmgr.endSpan(traceId, 'span-1', 'OK');\n\nconst trace = mgr.getTrace(traceId);\nconsole.log(`Trace ${trace.traceId}: ${trace.spanCount} spans`);\ntrace.spans.forEach(s => {\n  const indent = s.parent ? (s.parent === 'span-1' ? '  ' : '    ') : '';\n  console.log(`${indent}[${s.service}] ${s.operation} - ${s.status}`);\n});",
      "output": "Trace trace-001: 3 spans\n[api-gateway] HTTP GET /order - OK\n  [order-service] getOrder - OK\n    [postgres] SELECT orders - OK"
    },
    {
      "question": "Program 7: Implement an Error Budget Calculator for multiple SLOs",
      "code": "class ErrorBudgetCalculator {\n  constructor(windowDays) {\n    this.windowMinutes = windowDays * 24 * 60;\n    this.slos = [];\n  }\n\n  addSLO(name, targetPercent) {\n    this.slos.push({ name, target: targetPercent });\n  }\n\n  calculate(name, actualPercent) {\n    const slo = this.slos.find(s => s.name === name);\n    if (!slo) return null;\n\n    const budgetPercent = 100 - slo.target;\n    const budgetMinutes = (budgetPercent / 100) * this.windowMinutes;\n    const actualErrorPercent = 100 - actualPercent;\n    const consumedMinutes = (actualErrorPercent / 100) * this.windowMinutes;\n    const remainingMinutes = budgetMinutes - consumedMinutes;\n    const consumedPercent = (consumedMinutes / budgetMinutes) * 100;\n\n    return {\n      slo: `${slo.name} (${slo.target}%)`,\n      actual: `${actualPercent}%`,\n      budgetTotal: `${budgetMinutes.toFixed(1)} min`,\n      budgetConsumed: `${consumedMinutes.toFixed(1)} min (${consumedPercent.toFixed(1)}%)`,\n      budgetRemaining: `${Math.max(0, remainingMinutes).toFixed(1)} min`,\n      status: consumedPercent > 100 ? 'BUDGET_EXHAUSTED' :\n              consumedPercent > 75 ? 'WARNING' : 'HEALTHY'\n    };\n  }\n}\n\nconst calc = new ErrorBudgetCalculator(30); // 30-day window\ncalc.addSLO('API Availability', 99.9);\ncalc.addSLO('Payment Success', 99.99);\ncalc.addSLO('Page Load', 99.5);\n\nconsole.log(calc.calculate('API Availability', 99.85));\nconsole.log(calc.calculate('Payment Success', 99.95));\nconsole.log(calc.calculate('Page Load', 99.6));",
      "output": "{\n  slo: 'API Availability (99.9%)',\n  actual: '99.85%',\n  budgetTotal: '43.2 min',\n  budgetConsumed: '64.8 min (150.0%)',\n  budgetRemaining: '0.0 min',\n  status: 'BUDGET_EXHAUSTED'\n}\n{\n  slo: 'Payment Success (99.99%)',\n  actual: '99.95%',\n  budgetTotal: '4.3 min',\n  budgetConsumed: '21.6 min (500.0%)',\n  budgetRemaining: '0.0 min',\n  status: 'BUDGET_EXHAUSTED'\n}\n{\n  slo: 'Page Load (99.5%)',\n  actual: '99.6%',\n  budgetTotal: '216.0 min',\n  budgetConsumed: '172.8 min (80.0%)',\n  budgetRemaining: '43.2 min',\n  status: 'WARNING'\n}"
    },
    {
      "question": "Program 8: Implement a Dashboard Data Aggregator with time-series rollups",
      "code": "class DashboardAggregator {\n  constructor() {\n    this.dataPoints = [];\n  }\n\n  addPoint(metric, value, timestamp) {\n    this.dataPoints.push({ metric, value, timestamp });\n  }\n\n  rollup(metric, intervalSec) {\n    const points = this.dataPoints\n      .filter(p => p.metric === metric)\n      .sort((a, b) => a.timestamp - b.timestamp);\n\n    if (points.length === 0) return [];\n\n    const buckets = new Map();\n    for (const p of points) {\n      const bucketKey = Math.floor(p.timestamp / intervalSec) * intervalSec;\n      if (!buckets.has(bucketKey)) buckets.set(bucketKey, []);\n      buckets.get(bucketKey).push(p.value);\n    }\n\n    return Array.from(buckets.entries()).map(([time, values]) => {\n      const sum = values.reduce((a, b) => a + b, 0);\n      return {\n        time,\n        avg: Math.round((sum / values.length) * 100) / 100,\n        min: Math.min(...values),\n        max: Math.max(...values),\n        count: values.length\n      };\n    });\n  }\n\n  topN(metric, n) {\n    const points = this.dataPoints.filter(p => p.metric === metric);\n    return points\n      .sort((a, b) => b.value - a.value)\n      .slice(0, n)\n      .map(p => ({ value: p.value, time: p.timestamp }));\n  }\n}\n\nconst dash = new DashboardAggregator();\n\n// Simulate 1 minute of latency data (points every 10 seconds)\nconst latencies = [45, 52, 38, 120, 55, 200];\nlatencies.forEach((v, i) => dash.addPoint('latency', v, i * 10));\n\n// Add request counts\n[100, 150, 120, 80, 200, 180].forEach((v, i) => dash.addPoint('requests', v, i * 10));\n\nconsole.log('Latency rollup (30s intervals):');\nconsole.log(dash.rollup('latency', 30));\n\nconsole.log('\\nTop 3 latency spikes:');\nconsole.log(dash.topN('latency', 3));",
      "output": "Latency rollup (30s intervals):\n[\n  { time: 0, avg: 45, min: 38, max: 52, count: 3 },\n  { time: 30, avg: 125, min: 55, max: 200, count: 3 }\n]\n\nTop 3 latency spikes:\n[\n  { value: 200, time: 50 },\n  { value: 120, time: 30 },\n  { value: 55, time: 40 }\n]"
    },
    {
      "question": "Program 9: Implement an Anomaly Detector using moving average and standard deviation",
      "code": "class AnomalyDetector {\n  constructor(windowSize, sensitivityMultiplier) {\n    this.windowSize = windowSize;\n    this.sensitivity = sensitivityMultiplier; // number of std devs\n    this.history = [];\n  }\n\n  _mean(arr) {\n    return arr.reduce((a, b) => a + b, 0) / arr.length;\n  }\n\n  _stdDev(arr) {\n    const avg = this._mean(arr);\n    const squareDiffs = arr.map(v => Math.pow(v - avg, 2));\n    return Math.sqrt(this._mean(squareDiffs));\n  }\n\n  addPoint(value) {\n    const result = { value, anomaly: false, reason: null };\n\n    if (this.history.length >= this.windowSize) {\n      const window = this.history.slice(-this.windowSize);\n      const mean = this._mean(window);\n      const stdDev = this._stdDev(window);\n      const upperBound = Math.round((mean + this.sensitivity * stdDev) * 100) / 100;\n      const lowerBound = Math.round((mean - this.sensitivity * stdDev) * 100) / 100;\n\n      if (value > upperBound) {\n        result.anomaly = true;\n        result.reason = `Above upper bound (${upperBound})`;\n      } else if (value < lowerBound) {\n        result.anomaly = true;\n        result.reason = `Below lower bound (${lowerBound})`;\n      }\n\n      result.mean = Math.round(mean * 100) / 100;\n      result.stdDev = Math.round(stdDev * 100) / 100;\n      result.bounds = [lowerBound, upperBound];\n    }\n\n    this.history.push(value);\n    return result;\n  }\n}\n\nconst detector = new AnomalyDetector(5, 2); // 5-point window, 2 std devs\n\nconst values = [50, 52, 48, 51, 49, 53, 47, 200, 45, 5];\nvalues.forEach(v => {\n  const r = detector.addPoint(v);\n  const flag = r.anomaly ? ' << ANOMALY: ' + r.reason : '';\n  console.log(`Value: ${v}${flag}`);\n});",
      "output": "Value: 50\nValue: 52\nValue: 48\nValue: 51\nValue: 49\nValue: 53\nValue: 47\nValue: 200 << ANOMALY: Above upper bound (55.47)\nValue: 45\nValue: 5 << ANOMALY: Below lower bound (-38.96)"
    },
    {
      "question": "Program 10: Implement an Incident Severity Classifier based on impact metrics",
      "code": "class IncidentClassifier {\n  constructor(thresholds) {\n    this.thresholds = thresholds;\n    this.incidents = [];\n  }\n\n  classify(metrics) {\n    let severity = 'SEV4';\n    const triggers = [];\n\n    for (const [sev, rules] of Object.entries(this.thresholds)) {\n      for (const rule of rules) {\n        const value = metrics[rule.metric];\n        if (value === undefined) continue;\n        const triggered =\n          rule.operator === '>' ? value > rule.value :\n          rule.operator === '<' ? value < rule.value :\n          rule.operator === '>=' ? value >= rule.value :\n          value <= rule.value;\n\n        if (triggered) {\n          if (this._sevRank(sev) < this._sevRank(severity)) {\n            severity = sev;\n          }\n          triggers.push(`${rule.metric} ${rule.operator} ${rule.value} (actual: ${value})`);\n        }\n      }\n    }\n\n    const incident = {\n      severity,\n      triggers,\n      action: this._getAction(severity),\n      timestamp: new Date().toISOString()\n    };\n    this.incidents.push(incident);\n    return incident;\n  }\n\n  _sevRank(sev) {\n    return { SEV1: 1, SEV2: 2, SEV3: 3, SEV4: 4 }[sev] || 5;\n  }\n\n  _getAction(sev) {\n    const actions = {\n      SEV1: 'Page on-call immediately. Start incident bridge. Notify stakeholders.',\n      SEV2: 'Page on-call. Begin investigation within 15 minutes.',\n      SEV3: 'Create ticket. Investigate within 4 hours.',\n      SEV4: 'Log for review. Address in next sprint.'\n    };\n    return actions[sev];\n  }\n}\n\nconst classifier = new IncidentClassifier({\n  SEV1: [\n    { metric: 'error_rate', operator: '>', value: 10 },\n    { metric: 'availability', operator: '<', value: 99 }\n  ],\n  SEV2: [\n    { metric: 'error_rate', operator: '>', value: 5 },\n    { metric: 'p99_latency_ms', operator: '>', value: 2000 }\n  ],\n  SEV3: [\n    { metric: 'error_rate', operator: '>', value: 1 },\n    { metric: 'p99_latency_ms', operator: '>', value: 1000 }\n  ]\n});\n\nconsole.log('--- Normal ---');\nconsole.log(classifier.classify({ error_rate: 0.5, availability: 99.99, p99_latency_ms: 200 }));\n\nconsole.log('\\n--- Degraded ---');\nconsole.log(classifier.classify({ error_rate: 3, availability: 99.5, p99_latency_ms: 1500 }));\n\nconsole.log('\\n--- Major Outage ---');\nconsole.log(classifier.classify({ error_rate: 15, availability: 97, p99_latency_ms: 5000 }));",
      "output": "--- Normal ---\n{\n  severity: 'SEV4',\n  triggers: [],\n  action: 'Log for review. Address in next sprint.',\n  timestamp: '2026-02-14T...'\n}\n\n--- Degraded ---\n{\n  severity: 'SEV3',\n  triggers: [\n    'error_rate > 1 (actual: 3)',\n    'p99_latency_ms > 1000 (actual: 1500)'\n  ],\n  action: 'Create ticket. Investigate within 4 hours.',\n  timestamp: '2026-02-14T...'\n}\n\n--- Major Outage ---\n{\n  severity: 'SEV1',\n  triggers: [\n    'error_rate > 10 (actual: 15)',\n    'availability < 99 (actual: 97)',\n    'error_rate > 5 (actual: 15)',\n    'p99_latency_ms > 2000 (actual: 5000)',\n    'error_rate > 1 (actual: 15)',\n    'p99_latency_ms > 1000 (actual: 5000)'\n  ],\n  action: 'Page on-call immediately. Start incident bridge. Notify stakeholders.',\n  timestamp: '2026-02-14T...'\n}"
    }
  ]
}