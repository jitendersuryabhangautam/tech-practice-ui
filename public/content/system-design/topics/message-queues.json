{
  "id": "message-queues",
  "title": "Message Queues & Event Streaming",
  "category": "Foundations",
  "description": "Message queues and event streaming systems decouple producers from consumers, enabling asynchronous communication, load leveling, and fault tolerance in distributed architectures.",
  "explanation": "Message queues are middleware components that facilitate asynchronous communication between services by temporarily storing messages until consumers are ready to process them. They decouple producers from consumers, allowing each to scale independently and improving system resilience.\n\nApache Kafka is a distributed event streaming platform built around the concept of an append-only commit log. Its architecture consists of brokers (servers), topics (logical channels), partitions (ordered sequences within a topic for parallelism), and consumer groups (sets of consumers that share work). Kafka excels at high-throughput, ordered event streaming with configurable retention.\n\nRabbitMQ follows the AMQP model with exchanges (routing logic), queues (message storage), and bindings (rules connecting exchanges to queues). Exchange types include direct, fanout, topic, and headers, each offering different routing strategies.\n\nAmazon SQS provides a fully managed queue service with standard (at-least-once, best-effort ordering) and FIFO (exactly-once, strict ordering) variants.\n\nDelivery semantics are critical: at-most-once means messages may be lost but never duplicated; at-least-once guarantees delivery but may duplicate; exactly-once processing uses idempotent consumers or transactional writes. Dead letter queues (DLQs) capture messages that fail processing after a configured number of retries.\n\nBackpressure mechanisms prevent consumers from being overwhelmed — techniques include pull-based consumption, bounded queues, and flow control signals. Event sourcing persists all state changes as an immutable sequence of events, while CQRS separates read and write models for optimal scaling. Message ordering guarantees vary: Kafka guarantees order within a partition, while SQS standard queues offer best-effort ordering only.",
  "code": "// Simple in-memory message queue implementation\nclass MessageQueue {\n  constructor() {\n    this.queues = new Map();       // topic -> messages[]\n    this.subscribers = new Map();  // topic -> callback[]\n    this.dlq = new Map();          // topic -> failed messages[]\n    this.maxRetries = 3;\n  }\n\n  // Create a topic\n  createTopic(topic) {\n    if (!this.queues.has(topic)) {\n      this.queues.set(topic, []);\n      this.subscribers.set(topic, []);\n      this.dlq.set(topic, []);\n    }\n  }\n\n  // Publish a message to a topic\n  publish(topic, message) {\n    if (!this.queues.has(topic)) this.createTopic(topic);\n    const envelope = {\n      id: Date.now() + Math.random().toString(36),\n      topic,\n      payload: message,\n      timestamp: new Date().toISOString(),\n      retries: 0\n    };\n    this.queues.get(topic).push(envelope);\n    this._notifySubscribers(topic, envelope);\n    return envelope.id;\n  }\n\n  // Subscribe to a topic (pub/sub pattern)\n  subscribe(topic, callback) {\n    if (!this.subscribers.has(topic)) this.createTopic(topic);\n    this.subscribers.get(topic).push(callback);\n  }\n\n  // Pull a message from the queue (point-to-point)\n  consume(topic) {\n    if (!this.queues.has(topic)) return null;\n    return this.queues.get(topic).shift() || null;\n  }\n\n  // Process with retry and DLQ support\n  async processWithRetry(topic, handler) {\n    const msg = this.consume(topic);\n    if (!msg) return;\n    try {\n      await handler(msg.payload);\n    } catch (err) {\n      msg.retries++;\n      if (msg.retries >= this.maxRetries) {\n        this.dlq.get(topic).push({ ...msg, error: err.message });\n        console.log(`Message ${msg.id} moved to DLQ`);\n      } else {\n        this.queues.get(topic).unshift(msg); // re-enqueue\n      }\n    }\n  }\n\n  _notifySubscribers(topic, envelope) {\n    const subs = this.subscribers.get(topic) || [];\n    subs.forEach(cb => cb(envelope.payload));\n  }\n\n  getDLQMessages(topic) {\n    return this.dlq.get(topic) || [];\n  }\n}",
  "example": "// Usage: Message Queue with pub/sub and DLQ\nconst mq = new MessageQueue();\n\n// Subscribe to order events\nmq.subscribe('orders', (msg) => {\n  console.log('Notification service received:', msg);\n});\nmq.subscribe('orders', (msg) => {\n  console.log('Analytics service received:', msg);\n});\n\n// Publish order events\nmq.publish('orders', { orderId: 1, item: 'Laptop', amount: 999 });\nmq.publish('orders', { orderId: 2, item: 'Phone', amount: 699 });\n\n// Point-to-point consumption\nmq.publish('tasks', { taskId: 'a1', action: 'send-email' });\nconst task = mq.consume('tasks');\nconsole.log('Processing task:', task.payload);\n\n// Process with retry logic\nmq.publish('payments', { paymentId: 'p1', amount: 100 });\nawait mq.processWithRetry('payments', async (msg) => {\n  if (Math.random() < 0.5) throw new Error('Payment gateway timeout');\n  console.log('Payment processed:', msg.paymentId);\n});\nconsole.log('DLQ messages:', mq.getDLQMessages('payments'));",
  "useCase": "Used in distributed systems for asynchronous inter-service communication, event-driven architectures, order processing pipelines, and decoupling microservices.",
  "interviewQuestions": [
    {
      "question": "What is the difference between a message queue and an event stream?",
      "answer": "A message queue (e.g., RabbitMQ, SQS) delivers messages to a single consumer and removes them after acknowledgment — it's point-to-point. An event stream (e.g., Kafka) is an append-only log where messages are retained for a configurable period and multiple consumer groups can independently read from any offset. Queues are for task distribution; streams are for event replay and multi-subscriber scenarios."
    },
    {
      "question": "Explain Kafka's architecture: brokers, topics, partitions, and consumer groups.",
      "answer": "A Kafka cluster consists of brokers (servers) that store data. Topics are logical categories for messages. Each topic is divided into partitions — ordered, immutable sequences of records that enable parallel processing. Consumer groups are sets of consumers that divide partition ownership so each partition is consumed by exactly one consumer in the group, enabling horizontal scaling."
    },
    {
      "question": "What are the differences between at-most-once, at-least-once, and exactly-once delivery?",
      "answer": "At-most-once: the message is delivered zero or one time — the producer sends and doesn't retry (risk of loss). At-least-once: the producer retries until acknowledged — the message is delivered one or more times (risk of duplicates). Exactly-once: achieved through idempotent producers and transactional consumers, ensuring each message is processed exactly once despite retries."
    },
    {
      "question": "What is a dead letter queue (DLQ) and when should you use one?",
      "answer": "A DLQ is a separate queue where messages that fail processing after a configured number of retries are sent. It prevents poison messages (messages that always cause errors) from blocking the main queue. Use DLQs for debugging failed messages, manual intervention, and ensuring the main processing pipeline continues unblocked."
    },
    {
      "question": "How does RabbitMQ's exchange model work?",
      "answer": "RabbitMQ routes messages through exchanges to queues using bindings. Exchange types: Direct — routes by exact routing key match; Fanout — broadcasts to all bound queues; Topic — routes by pattern matching on routing keys (e.g., 'order.*'); Headers — routes based on message header attributes. Producers send to exchanges, never directly to queues."
    },
    {
      "question": "What is backpressure and how do messaging systems handle it?",
      "answer": "Backpressure occurs when a producer sends messages faster than consumers can process them. Handling strategies include: pull-based consumption (Kafka consumers pull at their own pace), bounded queues that reject or block producers when full, flow control signals (RabbitMQ credit-based flow control), and consumer-side rate limiting. Without backpressure handling, systems risk memory exhaustion."
    },
    {
      "question": "Explain event sourcing and how it relates to message queues.",
      "answer": "Event sourcing stores all changes to application state as a sequence of immutable events rather than storing current state. The event log (often implemented with Kafka) becomes the source of truth. Current state is derived by replaying events. This pairs with message queues by publishing each event to a topic, enabling other services to react, rebuild state, or create materialized views."
    },
    {
      "question": "What is CQRS and why is it paired with event sourcing?",
      "answer": "CQRS (Command Query Responsibility Segregation) separates the write model (commands that change state) from the read model (queries that return data). It pairs naturally with event sourcing because write events can be projected into optimized read models via message consumers. This allows each side to scale independently and use storage optimized for its access patterns."
    },
    {
      "question": "How does Kafka guarantee message ordering?",
      "answer": "Kafka guarantees ordering only within a single partition. Messages with the same key are routed to the same partition (via key hashing), ensuring ordered processing for that key. Across partitions, there is no global ordering. To maintain order for related events (e.g., all events for a user), use the entity ID as the partition key."
    },
    {
      "question": "How would you implement exactly-once processing in a distributed system?",
      "answer": "Approaches include: 1) Idempotent consumers — store processed message IDs and skip duplicates. 2) Kafka's transactional API — atomic reads, processing, and offset commits. 3) Outbox pattern — write to a database table and a separate process reads the outbox. 4) Deduplication at the consumer using unique message IDs. The key is making the processing step idempotent so replays are safe."
    }
  ],
  "exercises": [
    {
      "type": "design",
      "question": "Design a notification system that sends emails, SMS, and push notifications using message queues. How would you handle failures and ensure no notification is sent twice?",
      "answer": "Use a fanout exchange or separate topic partitions for each channel. Each notification gets a unique ID stored in a deduplication table. Consumers check before sending. Use DLQs for failed sends with exponential backoff retries. Track delivery status per channel. Implement circuit breakers for each provider (email gateway, SMS API)."
    },
    {
      "type": "scenario",
      "question": "Your Kafka consumer group has 8 consumers but only 4 partitions. What happens and how would you fix it?",
      "answer": "4 consumers will be idle because Kafka assigns at most one consumer per partition within a group. Fix: increase the partition count to at least match the consumer count. Note that increasing partitions doesn't redistribute existing data — only new messages will use new partitions. Over-partitioning also has overhead (more file handles, leader elections)."
    },
    {
      "type": "estimation",
      "question": "An e-commerce platform processes 10,000 orders per minute. Each order generates 5 events (created, payment, inventory, shipping, notification). Estimate the throughput requirements for the message queue.",
      "answer": "50,000 messages/minute = ~833 messages/second. Assuming each message is ~1KB, that's ~833 KB/s or ~50 MB/minute. With 3x replication in Kafka, storage write throughput is ~2.5 GB/hour. For 7-day retention: ~420 GB. A single Kafka partition handles ~10 MB/s, so 1 partition suffices for throughput, but use 8-12 partitions for consumer parallelism."
    },
    {
      "type": "debug",
      "question": "Messages are being processed out of order in your Kafka consumer. The partition key is set correctly. What could be causing this?",
      "answer": "Possible causes: 1) Multiple partitions with the same consumer group — ordering is only within a partition. 2) Consumer rebalancing caused offset issues. 3) Retries are reprocessing earlier messages after later ones succeeded. 4) Consumer is processing messages in parallel threads without preserving order. Fix: ensure single partition per ordering key, process sequentially per partition, and handle retries in-order."
    },
    {
      "type": "tricky",
      "question": "Can you achieve exactly-once delivery in a distributed system? Is it theoretically possible?",
      "answer": "True exactly-once delivery is impossible in distributed systems (due to the Two Generals problem). What systems achieve is exactly-once processing semantics — using idempotent operations, transactional writes, and deduplication. Kafka's exactly-once guarantee works within the Kafka ecosystem by coupling producer transactions with consumer offset commits atomically, but end-to-end exactly-once requires idempotent external systems."
    },
    {
      "type": "design",
      "question": "Design a distributed task queue (like Celery) that supports task priorities, retries, and scheduled execution.",
      "answer": "Use multiple queues per priority level (high, medium, low). Workers poll high-priority first. Each task has metadata: retryCount, maxRetries, scheduledAt, timeout. Use a delayed queue (or sorted set in Redis) for scheduled tasks. Failed tasks go to retry queue with exponential backoff. After max retries, move to DLQ. Track task state (pending, running, completed, failed) in a database."
    },
    {
      "type": "scenario",
      "question": "Your RabbitMQ cluster is running out of memory because consumers are slower than producers. What immediate and long-term actions do you take?",
      "answer": "Immediate: Enable flow control / memory alarms to block publishers. Increase consumer count. Set queue TTL to drop old messages if acceptable. Long-term: Implement backpressure at the producer. Add consumer auto-scaling based on queue depth. Set queue length limits with overflow to DLQ. Consider switching to lazy queues (disk-backed) or migrating to Kafka for better backlog handling."
    },
    {
      "type": "explain",
      "question": "Explain the outbox pattern and why it's important for reliable messaging.",
      "answer": "The outbox pattern writes events to an 'outbox' table in the same database transaction as the business data change. A separate process (poller or CDC) reads the outbox and publishes events to the message broker. This ensures atomicity — if the transaction rolls back, no event is published. Without it, you risk the database write succeeding but the message publish failing (or vice versa), leading to inconsistency."
    },
    {
      "type": "estimation",
      "question": "A chat application has 1 million concurrent users, each sending an average of 2 messages per minute. Design the message queue layer.",
      "answer": "2 million messages/minute = ~33,333 messages/second. At ~500 bytes per message: ~16.7 MB/s throughput. Use Kafka with ~50 partitions (each handling ~700 msgs/s). Partition by chat room ID for ordering. With 3x replication: ~50 MB/s disk write. Consumer groups per feature (delivery, notifications, search indexing). Need at least 3-5 broker nodes. 24-hour retention ≈ 1.4 TB."
    },
    {
      "type": "debug",
      "question": "Your SQS FIFO queue is throttling at 300 messages/second even though the documented limit is 3,000. What's wrong?",
      "answer": "SQS FIFO queues have a limit of 300 messages/second per message group ID, not per queue. If all messages use the same message group ID, you hit 300 msg/s. Fix: distribute messages across multiple message group IDs (e.g., use customer ID or order ID). Each unique group ID gets its own 300 msg/s allowance. With 10 distinct group IDs, you can achieve 3,000 msg/s aggregate throughput."
    }
  ],
  "programExercises": [
    {
      "question": "Program 1: Implement a basic publish-subscribe system",
      "code": "class PubSub {\n  constructor() {\n    this.topics = new Map();\n  }\n\n  subscribe(topic, subscriber) {\n    if (!this.topics.has(topic)) this.topics.set(topic, []);\n    this.topics.get(topic).push(subscriber);\n  }\n\n  publish(topic, message) {\n    const subs = this.topics.get(topic) || [];\n    subs.forEach(fn => fn(message));\n    return subs.length;\n  }\n\n  unsubscribe(topic, subscriber) {\n    if (!this.topics.has(topic)) return;\n    const subs = this.topics.get(topic);\n    this.topics.set(topic, subs.filter(fn => fn !== subscriber));\n  }\n}\n\nconst ps = new PubSub();\nconst results = [];\nconst handler1 = (msg) => results.push(`H1: ${msg}`);\nconst handler2 = (msg) => results.push(`H2: ${msg}`);\n\nps.subscribe('news', handler1);\nps.subscribe('news', handler2);\nps.publish('news', 'Breaking!');\nps.unsubscribe('news', handler1);\nps.publish('news', 'Update!');\nconsole.log(results);",
      "output": "[ 'H1: Breaking!', 'H2: Breaking!', 'H2: Update!' ]"
    },
    {
      "question": "Program 2: Implement a message queue with acknowledgment",
      "code": "class AckQueue {\n  constructor() {\n    this.pending = [];\n    this.inflight = new Map();\n    this.visibilityTimeout = 5000;\n  }\n\n  send(message) {\n    this.pending.push({ id: String(this.pending.length + this.inflight.size + 1), body: message });\n  }\n\n  receive() {\n    const msg = this.pending.shift();\n    if (!msg) return null;\n    this.inflight.set(msg.id, { ...msg, receivedAt: Date.now() });\n    return msg;\n  }\n\n  ack(id) {\n    const deleted = this.inflight.delete(id);\n    return deleted;\n  }\n\n  nack(id) {\n    const msg = this.inflight.get(id);\n    if (msg) {\n      this.pending.unshift({ id: msg.id, body: msg.body });\n      this.inflight.delete(id);\n    }\n  }\n\n  stats() {\n    return { pending: this.pending.length, inflight: this.inflight.size };\n  }\n}\n\nconst q = new AckQueue();\nq.send('email-job-1');\nq.send('email-job-2');\nq.send('email-job-3');\n\nconst m1 = q.receive();\nconsole.log('Received:', m1.body);\nconsole.log('Stats after receive:', q.stats());\n\nq.ack(m1.id);\nconsole.log('Stats after ack:', q.stats());\n\nconst m2 = q.receive();\nq.nack(m2.id);\nconsole.log('Stats after nack:', q.stats());",
      "output": "Received: email-job-1\nStats after receive: { pending: 2, inflight: 1 }\nStats after ack: { pending: 2, inflight: 0 }\nStats after nack: { pending: 2, inflight: 0 }"
    },
    {
      "question": "Program 3: Implement a dead letter queue handler",
      "code": "class DLQHandler {\n  constructor(maxRetries = 3) {\n    this.queue = [];\n    this.dlq = [];\n    this.retryCount = new Map();\n    this.maxRetries = maxRetries;\n  }\n\n  enqueue(msg) {\n    this.queue.push(msg);\n    this.retryCount.set(msg.id, 0);\n  }\n\n  process(handler) {\n    const results = [];\n    while (this.queue.length > 0) {\n      const msg = this.queue.shift();\n      try {\n        handler(msg);\n        results.push(`Processed: ${msg.id}`);\n      } catch (e) {\n        const retries = this.retryCount.get(msg.id) + 1;\n        this.retryCount.set(msg.id, retries);\n        if (retries >= this.maxRetries) {\n          this.dlq.push({ ...msg, error: e.message, retries });\n          results.push(`DLQ: ${msg.id} after ${retries} retries`);\n        } else {\n          this.queue.push(msg);\n          results.push(`Retry ${retries}: ${msg.id}`);\n        }\n      }\n    }\n    return results;\n  }\n}\n\nconst handler = new DLQHandler(2);\nhandler.enqueue({ id: 'msg-1', data: 'good' });\nhandler.enqueue({ id: 'msg-2', data: 'bad' });\n\nconst results = handler.process((msg) => {\n  if (msg.data === 'bad') throw new Error('Processing failed');\n});\n\nconsole.log('Results:', results);\nconsole.log('DLQ:', handler.dlq.map(m => m.id));",
      "output": "Results: [ 'Processed: msg-1', 'Retry 1: msg-2', 'DLQ: msg-2 after 2 retries' ]\nDLQ: [ 'msg-2' ]"
    },
    {
      "question": "Program 4: Simulate Kafka partitioning with key-based routing",
      "code": "class KafkaSim {\n  constructor(numPartitions) {\n    this.partitions = Array.from({ length: numPartitions }, () => []);\n    this.numPartitions = numPartitions;\n  }\n\n  hash(key) {\n    let h = 0;\n    for (let i = 0; i < key.length; i++) {\n      h = (h * 31 + key.charCodeAt(i)) % this.numPartitions;\n    }\n    return h;\n  }\n\n  produce(key, value) {\n    const partition = this.hash(key);\n    this.partitions[partition].push({ key, value, offset: this.partitions[partition].length });\n    return partition;\n  }\n\n  consume(partitionId) {\n    return this.partitions[partitionId];\n  }\n\n  getPartitionSizes() {\n    return this.partitions.map((p, i) => ({ partition: i, count: p.length }));\n  }\n}\n\nconst kafka = new KafkaSim(3);\n\nconst keys = ['user-1', 'user-2', 'user-3', 'user-1', 'user-2', 'user-1'];\nkeys.forEach(k => {\n  const p = kafka.produce(k, `event for ${k}`);\n  console.log(`${k} -> partition ${p}`);\n});\n\nconsole.log('\\nPartition sizes:', kafka.getPartitionSizes());\nconsole.log('\\nPartition 0 messages:', kafka.consume(0).map(m => m.key));",
      "output": "user-1 -> partition 2\nuser-2 -> partition 0\nuser-3 -> partition 1\nuser-1 -> partition 2\nuser-2 -> partition 0\nuser-1 -> partition 2\n\nPartition sizes: [ { partition: 0, count: 2 }, { partition: 1, count: 1 }, { partition: 2, count: 3 } ]\n\nPartition 0 messages: [ 'user-2', 'user-2' ]"
    },
    {
      "question": "Program 5: Implement a priority message queue",
      "code": "class PriorityQueue {\n  constructor() {\n    this.queues = { high: [], medium: [], low: [] };\n    this.processed = [];\n  }\n\n  enqueue(message, priority = 'medium') {\n    this.queues[priority].push(message);\n  }\n\n  dequeue() {\n    if (this.queues.high.length > 0) return { ...this.queues.high.shift(), priority: 'high' };\n    if (this.queues.medium.length > 0) return { ...this.queues.medium.shift(), priority: 'medium' };\n    if (this.queues.low.length > 0) return { ...this.queues.low.shift(), priority: 'low' };\n    return null;\n  }\n\n  processAll() {\n    let msg;\n    while ((msg = this.dequeue()) !== null) {\n      this.processed.push(`${msg.priority}: ${msg.task}`);\n    }\n    return this.processed;\n  }\n\n  size() {\n    return Object.values(this.queues).reduce((sum, q) => sum + q.length, 0);\n  }\n}\n\nconst pq = new PriorityQueue();\npq.enqueue({ task: 'backup-logs' }, 'low');\npq.enqueue({ task: 'process-payment' }, 'high');\npq.enqueue({ task: 'send-email' }, 'medium');\npq.enqueue({ task: 'alert-oncall' }, 'high');\npq.enqueue({ task: 'update-cache' }, 'low');\n\nconsole.log('Queue size:', pq.size());\nconsole.log('Processing order:', pq.processAll());",
      "output": "Queue size: 5\nProcessing order: [\n  'high: process-payment',\n  'high: alert-oncall',\n  'medium: send-email',\n  'low: backup-logs',\n  'low: update-cache'\n]"
    },
    {
      "question": "Program 6: Implement an event sourcing store",
      "code": "class EventStore {\n  constructor() {\n    this.events = [];\n  }\n\n  append(aggregateId, eventType, data) {\n    const event = {\n      id: this.events.length + 1,\n      aggregateId,\n      type: eventType,\n      data,\n      timestamp: this.events.length // simplified\n    };\n    this.events.push(event);\n    return event;\n  }\n\n  getEvents(aggregateId) {\n    return this.events.filter(e => e.aggregateId === aggregateId);\n  }\n\n  replay(aggregateId, reducer, initial) {\n    return this.getEvents(aggregateId).reduce(reducer, initial);\n  }\n}\n\nconst store = new EventStore();\n\n// Record account events\nstore.append('acc-1', 'ACCOUNT_CREATED', { owner: 'Alice', balance: 0 });\nstore.append('acc-1', 'MONEY_DEPOSITED', { amount: 100 });\nstore.append('acc-1', 'MONEY_WITHDRAWN', { amount: 30 });\nstore.append('acc-1', 'MONEY_DEPOSITED', { amount: 50 });\n\n// Replay to get current state\nconst state = store.replay('acc-1', (acc, event) => {\n  switch (event.type) {\n    case 'ACCOUNT_CREATED': return { ...event.data };\n    case 'MONEY_DEPOSITED': return { ...acc, balance: acc.balance + event.data.amount };\n    case 'MONEY_WITHDRAWN': return { ...acc, balance: acc.balance - event.data.amount };\n    default: return acc;\n  }\n}, {});\n\nconsole.log('Events:', store.getEvents('acc-1').length);\nconsole.log('Current state:', state);",
      "output": "Events: 4\nCurrent state: { owner: 'Alice', balance: 120 }"
    },
    {
      "question": "Program 7: Implement a message deduplication filter",
      "code": "class DeduplicationFilter {\n  constructor(windowSize = 100) {\n    this.seen = new Set();\n    this.order = [];\n    this.windowSize = windowSize;\n    this.processed = [];\n    this.duplicates = 0;\n  }\n\n  process(messageId, payload) {\n    if (this.seen.has(messageId)) {\n      this.duplicates++;\n      return false; // duplicate\n    }\n    this.seen.add(messageId);\n    this.order.push(messageId);\n    this.processed.push(payload);\n\n    // Evict old entries beyond window\n    if (this.order.length > this.windowSize) {\n      const old = this.order.shift();\n      this.seen.delete(old);\n    }\n    return true; // new message\n  }\n\n  stats() {\n    return {\n      processed: this.processed.length,\n      duplicates: this.duplicates,\n      windowSize: this.seen.size\n    };\n  }\n}\n\nconst dedup = new DeduplicationFilter(50);\n\nconst messages = [\n  { id: 'a1', data: 'order-created' },\n  { id: 'a2', data: 'payment-received' },\n  { id: 'a1', data: 'order-created' },   // duplicate\n  { id: 'a3', data: 'item-shipped' },\n  { id: 'a2', data: 'payment-received' }, // duplicate\n  { id: 'a4', data: 'delivered' },\n];\n\nmessages.forEach(m => {\n  const isNew = dedup.process(m.id, m.data);\n  console.log(`${m.id}: ${isNew ? 'PROCESSED' : 'DUPLICATE'}`);\n});\n\nconsole.log('Stats:', dedup.stats());",
      "output": "a1: PROCESSED\na2: PROCESSED\na1: DUPLICATE\na3: PROCESSED\na2: DUPLICATE\na4: PROCESSED\nStats: { processed: 4, duplicates: 2, windowSize: 4 }"
    },
    {
      "question": "Program 8: Implement a fan-out message dispatcher",
      "code": "class FanOutDispatcher {\n  constructor() {\n    this.exchanges = new Map();\n    this.deliveryLog = [];\n  }\n\n  createExchange(name, type) {\n    this.exchanges.set(name, { type, bindings: [] });\n  }\n\n  bind(exchangeName, queueName, routingKey = '') {\n    const exchange = this.exchanges.get(exchangeName);\n    exchange.bindings.push({ queue: queueName, routingKey });\n  }\n\n  publish(exchangeName, routingKey, message) {\n    const exchange = this.exchanges.get(exchangeName);\n    const delivered = [];\n\n    exchange.bindings.forEach(binding => {\n      let shouldDeliver = false;\n      if (exchange.type === 'fanout') shouldDeliver = true;\n      else if (exchange.type === 'direct') shouldDeliver = binding.routingKey === routingKey;\n      else if (exchange.type === 'topic') {\n        const pattern = new RegExp('^' + binding.routingKey.replace(/\\*/g, '[^.]+').replace(/#/g, '.*') + '$');\n        shouldDeliver = pattern.test(routingKey);\n      }\n      if (shouldDeliver) {\n        delivered.push(binding.queue);\n        this.deliveryLog.push({ queue: binding.queue, message, routingKey });\n      }\n    });\n    return delivered;\n  }\n}\n\nconst dispatcher = new FanOutDispatcher();\n\n// Fanout exchange - broadcasts to all\ndispatcher.createExchange('logs', 'fanout');\ndispatcher.bind('logs', 'file-logger');\ndispatcher.bind('logs', 'console-logger');\ndispatcher.bind('logs', 'metrics');\n\nconst fanoutResult = dispatcher.publish('logs', '', 'System started');\nconsole.log('Fanout delivered to:', fanoutResult);\n\n// Direct exchange - routes by key\ndispatcher.createExchange('tasks', 'direct');\ndispatcher.bind('tasks', 'email-worker', 'email');\ndispatcher.bind('tasks', 'sms-worker', 'sms');\n\nconst directResult = dispatcher.publish('tasks', 'email', 'Send welcome email');\nconsole.log('Direct delivered to:', directResult);",
      "output": "Fanout delivered to: [ 'file-logger', 'console-logger', 'metrics' ]\nDirect delivered to: [ 'email-worker' ]"
    },
    {
      "question": "Program 9: Implement a consumer group simulator",
      "code": "class ConsumerGroup {\n  constructor(groupId, partitions) {\n    this.groupId = groupId;\n    this.consumers = [];\n    this.partitions = partitions; // array of partition arrays\n    this.assignment = new Map();\n    this.consumed = new Map();\n  }\n\n  addConsumer(consumerId) {\n    this.consumers.push(consumerId);\n    this.consumed.set(consumerId, []);\n    this.rebalance();\n  }\n\n  rebalance() {\n    this.assignment.clear();\n    this.consumers.forEach(c => this.assignment.set(c, []));\n    this.partitions.forEach((_, idx) => {\n      const consumer = this.consumers[idx % this.consumers.length];\n      this.assignment.get(consumer).push(idx);\n    });\n  }\n\n  consumeAll() {\n    for (const [consumer, partitionIds] of this.assignment) {\n      for (const pId of partitionIds) {\n        while (this.partitions[pId].length > 0) {\n          const msg = this.partitions[pId].shift();\n          this.consumed.get(consumer).push(msg);\n        }\n      }\n    }\n  }\n\n  getAssignment() {\n    const result = {};\n    for (const [c, parts] of this.assignment) result[c] = parts;\n    return result;\n  }\n\n  getConsumed() {\n    const result = {};\n    for (const [c, msgs] of this.consumed) result[c] = msgs;\n    return result;\n  }\n}\n\nconst partitions = [\n  ['p0-msg1', 'p0-msg2'],\n  ['p1-msg1'],\n  ['p2-msg1', 'p2-msg2', 'p2-msg3'],\n  ['p3-msg1']\n];\n\nconst group = new ConsumerGroup('group-1', partitions);\ngroup.addConsumer('consumer-A');\ngroup.addConsumer('consumer-B');\n\nconsole.log('Assignment:', group.getAssignment());\ngroup.consumeAll();\nconsole.log('Consumed:', group.getConsumed());",
      "output": "Assignment: { 'consumer-A': [ 0, 2 ], 'consumer-B': [ 1, 3 ] }\nConsumed: {\n  'consumer-A': [ 'p0-msg1', 'p0-msg2', 'p2-msg1', 'p2-msg2', 'p2-msg3' ],\n  'consumer-B': [ 'p1-msg1', 'p3-msg1' ]\n}"
    },
    {
      "question": "Program 10: Implement a delayed message queue with scheduling",
      "code": "class DelayedQueue {\n  constructor() {\n    this.ready = [];\n    this.delayed = [];\n    this.currentTime = 0;\n  }\n\n  send(message, delayMs = 0) {\n    if (delayMs === 0) {\n      this.ready.push({ message, enqueuedAt: this.currentTime });\n    } else {\n      this.delayed.push({\n        message,\n        deliverAt: this.currentTime + delayMs,\n        enqueuedAt: this.currentTime\n      });\n      this.delayed.sort((a, b) => a.deliverAt - b.deliverAt);\n    }\n  }\n\n  advanceTime(ms) {\n    this.currentTime += ms;\n    const nowReady = [];\n    const stillDelayed = [];\n    this.delayed.forEach(item => {\n      if (item.deliverAt <= this.currentTime) {\n        nowReady.push({ message: item.message, enqueuedAt: item.enqueuedAt });\n      } else {\n        stillDelayed.push(item);\n      }\n    });\n    this.ready.push(...nowReady);\n    this.delayed = stillDelayed;\n    return nowReady.length;\n  }\n\n  receive() {\n    return this.ready.shift() || null;\n  }\n\n  stats() {\n    return { ready: this.ready.length, delayed: this.delayed.length, time: this.currentTime };\n  }\n}\n\nconst dq = new DelayedQueue();\n\ndq.send('instant-msg');\ndq.send('delayed-5s', 5000);\ndq.send('delayed-2s', 2000);\ndq.send('delayed-10s', 10000);\n\nconsole.log('Initial:', dq.stats());\n\nconst promoted1 = dq.advanceTime(3000);\nconsole.log(`After 3s: promoted ${promoted1}`, dq.stats());\n\nconst promoted2 = dq.advanceTime(3000);\nconsole.log(`After 6s: promoted ${promoted2}`, dq.stats());\n\nconst messages = [];\nlet m;\nwhile ((m = dq.receive()) !== null) {\n  messages.push(m.message);\n}\nconsole.log('Received:', messages);",
      "output": "Initial: { ready: 1, delayed: 3, time: 0 }\nAfter 3s: promoted 1 { ready: 2, delayed: 2, time: 3000 }\nAfter 6s: promoted 1 { ready: 3, delayed: 1, time: 6000 }\nReceived: [ 'instant-msg', 'delayed-2s', 'delayed-5s' ]"
    }
  ]
}
