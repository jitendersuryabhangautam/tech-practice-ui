{
  "id": "youtube-lld",
  "title": "YouTube: Video Upload & Processing",
  "category": "Company LLD",
  "description": "Low-level design of YouTube's video upload pipeline, transcoding, thumbnail generation, and video serving.",
  "explanation": "YouTube handles 500+ hours of video uploaded every minute. The upload pipeline must handle large files reliably, transcode into multiple formats/resolutions, generate thumbnails, run content moderation, and make videos searchable — all while providing real-time progress feedback.\n\nUpload pipeline:\n1. **Resumable upload**: Client splits video into chunks (5-10MB each). Each chunk is uploaded with a byte-range header. Server tracks progress. If upload fails, client resumes from last acknowledged chunk.\n2. **Upload service** validates metadata, assigns a video_id, stores raw file in object storage (GCS/S3).\n3. **Transcoding pipeline** (async): Raw video → multiple renditions (144p, 360p, 480p, 720p, 1080p, 4K). Each rendition encoded in multiple codecs (H.264, VP9, AV1). Total: 20-30 output files per video.\n4. **Thumbnail generation**: Extract keyframes, run quality scoring, select top 3, offer to creator.\n5. **Content moderation**: ML models scan for policy violations (nudity, violence, copyright). Copyright check via Content ID (audio/video fingerprinting).\n6. **Metadata indexing**: Title, description, tags → search index. Auto-generated captions via speech-to-text.\n7. **CDN distribution**: Transcoded segments pushed to edge locations. Popular videos pre-cached; long-tail served from origin.\n\nVideo serving:\n- Adaptive bitrate streaming (DASH/HLS) same as Netflix.\n- Manifest contains all renditions. Client picks based on bandwidth.\n- Seek operations: segment index maps timestamps to byte ranges.\n\nRecommendation signals:\n- Watch time, completion rate, likes/dislikes, click-through rate from thumbnail.\n- Collaborative filtering + content-based features.\n\nScale challenges:\n- 1B hours watched per day.\n- Storage: ~1PB of new video per day.\n- Transcoding: 100K+ transcoding jobs running simultaneously.",
  "code": "// Resumable upload service\nclass UploadService {\n  constructor() {\n    this.uploads = new Map();\n    this.completedVideos = [];\n  }\n\n  initUpload(userId, metadata) {\n    const videoId = `vid_${Date.now()}`;\n    const upload = {\n      videoId,\n      userId,\n      metadata,\n      chunks: [],\n      totalSize: metadata.fileSize,\n      uploadedBytes: 0,\n      status: 'UPLOADING',\n      createdAt: Date.now(),\n    };\n    this.uploads.set(videoId, upload);\n    return { videoId, uploadUrl: `/upload/${videoId}` };\n  }\n\n  uploadChunk(videoId, chunkIndex, chunkSize, data) {\n    const upload = this.uploads.get(videoId);\n    if (!upload) throw new Error('Upload not found');\n    if (upload.status !== 'UPLOADING') throw new Error(`Upload status: ${upload.status}`);\n\n    // Idempotent: skip if chunk already uploaded\n    if (upload.chunks.includes(chunkIndex)) {\n      return { status: 'DUPLICATE', uploadedBytes: upload.uploadedBytes };\n    }\n\n    upload.chunks.push(chunkIndex);\n    upload.uploadedBytes += chunkSize;\n    const progress = ((upload.uploadedBytes / upload.totalSize) * 100).toFixed(1);\n\n    if (upload.uploadedBytes >= upload.totalSize) {\n      upload.status = 'PROCESSING';\n      this.completedVideos.push(videoId);\n      return { status: 'COMPLETE', progress: '100.0%', videoId };\n    }\n\n    return { status: 'IN_PROGRESS', progress: progress + '%', chunkIndex };\n  }\n\n  getUploadStatus(videoId) {\n    const u = this.uploads.get(videoId);\n    if (!u) return null;\n    return { videoId, status: u.status, progress: ((u.uploadedBytes / u.totalSize) * 100).toFixed(1) + '%', chunks: u.chunks.length };\n  }\n}\n\nconst upload = new UploadService();\nconst { videoId } = upload.initUpload('user1', { title: 'My Video', fileSize: 30 });\nconsole.log(upload.uploadChunk(videoId, 0, 10, 'data0'));\nconsole.log(upload.uploadChunk(videoId, 1, 10, 'data1'));\nconsole.log(upload.uploadChunk(videoId, 1, 10, 'data1')); // duplicate\nconsole.log(upload.uploadChunk(videoId, 2, 10, 'data2'));\nconsole.log(upload.getUploadStatus(videoId));",
  "example": "// Transcoding job manager\nclass TranscodingPipeline {\n  constructor() {\n    this.jobs = new Map();\n  }\n\n  createJobs(videoId, renditions) {\n    const jobs = renditions.map((r, i) => ({\n      jobId: `${videoId}_${r.label}`,\n      videoId,\n      rendition: r,\n      status: 'QUEUED',\n      progress: 0,\n      startTime: null,\n      endTime: null,\n    }));\n    jobs.forEach(j => this.jobs.set(j.jobId, j));\n    return jobs.map(j => j.jobId);\n  }\n\n  processJob(jobId) {\n    const job = this.jobs.get(jobId);\n    if (!job) return null;\n    job.status = 'PROCESSING';\n    job.startTime = Date.now();\n    // Simulate processing\n    job.progress = 100;\n    job.status = 'COMPLETED';\n    job.endTime = Date.now();\n    return job;\n  }\n\n  getVideoStatus(videoId) {\n    const videoJobs = [...this.jobs.values()].filter(j => j.videoId === videoId);\n    return {\n      total: videoJobs.length,\n      completed: videoJobs.filter(j => j.status === 'COMPLETED').length,\n      failed: videoJobs.filter(j => j.status === 'FAILED').length,\n      allDone: videoJobs.every(j => j.status === 'COMPLETED'),\n    };\n  }\n}\n\nconst pipeline = new TranscodingPipeline();\nconst jobIds = pipeline.createJobs('vid1', [\n  { label: '360p', bitrate: 500 },\n  { label: '720p', bitrate: 1500 },\n  { label: '1080p', bitrate: 4000 },\n]);\njobIds.forEach(id => pipeline.processJob(id));\nconsole.log(pipeline.getVideoStatus('vid1'));",
  "useCase": "Video platforms, media processing pipelines, content management systems, live streaming platforms.",
  "interviewQuestions": [
    { "question": "How does resumable upload work?", "answer": "Client splits file into chunks. Each chunk uploaded with byte-range header. Server tracks received chunks. On failure, client queries server for last received byte position and resumes from there. This handles network interruptions gracefully for large files." },
    { "question": "How do you handle transcoding at scale?", "answer": "Transcoding is CPU-intensive. Use a job queue (SQS/Kafka). Worker pool pulls jobs. Each video creates N jobs (one per rendition). Workers run FFmpeg or hardware encoders. Auto-scale workers based on queue depth. Priority queue for premium creators." },
    { "question": "How do you ensure exactly-once processing in the upload pipeline?", "answer": "Idempotency keys: each chunk has a unique ID (videoId + chunkIndex). Deduplicate on receive. For transcoding: job ID = videoId + rendition. If job already exists, skip. Message queue with visibility timeout prevents double-processing." },
    { "question": "How do you generate good thumbnails?", "answer": "Extract frames at regular intervals (every 2 seconds). Score each frame: face detection, color vibrancy, sharpness, motion blur rejection. ML model trained on click-through-rate data selects best candidates. Offer top 3 to creator; auto-select best one as default." },
    { "question": "What is Content ID and how does it work?", "answer": "Audio and video fingerprinting. Rights holders upload reference files. On each new upload, generate fingerprint and compare against reference database. Matches trigger: block, monetize (ads for rights holder), or track. Uses locality-sensitive hashing for fast matching." },
    { "question": "How do you handle a video that goes viral immediately after upload?", "answer": "Proactive CDN caching: if view velocity exceeds threshold, push to more edges. Serve lower renditions first (smaller, faster to distribute). Pre-warm popular regions. Degrade gracefully: serve 720p while 4K propagates." },
    { "question": "What is the video segment index used for?", "answer": "Maps timestamps to byte ranges in the encoded file. Enables seeking without downloading the entire video. Client requests segment index first, then fetches specific byte ranges for the desired time position. Essential for scrubbing and chapter navigation." },
    { "question": "How do you handle live streaming differently from uploads?", "answer": "Live: ingest via RTMP/SRT, transcode in real-time, segment immediately (2-4s chunks), push to CDN with ultra-low latency. No resumable upload. Separate infrastructure for ingest, encoding, and distribution. DVR: store segments for rewind." },
    { "question": "How do you optimize storage costs for petabytes of video?", "answer": "Tiering: hot (SSD/RAM cache for popular), warm (HDD for recent), cold (tape/glacier for old, rarely viewed). Delete low-view-count renditions after 90 days (keep source + 1 rendition, re-transcode on demand). Compress with newer codecs (AV1) for 30% size reduction." },
    { "question": "How do you build the video recommendation system?", "answer": "Signals: watch time, completion rate, click-through rate, likes, shares. Collaborative filtering: users who watched X also watched Y. Content-based: video embeddings from title, tags, visual features. Two-tower model: user embedding × video embedding. Serve from pre-computed candidate lists, re-rank in real-time." }
  ],
  "exercises": [
    { "type": "design", "question": "Design the resumable upload protocol for videos up to 256GB.", "answer": "Chunk size: 8MB. Init: POST /uploads → returns upload_id. Upload chunk: PUT /uploads/{id}?chunk={n} with Content-Range header. Server writes chunk to object storage, updates progress in DB. Resume: GET /uploads/{id}/status returns last received byte. Final: server assembles chunks. Timeout: expire incomplete uploads after 7 days." },
    { "type": "estimation", "question": "500 hours of video uploaded per minute. Average video: 10 minutes, 1GB raw. Estimate daily raw storage and transcoding compute.", "answer": "Videos/min: 500×60/10 = 3000 videos/min = 4.32M videos/day. Storage: 4.32M × 1GB = 4.32PB/day raw. Transcoding: 5 renditions × 4.32M = 21.6M transcoding jobs/day. At 1 job per core-minute: 21.6M core-minutes/day = 15,000 cores running 24/7." },
    { "type": "scenario", "question": "A transcoding job fails midway through. How do you handle recovery?", "answer": "Checkpointing: FFmpeg supports segment-level resumption. Log last completed segment. On retry, resume from checkpoint. Max 3 retries. If persistent failure, route to different worker (maybe hardware issue). DLQ for manual investigation. Notify creator of delays." },
    { "type": "debug", "question": "Videos are transcoded successfully but some renditions show audio/video sync drift. What are possible causes?", "answer": "1) Timestamp discontinuities in source video. 2) Audio resampling without proper PTS adjustment. 3) Variable frame rate not handled correctly. 4) Segment boundaries not aligned between audio and video tracks. Fix: force constant frame rate, align segment boundaries on keyframes." },
    { "type": "design", "question": "Design the thumbnail selection pipeline.", "answer": "1) Extract 1 frame per 2 seconds. 2) Filter: remove black frames, high motion blur, low contrast. 3) Quality scoring ML model: face detection (bonus), rule of thirds, vibrant colors. 4) Diversity: cluster frames by visual similarity, pick top from each cluster. 5) A/B test thumbnails for click-through rate. 6) Auto-update thumbnail if creator doesn't choose within 24h." },
    { "type": "tricky", "question": "Why transcode into multiple codecs (H.264, VP9, AV1) rather than just the best one?", "answer": "Device compatibility: older devices only support H.264. VP9: 30% better compression, supported by Chrome/Android. AV1: 50% better than H.264 but requires modern hardware. Browser negotiates best codec via Accept header. Serving AV1 where supported saves 50% bandwidth cost at scale." },
    { "type": "estimation", "question": "1B daily active users, each watches 40 minutes. Average bitrate 3Mbps. Estimate peak bandwidth.", "answer": "Assuming 20% of daily viewing concentrated in 4 peak hours. Peak viewers: 1B × 0.2 / 4hrs × (40/1440 fraction of day... better: 1B × 40min = 40B minutes/day. Peak 4hrs = 240min, handles 33% of views: 13.2B minutes in 240 min = 55M concurrent viewers. Bandwidth: 55M × 3Mbps = 165Tbps." },
    { "type": "design", "question": "Design the auto-captioning pipeline.", "answer": "1) Extract audio track from video. 2) Chunk audio into 30-second segments (with overlap for continuity). 3) Speech-to-text via ML model (Whisper-like). 4) Align timestamps with video frames. 5) Post-process: punctuation, capitalization, profanity filtering. 6) Store as WebVTT format. 7) Support 100+ languages via translation pipeline. 8) Creator can edit captions." },
    { "type": "scenario", "question": "You need to re-encode 100M existing videos with a new codec. How do you plan the migration?", "answer": "1) Prioritize by view count (top 1% of videos get 80% of views). 2) Batch job over 6 months. 3) Process during off-peak hours. 4) Don't delete old renditions until new ones verified. 5) A/B test quality. 6) Progress dashboard for tracking. 7) Fallback to old rendition if re-encode fails." },
    { "type": "output", "question": "A 2-hour video at 30fps needs thumbnails every 2 seconds. How many candidate frames? If ML scoring takes 10ms per frame, total scoring time?", "answer": "Duration: 7200 seconds. Frames: 7200 / 2 = 3600 candidate frames. Scoring: 3600 × 10ms = 36 seconds. With 10 parallel workers: 3.6 seconds. Acceptable latency for async pipeline." }
  ],
  "programExercises": [
    {
      "question": "Program 1: Resumable chunk upload tracker",
      "code": "class ChunkTracker {\n  constructor(totalSize, chunkSize) {\n    this.totalSize = totalSize;\n    this.chunkSize = chunkSize;\n    this.totalChunks = Math.ceil(totalSize / chunkSize);\n    this.received = new Set();\n  }\n  receiveChunk(index) {\n    if (index >= this.totalChunks) return { error: 'Invalid chunk index' };\n    if (this.received.has(index)) return { status: 'duplicate', index };\n    this.received.add(index);\n    const progress = ((this.received.size / this.totalChunks) * 100).toFixed(1);\n    return {\n      status: this.received.size === this.totalChunks ? 'COMPLETE' : 'IN_PROGRESS',\n      progress: progress + '%',\n      remaining: this.totalChunks - this.received.size,\n    };\n  }\n  getMissing() {\n    const missing = [];\n    for (let i = 0; i < this.totalChunks; i++) {\n      if (!this.received.has(i)) missing.push(i);\n    }\n    return missing;\n  }\n}\n\nconst tracker = new ChunkTracker(50, 10); // 50MB file, 10MB chunks\nconsole.log(tracker.receiveChunk(0));\nconsole.log(tracker.receiveChunk(2)); // out of order OK\nconsole.log(tracker.receiveChunk(0)); // duplicate\nconsole.log('Missing:', tracker.getMissing());\nconsole.log(tracker.receiveChunk(1));\nconsole.log(tracker.receiveChunk(3));\nconsole.log(tracker.receiveChunk(4));",
      "output": "{ status: 'IN_PROGRESS', progress: '20.0%', remaining: 4 }\n{ status: 'IN_PROGRESS', progress: '40.0%', remaining: 3 }\n{ status: 'duplicate', index: 0 }\nMissing: [ 1, 3, 4 ]\n{ status: 'IN_PROGRESS', progress: '60.0%', remaining: 2 }\n{ status: 'IN_PROGRESS', progress: '80.0%', remaining: 1 }\n{ status: 'COMPLETE', progress: '100.0%', remaining: 0 }"
    },
    {
      "question": "Program 2: Transcoding job queue",
      "code": "class TranscodeQueue {\n  constructor() { this.queue = []; this.results = []; }\n  enqueue(videoId, renditions) {\n    renditions.forEach(r => {\n      this.queue.push({ videoId, rendition: r, status: 'QUEUED', priority: r.priority || 0 });\n    });\n    this.queue.sort((a, b) => b.priority - a.priority);\n  }\n  processNext() {\n    const job = this.queue.shift();\n    if (!job) return null;\n    job.status = 'COMPLETED';\n    this.results.push(job);\n    return { videoId: job.videoId, rendition: job.rendition.label, status: 'COMPLETED' };\n  }\n  status() {\n    return { queued: this.queue.length, completed: this.results.length };\n  }\n}\n\nconst q = new TranscodeQueue();\nq.enqueue('vid1', [\n  { label: '360p', bitrate: 500, priority: 1 },\n  { label: '1080p', bitrate: 4000, priority: 3 },\n  { label: '720p', bitrate: 1500, priority: 2 },\n]);\nconsole.log(q.processNext()); // highest priority first\nconsole.log(q.processNext());\nconsole.log(q.processNext());\nconsole.log(q.status());",
      "output": "{ videoId: 'vid1', rendition: '1080p', status: 'COMPLETED' }\n{ videoId: 'vid1', rendition: '720p', status: 'COMPLETED' }\n{ videoId: 'vid1', rendition: '360p', status: 'COMPLETED' }\n{ queued: 0, completed: 3 }"
    },
    {
      "question": "Program 3: Thumbnail quality scorer",
      "code": "function scoreThumbnails(frames) {\n  return frames.map(frame => {\n    let score = 0;\n    // Face detection bonus\n    if (frame.hasFace) score += 30;\n    // Color vibrancy (0-100)\n    score += frame.colorVibrancy * 0.3;\n    // Sharpness (0-100)\n    score += frame.sharpness * 0.2;\n    // Motion blur penalty\n    if (frame.motionBlur > 50) score -= 15;\n    // Black frame penalty\n    if (frame.brightness < 10) score -= 40;\n    return { timestamp: frame.timestamp, score: Math.round(score), selected: false };\n  })\n  .sort((a, b) => b.score - a.score)\n  .map((t, i) => ({ ...t, selected: i < 3 })); // top 3\n}\n\nconst frames = [\n  { timestamp: '0:15', hasFace: true, colorVibrancy: 80, sharpness: 90, motionBlur: 10, brightness: 70 },\n  { timestamp: '1:30', hasFace: false, colorVibrancy: 60, sharpness: 70, motionBlur: 60, brightness: 50 },\n  { timestamp: '3:00', hasFace: true, colorVibrancy: 90, sharpness: 85, motionBlur: 20, brightness: 65 },\n  { timestamp: '5:00', hasFace: false, colorVibrancy: 20, sharpness: 30, motionBlur: 5, brightness: 5 },\n  { timestamp: '7:30', hasFace: true, colorVibrancy: 70, sharpness: 95, motionBlur: 15, brightness: 80 },\n];\nconsole.log(scoreThumbnails(frames));",
      "output": "[\n  { timestamp: '3:00', score: 74, selected: true },\n  { timestamp: '0:15', score: 72, selected: true },\n  { timestamp: '7:30', score: 70, selected: true },\n  { timestamp: '1:30', score: 17, selected: false },\n  { timestamp: '5:00', score: -28, selected: false }\n]"
    },
    {
      "question": "Program 4: Content ID fingerprint matcher",
      "code": "class ContentIdService {\n  constructor() { this.references = new Map(); }\n  \n  addReference(ownerId, contentId, fingerprint) {\n    this.references.set(fingerprint, { ownerId, contentId });\n  }\n  \n  match(uploadFingerprint, threshold = 0.8) {\n    for (const [refFp, info] of this.references) {\n      const similarity = this.compareFP(uploadFingerprint, refFp);\n      if (similarity >= threshold) {\n        return { matched: true, similarity: similarity.toFixed(2), owner: info.ownerId, contentId: info.contentId, action: 'MONETIZE' };\n      }\n    }\n    return { matched: false };\n  }\n  \n  compareFP(a, b) {\n    // Simulated: compare character overlap\n    const setA = new Set(a.split(''));\n    const setB = new Set(b.split(''));\n    const intersection = [...setA].filter(x => setB.has(x)).length;\n    return intersection / Math.max(setA.size, setB.size);\n  }\n}\n\nconst cid = new ContentIdService();\ncid.addReference('Universal', 'song-123', 'abcdefghij');\ncid.addReference('Warner', 'movie-456', 'klmnopqrst');\n\nconsole.log(cid.match('abcdefghxy')); // similar to Universal's song\nconsole.log(cid.match('zzzzzzz'));     // no match",
      "output": "{ matched: true, similarity: '0.80', owner: 'Universal', contentId: 'song-123', action: 'MONETIZE' }\n{ matched: false }"
    },
    {
      "question": "Program 5: Video processing pipeline orchestrator",
      "code": "class PipelineOrchestrator {\n  constructor() { this.pipelines = new Map(); }\n  \n  startPipeline(videoId) {\n    const stages = [\n      { name: 'validate', status: 'pending' },\n      { name: 'transcode', status: 'pending' },\n      { name: 'thumbnail', status: 'pending' },\n      { name: 'contentId', status: 'pending' },\n      { name: 'index', status: 'pending' },\n    ];\n    this.pipelines.set(videoId, { stages, currentStage: 0 });\n    return this.advancePipeline(videoId);\n  }\n  \n  advancePipeline(videoId) {\n    const pipeline = this.pipelines.get(videoId);\n    const log = [];\n    while (pipeline.currentStage < pipeline.stages.length) {\n      const stage = pipeline.stages[pipeline.currentStage];\n      stage.status = 'completed';\n      log.push(`✅ ${stage.name}`);\n      pipeline.currentStage++;\n    }\n    return { videoId, log, status: 'READY' };\n  }\n  \n  getStatus(videoId) {\n    const p = this.pipelines.get(videoId);\n    return {\n      progress: `${p.currentStage}/${p.stages.length}`,\n      stages: p.stages.map(s => `${s.status === 'completed' ? '✅' : '⏳'} ${s.name}`),\n    };\n  }\n}\n\nconst orch = new PipelineOrchestrator();\nconsole.log(orch.startPipeline('vid-001'));\nconsole.log(orch.getStatus('vid-001'));",
      "output": "{\n  videoId: 'vid-001',\n  log: ['✅ validate', '✅ transcode', '✅ thumbnail', '✅ contentId', '✅ index'],\n  status: 'READY'\n}\n{\n  progress: '5/5',\n  stages: ['✅ validate', '✅ transcode', '✅ thumbnail', '✅ contentId', '✅ index']\n}"
    },
    {
      "question": "Program 6: Video storage tier manager",
      "code": "class StorageTierManager {\n  constructor() { this.videos = new Map(); }\n  \n  addVideo(videoId, viewsPerDay, ageInDays, sizeGB) {\n    let tier;\n    if (viewsPerDay > 10000) tier = 'HOT';\n    else if (viewsPerDay > 100 || ageInDays < 30) tier = 'WARM';\n    else if (viewsPerDay > 0) tier = 'COLD';\n    else tier = 'ARCHIVE';\n    \n    const costPerGB = { HOT: 0.023, WARM: 0.013, COLD: 0.004, ARCHIVE: 0.001 };\n    this.videos.set(videoId, {\n      tier, viewsPerDay, ageInDays, sizeGB,\n      monthlyCost: +(sizeGB * costPerGB[tier]).toFixed(4),\n    });\n    return this.videos.get(videoId);\n  }\n  \n  summary() {\n    const tiers = { HOT: 0, WARM: 0, COLD: 0, ARCHIVE: 0 };\n    let totalCost = 0;\n    for (const v of this.videos.values()) {\n      tiers[v.tier]++;\n      totalCost += v.monthlyCost;\n    }\n    return { tiers, totalMonthlyCost: '$' + totalCost.toFixed(2) };\n  }\n}\n\nconst mgr = new StorageTierManager();\nmgr.addVideo('v1', 50000, 5, 10);   // HOT\nmgr.addVideo('v2', 500, 20, 8);     // WARM\nmgr.addVideo('v3', 5, 200, 12);     // COLD\nmgr.addVideo('v4', 0, 365, 15);     // ARCHIVE\nconsole.log(mgr.summary());",
      "output": "{\n  tiers: { HOT: 1, WARM: 1, COLD: 1, ARCHIVE: 1 },\n  totalMonthlyCost: '$0.40'\n}"
    },
    {
      "question": "Program 7: Watch history and recommendations",
      "code": "function recommend(userId, watchHistory, videoDb) {\n  // Simple collaborative filtering: find similar users, recommend their watches\n  const userTags = new Set();\n  watchHistory[userId]?.forEach(vid => {\n    (videoDb[vid]?.tags || []).forEach(t => userTags.add(t));\n  });\n  \n  const watched = new Set(watchHistory[userId] || []);\n  const candidates = Object.entries(videoDb)\n    .filter(([id]) => !watched.has(id))\n    .map(([id, video]) => {\n      const overlap = video.tags.filter(t => userTags.has(t)).length;\n      return { id, title: video.title, score: overlap };\n    })\n    .sort((a, b) => b.score - a.score)\n    .slice(0, 3);\n  \n  return candidates;\n}\n\nconst videoDb = {\n  v1: { title: 'React Tutorial', tags: ['react', 'js', 'frontend'] },\n  v2: { title: 'Node.js Crash Course', tags: ['node', 'js', 'backend'] },\n  v3: { title: 'Go Concurrency', tags: ['golang', 'concurrency'] },\n  v4: { title: 'JS Design Patterns', tags: ['js', 'patterns', 'frontend'] },\n  v5: { title: 'Docker Basics', tags: ['docker', 'devops'] },\n};\n\nconst history = { user1: ['v1', 'v2'] }; // watched React + Node\nconsole.log(recommend('user1', history, videoDb));",
      "output": "[\n  { id: 'v4', title: 'JS Design Patterns', score: 2 },\n  { id: 'v3', title: 'Go Concurrency', score: 0 },\n  { id: 'v5', title: 'Docker Basics', score: 0 }\n]"
    },
    {
      "question": "Program 8: Segment index for video seeking",
      "code": "function buildSegmentIndex(segments) {\n  return segments.map((seg, i) => ({\n    index: i,\n    startTime: seg.startTime,\n    endTime: seg.endTime,\n    byteOffset: seg.byteOffset,\n    byteLength: seg.byteLength,\n  }));\n}\n\nfunction seekToTime(index, targetTime) {\n  const segment = index.find(s => targetTime >= s.startTime && targetTime < s.endTime);\n  if (!segment) return { error: 'Time out of range' };\n  return {\n    segment: segment.index,\n    byteRange: `${segment.byteOffset}-${segment.byteOffset + segment.byteLength - 1}`,\n    startTime: segment.startTime,\n  };\n}\n\nconst index = buildSegmentIndex([\n  { startTime: 0, endTime: 6, byteOffset: 0, byteLength: 500000 },\n  { startTime: 6, endTime: 12, byteOffset: 500000, byteLength: 480000 },\n  { startTime: 12, endTime: 18, byteOffset: 980000, byteLength: 520000 },\n  { startTime: 18, endTime: 24, byteOffset: 1500000, byteLength: 490000 },\n]);\n\nconsole.log('Seek to 8s:', seekToTime(index, 8));\nconsole.log('Seek to 0s:', seekToTime(index, 0));\nconsole.log('Seek to 20s:', seekToTime(index, 20));",
      "output": "Seek to 8s: { segment: 1, byteRange: '500000-979999', startTime: 6 }\nSeek to 0s: { segment: 0, byteRange: '0-499999', startTime: 0 }\nSeek to 20s: { segment: 3, byteRange: '1500000-1989999', startTime: 18 }"
    },
    {
      "question": "Program 9: View count with eventual consistency",
      "code": "class ViewCounter {\n  constructor(shards) {\n    this.shards = Array.from({ length: shards }, () => new Map());\n  }\n  \n  increment(videoId) {\n    const shard = this.shards[videoId.charCodeAt(0) % this.shards.length];\n    shard.set(videoId, (shard.get(videoId) || 0) + 1);\n  }\n  \n  getCount(videoId) {\n    return this.shards.reduce((sum, shard) => sum + (shard.get(videoId) || 0), 0);\n  }\n  \n  getApproxDisplay(videoId) {\n    const count = this.getCount(videoId);\n    if (count < 1000) return `${count} views`;\n    if (count < 1000000) return `${(count/1000).toFixed(1)}K views`;\n    return `${(count/1000000).toFixed(1)}M views`;\n  }\n}\n\nconst counter = new ViewCounter(3);\nfor (let i = 0; i < 1500; i++) counter.increment('vid1');\nfor (let i = 0; i < 2500000; i++) counter.increment('vid2');\nconsole.log(counter.getApproxDisplay('vid1'));\nconsole.log(counter.getApproxDisplay('vid2'));",
      "output": "1.5K views\n2.5M views"
    },
    {
      "question": "Program 10: Video processing event log",
      "code": "class ProcessingLog {\n  constructor() { this.events = []; }\n  log(videoId, stage, status, details) {\n    this.events.push({ videoId, stage, status, details, time: new Date().toISOString() });\n  }\n  getTimeline(videoId) {\n    return this.events\n      .filter(e => e.videoId === videoId)\n      .map(e => `[${e.status}] ${e.stage}: ${e.details}`);\n  }\n  getStageDurations(videoId) {\n    const events = this.events.filter(e => e.videoId === videoId);\n    const stages = {};\n    events.forEach((e, i) => {\n      if (!stages[e.stage]) stages[e.stage] = { start: i, end: i };\n      stages[e.stage].end = i;\n    });\n    return Object.entries(stages).map(([name, { start, end }]) => ({ stage: name, events: end - start + 1 }));\n  }\n}\n\nconst log = new ProcessingLog();\nlog.log('v1', 'upload', 'START', 'File received');\nlog.log('v1', 'upload', 'DONE', '100% uploaded');\nlog.log('v1', 'transcode', 'START', 'Creating 5 renditions');\nlog.log('v1', 'transcode', 'PROGRESS', '3/5 complete');\nlog.log('v1', 'transcode', 'DONE', 'All renditions ready');\nlog.log('v1', 'publish', 'DONE', 'Video is live');\nconsole.log(log.getTimeline('v1'));\nconsole.log(log.getStageDurations('v1'));",
      "output": "[\n  '[START] upload: File received',\n  '[DONE] upload: 100% uploaded',\n  '[START] transcode: Creating 5 renditions',\n  '[PROGRESS] transcode: 3/5 complete',\n  '[DONE] transcode: All renditions ready',\n  '[DONE] publish: Video is live'\n]\n[\n  { stage: 'upload', events: 2 },\n  { stage: 'transcode', events: 3 },\n  { stage: 'publish', events: 1 }\n]"
    }
  ]
}
