{
  "id": "facebook-feed-lld",
  "title": "Facebook News Feed",
  "category": "Company LLD",
  "description": "Low-level design of Facebook's News Feed — ranking, fanout, real-time updates, and feed assembly pipeline.",
  "explanation": "Facebook News Feed serves 2B+ users, each seeing a personalized stream of posts from friends, pages, groups, and ads. The core challenge: selecting and ranking ~1500 candidate stories into ~300 shown stories, all within 200ms.\n\nFeed generation approaches:\n1. **Fanout on Write (Push)**: When a user publishes a post, push it into all followers' feed caches. Fast reads but expensive writes for users with millions of followers (celebrity problem). Used for most users.\n2. **Fanout on Read (Pull)**: When a user opens their feed, query all friends' timelines and merge/rank on the fly. Expensive reads but handles celebrities efficiently. Used for high-follower accounts.\n3. **Hybrid**: Push for normal users, pull for celebrities. Facebook and Twitter both use hybrid approaches.\n\nRanking pipeline:\n1. **Candidate generation**: Collect posts from friends, pages, groups (last 7 days). ~1500 candidates.\n2. **Feature extraction**: Engagement signals (likes, comments, shares), recency, relationship strength (interaction frequency), content type, creator quality score.\n3. **Scoring**: ML model (originally EdgeRank: Affinity × Weight × Decay) assigns relevance score. Modern: deep learning models.\n4. **Filtering**: Remove duplicates, content policy violations, user-hidden content.\n5. **Diversity injection**: Ensure mix of content types (not all videos, not all from same friend).\n6. **Pagination**: Return top ~50 stories per page, cursor-based pagination for infinite scroll.\n\nReal-time updates:\n- Long polling or WebSocket connection. Server pushes new story IDs. Client fetches story details and inserts into feed. Priority-based: comments on your post > friend's new post > page update.\n\nStorage:\n- Post table: post_id, author_id, content, created_at, type.\n- Feed cache: Redis sorted set per user. Score = ranking score. Member = post_id.\n- Social graph: adjacency list in memory (who follows whom).",
  "code": "// Feed assembly pipeline\nclass FeedService {\n  constructor() {\n    this.feedCache = new Map(); // userId -> sorted posts\n    this.posts = new Map();\n    this.graph = new Map(); // userId -> Set of followedIds\n  }\n\n  follow(userId, targetId) {\n    if (!this.graph.has(userId)) this.graph.set(userId, new Set());\n    this.graph.get(userId).add(targetId);\n  }\n\n  publish(authorId, post) {\n    const entry = { id: post.id, authorId, content: post.content, type: post.type, createdAt: Date.now(), likes: 0, comments: 0 };\n    this.posts.set(post.id, entry);\n    // Fanout on write for normal users\n    this.fanoutWrite(authorId, entry);\n    return entry;\n  }\n\n  fanoutWrite(authorId, entry) {\n    // Push to all followers' caches\n    for (const [userId, following] of this.graph) {\n      if (following.has(authorId)) {\n        if (!this.feedCache.has(userId)) this.feedCache.set(userId, []);\n        this.feedCache.get(userId).push(entry);\n      }\n    }\n  }\n\n  getFeed(userId, limit = 10) {\n    const cached = this.feedCache.get(userId) || [];\n    // Rank and return top entries\n    const ranked = cached\n      .map(post => ({ ...post, score: this.rank(userId, post) }))\n      .sort((a, b) => b.score - a.score)\n      .slice(0, limit);\n    return ranked;\n  }\n\n  rank(userId, post) {\n    const recency = 1 / (1 + (Date.now() - post.createdAt) / 3600000); // decay over hours\n    const engagement = post.likes * 1.0 + post.comments * 2.0;\n    const typeWeight = { text: 1, image: 1.2, video: 1.5, link: 0.8 };\n    return recency * 10 + engagement + (typeWeight[post.type] || 1);\n  }\n}\n\nconst feed = new FeedService();\nfeed.follow('alice', 'bob');\nfeed.follow('alice', 'carol');\n\nconst p1 = feed.publish('bob', { id: 'p1', content: 'Hello World!', type: 'text' });\nconst p2 = feed.publish('carol', { id: 'p2', content: 'Check this video', type: 'video' });\np2.likes = 10;\n\nconst aliceFeed = feed.getFeed('alice');\nconsole.log(aliceFeed.map(p => ({ content: p.content, score: p.score.toFixed(2) })));",
  "example": "// EdgeRank scoring simulation\nfunction edgeRank(affinity, weight, timeSinceHours) {\n  // Original EdgeRank formula: Score = Affinity × Weight × Decay\n  const decay = 1 / (1 + timeSinceHours);\n  return affinity * weight * decay;\n}\n\n// Calculate scores for candidate stories\nconst candidates = [\n  { post: 'Best friend photo',    affinity: 0.9, weight: 1.2, hoursSince: 1 },\n  { post: 'Page video',           affinity: 0.3, weight: 1.5, hoursSince: 0.5 },\n  { post: 'Acquaintance text',    affinity: 0.2, weight: 1.0, hoursSince: 2 },\n  { post: 'Close friend comment', affinity: 0.8, weight: 2.0, hoursSince: 3 },\n  { post: 'Old post reshared',    affinity: 0.5, weight: 0.8, hoursSince: 48 },\n];\n\nconst ranked = candidates\n  .map(c => ({ post: c.post, score: edgeRank(c.affinity, c.weight, c.hoursSince) }))\n  .sort((a, b) => b.score - a.score);\n\nconsole.log('Feed ranking:');\nranked.forEach((r, i) => console.log(`${i+1}. ${r.post} (score: ${r.score.toFixed(3)})`));",
  "useCase": "Social media feeds, personalized content streams, recommendation systems, timeline aggregation.",
  "interviewQuestions": [
    { "question": "Why does Facebook use a hybrid fanout strategy?", "answer": "Pure push (fanout-on-write) is expensive for celebrities with millions of followers — one post triggers millions of cache writes. Pure pull (fanout-on-read) makes feed loading slow — must query hundreds of friends' timelines and merge. Hybrid: push for regular users (fast reads), pull for celebrities (avoid write amplification)." },
    { "question": "How is the social graph stored and queried efficiently?", "answer": "In-memory adjacency list partitioned by user ID range. Each partition fits in a server's RAM. TAO (The Associations and Objects) system at Facebook caches graph edges with association lists. Queries like 'friends of user X' are single-hop lookups." },
    { "question": "How do you ensure feed freshness for active users?", "answer": "Long-poll or WebSocket connection. Server maintains a per-user notification channel. When a relevant post is published (fanout), push the post ID to the channel. Client fetches the full post and inserts it at the top or shows a 'new posts' indicator." },
    { "question": "How do you handle feed ranking at scale?", "answer": "Two-phase: lightweight candidate selection (SQL/cache lookup) narrows 1500 to ~500 candidates, then ML model scores each with features (affinity, engagement, recency, content type). Top 50 returned. Model is pre-computed for latency, retrained hourly." },
    { "question": "What is the celebrity problem in feed systems?", "answer": "A user with 10M followers publishes a post. Fanout-on-write means 10M cache writes. Solution: don't fanout for celebrities. When a follower opens their feed, pull the celebrity's latest posts on demand and merge into the pre-computed feed." },
    { "question": "How do you implement infinite scroll pagination?", "answer": "Cursor-based pagination: return a cursor (timestamp + post_id) with each response. Next request includes cursor. Server fetches posts with score < cursor. Avoids offset-based pagination problems (duplicates when new posts arrive)." },
    { "question": "How do you prevent duplicate stories in the feed?", "answer": "Deduplication at multiple levels: 1) Feed cache uses post_id as unique key. 2) Content fingerprinting detects reshares of the same content. 3) Client-side dedup merges server responses with already-displayed stories." },
    { "question": "How do you inject diversity into the feed?", "answer": "After ranking, apply diversity rules: max 2 consecutive posts from same author, max 3 videos in a row, ensure at least 1 friend post per 5 stories. Re-rank with diversity constraints using a greedy interleaving algorithm." },
    { "question": "How do you handle feed for a brand new user with no connections?", "answer": "Cold start: show trending content, popular pages in user's region, suggested friend activity. Use signup signals (interests, imported contacts) to bootstrap recommendations. Gradually transition to personalized feed as social graph grows." },
    { "question": "How do you A/B test feed algorithm changes?", "answer": "Assign users to experiment groups. Each group gets a different ranking model. Measure engagement metrics (time spent, clicks, likes, hides) per group. Use holdback groups and guard against network effects (treat friend clusters as units)." }
  ],
  "exercises": [
    { "type": "design", "question": "Design the fanout service that distributes posts to follower feeds.", "answer": "Publish event → Kafka topic. Fanout workers consume in parallel. For each post, look up author's follower list from social graph service. For each follower, ZADD to their Redis sorted set (feed cache) with ranking score. Skip fanout if author has > 100K followers (celebrity path). TTL on feed cache entries: 7 days." },
    { "type": "estimation", "question": "1B daily active users, average 300 posts in feed per day, average post size 1KB. Estimate daily storage for feed caches.", "answer": "Feed cache entries: 1B × 300 = 300B entries. Each entry: post_id (8B) + score (8B) = 16B. Total: 300B × 16B = 4.8TB for feed metadata. Post content stored separately: 300B unique posts × 1KB = 300TB. With replication: ~1PB." },
    { "type": "scenario", "question": "A viral post gets 1M shares in 10 minutes. How do you prevent system overload?", "answer": "1) Rate limit fanout workers per post. 2) Batch fanout operations. 3) For viral posts, switch to pull model (don't fanout, let users pull on demand). 4) Circuit breaker on write path. 5) Debounce engagement count updates (batch increments)." },
    { "type": "tricky", "question": "Why not just sort feed by timestamp (reverse chronological)?", "answer": "Users miss important content (close friend's wedding post buried under acquaintance's 20 check-ins). Engagement drops 30-40% without ranking. Users spend less time. Ranked feed surfaces high-quality, high-affinity content. But: provide 'Most Recent' as an option for user control." },
    { "type": "debug", "question": "Users report seeing the same post multiple times in their feed. What are possible causes?", "answer": "1) Fanout executed twice (idempotency failure). 2) Multiple reshares not deduplicated. 3) Cursor-based pagination returning overlapping windows. 4) Client cache not properly deduplicating. Fix: use post_id as dedup key; make fanout idempotent; fix cursor logic." },
    { "type": "design", "question": "Design the ranking feature extraction pipeline.", "answer": "Features: 1) User-author affinity (interaction count in last 30 days). 2) Post engagement (likes/comments/shares velocity). 3) Content type score. 4) Recency decay. 5) Author quality score. Pipeline: Precompute affinity scores daily (batch). Real-time features (engagement) from streaming counters. Feature vector assembled at query time. Cached per user for 5 minutes." },
    { "type": "output", "question": "EdgeRank: Affinity=0.8, Weight=1.5 (photo), Time=2 hours. Decay formula: 1/(1+hours). What is the score?", "answer": "Decay = 1/(1+2) = 0.333. Score = 0.8 × 1.5 × 0.333 = 0.4. Compare: same post at 0.5 hours: Decay = 1/1.5 = 0.667, Score = 0.8 × 1.5 × 0.667 = 0.8. The score halved in 1.5 hours." },
    { "type": "scenario", "question": "You need to deprecate the old ranking algorithm and roll out a new ML model. How do you do it safely?", "answer": "1) Shadow mode: run new model in parallel, log scores but don't serve. 2) Compare ranking quality offline. 3) 1% canary with real users. 4) Monitor engagement metrics (time spent, hides, unfollows). 5) Gradual rollout: 1% → 10% → 50% → 100%. 6) Keep rollback ready." },
    { "type": "estimation", "question": "A user has 500 friends, follows 100 pages. Average posts per entity per day: friends=2, pages=10. How many candidates per feed load?", "answer": "Friend posts/day: 500 × 2 = 1000. Page posts/day: 100 × 10 = 1000. Total candidates: 2000. With 7-day window: 14,000 candidates. After initial filtering (dedup, blocked): ~10,000. Ranking narrows to top 300." },
    { "type": "design", "question": "Design the real-time feed update system.", "answer": "WebSocket gateway maintains persistent connections. On new post fanout, publish event to user's channel in Redis Pub/Sub. Gateway forwards to client. Client inserts story at appropriate position. Throttle: max 1 push per 30 seconds per user. For inactive clients, batch updates and deliver on next app open." }
  ],
  "programExercises": [
    {
      "question": "Program 1: Feed assembly with fanout-on-write",
      "code": "class FeedCache {\n  constructor() { this.feeds = new Map(); }\n  addToFeed(userId, postId, score) {\n    if (!this.feeds.has(userId)) this.feeds.set(userId, []);\n    this.feeds.get(userId).push({ postId, score });\n    this.feeds.get(userId).sort((a, b) => b.score - a.score);\n  }\n  getFeed(userId, limit = 5) {\n    return (this.feeds.get(userId) || []).slice(0, limit);\n  }\n}\n\nclass SocialGraph {\n  constructor() { this.followers = new Map(); }\n  follow(follower, followed) {\n    if (!this.followers.has(followed)) this.followers.set(followed, []);\n    this.followers.get(followed).push(follower);\n  }\n  getFollowers(userId) { return this.followers.get(userId) || []; }\n}\n\nconst graph = new SocialGraph();\nconst cache = new FeedCache();\ngraph.follow('alice', 'bob');\ngraph.follow('charlie', 'bob');\ngraph.follow('alice', 'dave');\n\nfunction publish(authorId, postId, score) {\n  graph.getFollowers(authorId).forEach(f => cache.addToFeed(f, postId, score));\n}\n\npublish('bob', 'post1', 8.5);\npublish('bob', 'post2', 6.2);\npublish('dave', 'post3', 9.1);\n\nconsole.log('Alice feed:', cache.getFeed('alice'));\nconsole.log('Charlie feed:', cache.getFeed('charlie'));",
      "output": "Alice feed: [\n  { postId: 'post3', score: 9.1 },\n  { postId: 'post1', score: 8.5 },\n  { postId: 'post2', score: 6.2 }\n]\nCharlie feed: [\n  { postId: 'post1', score: 8.5 },\n  { postId: 'post2', score: 6.2 }\n]"
    },
    {
      "question": "Program 2: EdgeRank scoring engine",
      "code": "function edgeRank(affinity, weight, hoursSince) {\n  return affinity * weight * (1 / (1 + hoursSince));\n}\n\nfunction rankFeed(userId, interactions, posts) {\n  return posts.map(post => {\n    const affinity = interactions[`${userId}:${post.author}`] || 0.1;\n    const weight = { text: 1.0, image: 1.2, video: 1.5, link: 0.8 }[post.type] || 1.0;\n    const hours = (Date.now() - post.timestamp) / 3600000;\n    return { ...post, score: edgeRank(affinity, weight, hours) };\n  }).sort((a, b) => b.score - a.score);\n}\n\nconst now = Date.now();\nconst interactions = { 'alice:bob': 0.9, 'alice:carol': 0.3, 'alice:dave': 0.6 };\nconst posts = [\n  { id: 1, author: 'bob', type: 'image', text: 'Sunset', timestamp: now - 3600000 },\n  { id: 2, author: 'carol', type: 'video', text: 'Tutorial', timestamp: now - 1800000 },\n  { id: 3, author: 'dave', type: 'text', text: 'Hello!', timestamp: now - 7200000 },\n];\n\nconst ranked = rankFeed('alice', interactions, posts);\nranked.forEach((p, i) => console.log(`${i+1}. [${p.author}] ${p.text} (${p.score.toFixed(3)})`));",
      "output": "1. [bob] Sunset (0.540)\n2. [carol] Tutorial (0.300)\n3. [dave] Hello! (0.200)"
    },
    {
      "question": "Program 3: Feed diversity enforcer",
      "code": "function enforceDiversity(rankedPosts, rules) {\n  const result = [];\n  const authorCount = {};\n  const typeCount = {};\n  \n  for (const post of rankedPosts) {\n    const aCount = authorCount[post.author] || 0;\n    const tCount = typeCount[post.type] || 0;\n    if (aCount >= rules.maxPerAuthor) continue;\n    if (tCount >= rules.maxConsecutiveType && result.length > 0 && result[result.length-1].type === post.type) continue;\n    result.push(post);\n    authorCount[post.author] = aCount + 1;\n    typeCount[post.type] = tCount + 1;\n    if (result.length >= rules.feedSize) break;\n  }\n  return result;\n}\n\nconst posts = [\n  { id: 1, author: 'bob', type: 'video', score: 9 },\n  { id: 2, author: 'bob', type: 'video', score: 8.5 },\n  { id: 3, author: 'bob', type: 'image', score: 8 },\n  { id: 4, author: 'alice', type: 'video', score: 7.5 },\n  { id: 5, author: 'carol', type: 'text', score: 7 },\n  { id: 6, author: 'dave', type: 'image', score: 6.5 },\n];\n\nconst diverseFeed = enforceDiversity(posts, { maxPerAuthor: 2, maxConsecutiveType: 2, feedSize: 4 });\nconsole.log(diverseFeed.map(p => `${p.author}:${p.type}(${p.score})`));",
      "output": "[ 'bob:video(9)', 'bob:video(8.5)', 'alice:video(7.5)', 'carol:text(7)' ]"
    },
    {
      "question": "Program 4: Cursor-based pagination",
      "code": "class PaginatedFeed {\n  constructor(posts) {\n    this.posts = posts.sort((a, b) => b.score - a.score);\n  }\n  \n  getPage(cursor, pageSize) {\n    let startIndex = 0;\n    if (cursor) {\n      startIndex = this.posts.findIndex(p => p.id === cursor.afterId) + 1;\n    }\n    const items = this.posts.slice(startIndex, startIndex + pageSize);\n    const hasMore = startIndex + pageSize < this.posts.length;\n    const nextCursor = hasMore ? { afterId: items[items.length - 1].id } : null;\n    return { items, nextCursor, hasMore };\n  }\n}\n\nconst feed = new PaginatedFeed([\n  { id: 'a', title: 'Post A', score: 10 },\n  { id: 'b', title: 'Post B', score: 9 },\n  { id: 'c', title: 'Post C', score: 8 },\n  { id: 'd', title: 'Post D', score: 7 },\n  { id: 'e', title: 'Post E', score: 6 },\n]);\n\nconst page1 = feed.getPage(null, 2);\nconsole.log('Page 1:', page1.items.map(p => p.title), 'hasMore:', page1.hasMore);\nconst page2 = feed.getPage(page1.nextCursor, 2);\nconsole.log('Page 2:', page2.items.map(p => p.title), 'hasMore:', page2.hasMore);\nconst page3 = feed.getPage(page2.nextCursor, 2);\nconsole.log('Page 3:', page3.items.map(p => p.title), 'hasMore:', page3.hasMore);",
      "output": "Page 1: [ 'Post A', 'Post B' ] hasMore: true\nPage 2: [ 'Post C', 'Post D' ] hasMore: true\nPage 3: [ 'Post E' ] hasMore: false"
    },
    {
      "question": "Program 5: Affinity score calculator",
      "code": "function calculateAffinity(interactions) {\n  const weights = { like: 1, comment: 3, share: 5, message: 8, tag: 4 };\n  const affinityMap = {};\n  \n  interactions.forEach(({ user, target, type, daysAgo }) => {\n    const key = `${user}:${target}`;\n    const decay = Math.exp(-daysAgo / 30); // exponential decay over 30 days\n    const score = (weights[type] || 1) * decay;\n    affinityMap[key] = (affinityMap[key] || 0) + score;\n  });\n  \n  // Normalize to 0-1 range\n  const maxScore = Math.max(...Object.values(affinityMap));\n  Object.keys(affinityMap).forEach(k => {\n    affinityMap[k] = +(affinityMap[k] / maxScore).toFixed(3);\n  });\n  return affinityMap;\n}\n\nconst affinity = calculateAffinity([\n  { user: 'alice', target: 'bob', type: 'like', daysAgo: 1 },\n  { user: 'alice', target: 'bob', type: 'comment', daysAgo: 2 },\n  { user: 'alice', target: 'bob', type: 'message', daysAgo: 5 },\n  { user: 'alice', target: 'carol', type: 'like', daysAgo: 15 },\n  { user: 'alice', target: 'carol', type: 'like', daysAgo: 30 },\n]);\nconsole.log(affinity);",
      "output": "{ 'alice:bob': 1, 'alice:carol': 0.079 }"
    },
    {
      "question": "Program 6: Celebrity fanout handler",
      "code": "class HybridFanout {\n  constructor(celebrityThreshold) {\n    this.threshold = celebrityThreshold;\n    this.followers = new Map();\n    this.feedCache = new Map();\n    this.celebrityPosts = new Map(); // celebrity posts not fanned out\n  }\n  \n  setFollowers(userId, followerList) {\n    this.followers.set(userId, followerList);\n  }\n  \n  publish(authorId, post) {\n    const fCount = (this.followers.get(authorId) || []).length;\n    if (fCount > this.threshold) {\n      // Celebrity: store post, don't fanout\n      if (!this.celebrityPosts.has(authorId)) this.celebrityPosts.set(authorId, []);\n      this.celebrityPosts.get(authorId).push(post);\n      return { strategy: 'pull', followers: fCount };\n    } else {\n      // Regular: fanout on write\n      (this.followers.get(authorId) || []).forEach(f => {\n        if (!this.feedCache.has(f)) this.feedCache.set(f, []);\n        this.feedCache.get(f).push(post);\n      });\n      return { strategy: 'push', followers: fCount };\n    }\n  }\n  \n  getFeed(userId, followedCelebs) {\n    const cached = this.feedCache.get(userId) || [];\n    // Merge celebrity posts on read\n    const celeb = followedCelebs.flatMap(c => this.celebrityPosts.get(c) || []);\n    return [...cached, ...celeb].sort((a, b) => b.score - a.score);\n  }\n}\n\nconst fanout = new HybridFanout(2);\nfanout.setFollowers('celeb', ['alice', 'bob', 'carol']); // 3 > threshold 2\nfanout.setFollowers('friend', ['alice']); // 1 <= threshold\n\nconsole.log(fanout.publish('celeb', { id: 'cp1', text: 'Celebrity post', score: 10 }));\nconsole.log(fanout.publish('friend', { id: 'fp1', text: 'Friend post', score: 7 }));\nconsole.log('Alice feed:', fanout.getFeed('alice', ['celeb']));",
      "output": "{ strategy: 'pull', followers: 3 }\n{ strategy: 'push', followers: 1 }\nAlice feed: [\n  { id: 'cp1', text: 'Celebrity post', score: 10 },\n  { id: 'fp1', text: 'Friend post', score: 7 }\n]"
    },
    {
      "question": "Program 7: Feed deduplication",
      "code": "function deduplicateFeed(rawFeed) {\n  const seen = new Set();\n  const contentHashes = new Set();\n  const deduped = [];\n  \n  for (const item of rawFeed) {\n    // Skip by post ID\n    if (seen.has(item.id)) { continue; }\n    // Skip by content fingerprint (reshares)\n    const hash = item.contentHash || item.text;\n    if (contentHashes.has(hash)) {\n      deduped.push({ ...item, grouped: true, note: 'reshare merged' });\n      continue;\n    }\n    seen.add(item.id);\n    contentHashes.add(hash);\n    deduped.push(item);\n  }\n  return deduped;\n}\n\nconst raw = [\n  { id: 'p1', text: 'Original post', author: 'bob', contentHash: 'abc123' },\n  { id: 'p1', text: 'Original post', author: 'bob', contentHash: 'abc123' }, // dup by ID\n  { id: 'p2', text: 'Reshared post', author: 'carol', contentHash: 'abc123' }, // dup by content\n  { id: 'p3', text: 'Unique post', author: 'dave', contentHash: 'xyz789' },\n];\nconsole.log(deduplicateFeed(raw));",
      "output": "[\n  { id: 'p1', text: 'Original post', author: 'bob', contentHash: 'abc123' },\n  { id: 'p2', text: 'Reshared post', author: 'carol', contentHash: 'abc123', grouped: true, note: 'reshare merged' },\n  { id: 'p3', text: 'Unique post', author: 'dave', contentHash: 'xyz789' }\n]"
    },
    {
      "question": "Program 8: Real-time feed notification",
      "code": "class RealtimeFeedService {\n  constructor() {\n    this.subscribers = new Map(); // userId -> callback[]\n    this.throttleTimers = new Map();\n  }\n  \n  subscribe(userId, callback) {\n    if (!this.subscribers.has(userId)) this.subscribers.set(userId, []);\n    this.subscribers.get(userId).push(callback);\n  }\n  \n  notify(userId, event) {\n    const callbacks = this.subscribers.get(userId) || [];\n    // Throttle: max 1 notification per 5 seconds per user\n    if (this.throttleTimers.has(userId)) {\n      return { status: 'throttled', userId };\n    }\n    callbacks.forEach(cb => cb(event));\n    this.throttleTimers.set(userId, setTimeout(() => this.throttleTimers.delete(userId), 100));\n    return { status: 'delivered', userId, listeners: callbacks.length };\n  }\n}\n\nconst rtService = new RealtimeFeedService();\nrtService.subscribe('alice', (e) => console.log(`Alice got: ${e.type} - ${e.postId}`));\n\nconsole.log(rtService.notify('alice', { type: 'new_post', postId: 'p1' }));\nconsole.log(rtService.notify('alice', { type: 'new_post', postId: 'p2' })); // throttled",
      "output": "Alice got: new_post - p1\n{ status: 'delivered', userId: 'alice', listeners: 1 }\n{ status: 'throttled', userId: 'alice' }"
    },
    {
      "question": "Program 9: Feed metrics tracker",
      "code": "class FeedMetrics {\n  constructor() { this.sessions = []; }\n  \n  record(session) { this.sessions.push(session); }\n  \n  aggregate() {\n    const n = this.sessions.length;\n    if (n === 0) return null;\n    const avg = (arr, key) => arr.reduce((s, v) => s + v[key], 0) / arr.length;\n    return {\n      totalSessions: n,\n      avgTimeSpentSec: Math.round(avg(this.sessions, 'timeSpentSec')),\n      avgPostsViewed: Math.round(avg(this.sessions, 'postsViewed')),\n      avgLikesGiven: +avg(this.sessions, 'likes').toFixed(1),\n      scrollDepth: {\n        shallow: this.sessions.filter(s => s.postsViewed < 10).length,\n        medium: this.sessions.filter(s => s.postsViewed >= 10 && s.postsViewed < 30).length,\n        deep: this.sessions.filter(s => s.postsViewed >= 30).length,\n      }\n    };\n  }\n}\n\nconst metrics = new FeedMetrics();\nmetrics.record({ timeSpentSec: 120, postsViewed: 25, likes: 3 });\nmetrics.record({ timeSpentSec: 300, postsViewed: 50, likes: 8 });\nmetrics.record({ timeSpentSec: 30, postsViewed: 5, likes: 0 });\nmetrics.record({ timeSpentSec: 180, postsViewed: 35, likes: 5 });\nconsole.log(metrics.aggregate());",
      "output": "{\n  totalSessions: 4,\n  avgTimeSpentSec: 158,\n  avgPostsViewed: 29,\n  avgLikesGiven: 4,\n  scrollDepth: { shallow: 1, medium: 1, deep: 2 }\n}"
    },
    {
      "question": "Program 10: Story candidate generator",
      "code": "function generateCandidates(userId, friends, pages, postStore, maxAge) {\n  const cutoff = Date.now() - maxAge;\n  const candidates = [];\n  const sources = [...friends.map(f => ({ id: f, type: 'friend' })), ...pages.map(p => ({ id: p, type: 'page' }))];\n  \n  sources.forEach(source => {\n    const posts = (postStore[source.id] || []).filter(p => p.timestamp >= cutoff);\n    posts.forEach(p => candidates.push({ ...p, sourceType: source.type, source: source.id }));\n  });\n  \n  return { userId, totalCandidates: candidates.length, byType: {\n    friend: candidates.filter(c => c.sourceType === 'friend').length,\n    page: candidates.filter(c => c.sourceType === 'page').length,\n  }, sample: candidates.slice(0, 3) };\n}\n\nconst now = Date.now();\nconst postStore = {\n  'bob': [{ id: 'p1', text: 'Hi', timestamp: now - 3600000 }],\n  'carol': [{ id: 'p2', text: 'Hey', timestamp: now - 86400000 }],\n  'techPage': [\n    { id: 'p3', text: 'News 1', timestamp: now - 1800000 },\n    { id: 'p4', text: 'News 2', timestamp: now - 7200000 },\n  ],\n};\n\nconsole.log(generateCandidates('alice', ['bob', 'carol'], ['techPage'], postStore, 7 * 86400000));",
      "output": "{\n  userId: 'alice',\n  totalCandidates: 4,\n  byType: { friend: 2, page: 2 },\n  sample: [\n    { id: 'p1', text: 'Hi', sourceType: 'friend', source: 'bob' },\n    { id: 'p2', text: 'Hey', sourceType: 'friend', source: 'carol' },\n    { id: 'p3', text: 'News 1', sourceType: 'page', source: 'techPage' }\n  ]\n}"
    }
  ]
}
