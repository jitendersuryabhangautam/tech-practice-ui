{
  "id": "microservices",
  "title": "Microservices Patterns",
  "category": "Foundations",
  "description": "Microservices patterns are proven architectural solutions for building, deploying, and managing distributed systems composed of small, independently deployable services that communicate over well-defined APIs.",
  "explanation": "Microservices architecture decomposes a monolithic application into small, autonomous services, each owning its own data and business logic. Two foundational patterns enable this: **Service Discovery** allows services to locate each other dynamically (client-side via a registry like Consul/Eureka, or server-side via a load balancer). **API Gateway** acts as the single entry point for all clients, handling routing, authentication, rate limiting, and response aggregation — so internal service topology stays hidden.\n\nResilience patterns prevent cascading failures in distributed systems. The **Circuit Breaker** pattern (states: CLOSED → OPEN → HALF_OPEN) stops a service from repeatedly calling a failing downstream dependency, giving it time to recover. The **Bulkhead** pattern isolates failures by partitioning resources (thread pools, connection pools) per dependency, so a slow service can't exhaust all resources. **Retry with Exponential Backoff** handles transient failures by retrying with increasing delays (e.g., 100ms, 200ms, 400ms) plus jitter to avoid thundering herds.\n\nData consistency across services is one of the hardest problems. The **Saga Pattern** manages distributed transactions by breaking them into a sequence of local transactions, each with a compensating action for rollback. Sagas come in two flavors: orchestration (a central coordinator drives the flow) and choreography (services emit and react to events). **Event-Driven Communication** decouples services using asynchronous messaging (Kafka, RabbitMQ), enabling eventual consistency and better scalability than synchronous REST calls.\n\nThe **Sidecar / Service Mesh** pattern (Istio, Linkerd) deploys a proxy alongside each service to handle cross-cutting concerns — mTLS, observability, traffic shaping, retries — without changing application code. **Health Checks** (liveness and readiness probes) let orchestrators like Kubernetes know whether a service instance is alive and ready to receive traffic, enabling automatic restarts and load balancer draining.\n\nMigration and decomposition patterns help evolve systems incrementally. The **Strangler Fig** pattern gradually replaces a monolith by routing new functionality to microservices while old code remains operational, eventually decommissioning the monolith piece by piece. This avoids risky big-bang rewrites and allows teams to validate the microservices approach incrementally.",
  "code": "// Circuit Breaker Pattern — Full Implementation\n// States: CLOSED (normal), OPEN (failing, block calls), HALF_OPEN (testing recovery)\n\nclass CircuitBreaker {\n  constructor(options = {}) {\n    this.state = 'CLOSED';\n    this.failureCount = 0;\n    this.successCount = 0;\n    this.failureThreshold = options.failureThreshold || 5;\n    this.successThreshold = options.successThreshold || 3;\n    this.timeout = options.timeout || 10000; // ms before trying HALF_OPEN\n    this.nextAttempt = Date.now();\n    this.monitor = [];  // event log\n  }\n\n  log(event) {\n    this.monitor.push({ time: new Date().toISOString(), ...event });\n  }\n\n  async call(fn) {\n    if (this.state === 'OPEN') {\n      if (Date.now() < this.nextAttempt) {\n        this.log({ action: 'BLOCKED', state: this.state });\n        throw new Error('Circuit is OPEN — request blocked');\n      }\n      // Timeout elapsed — transition to HALF_OPEN\n      this.state = 'HALF_OPEN';\n      this.log({ action: 'TRANSITION', from: 'OPEN', to: 'HALF_OPEN' });\n    }\n\n    try {\n      const result = await fn();\n      this.onSuccess();\n      return result;\n    } catch (error) {\n      this.onFailure();\n      throw error;\n    }\n  }\n\n  onSuccess() {\n    if (this.state === 'HALF_OPEN') {\n      this.successCount++;\n      if (this.successCount >= this.successThreshold) {\n        this.state = 'CLOSED';\n        this.failureCount = 0;\n        this.successCount = 0;\n        this.log({ action: 'TRANSITION', from: 'HALF_OPEN', to: 'CLOSED' });\n      } else {\n        this.log({ action: 'HALF_OPEN_SUCCESS', count: this.successCount });\n      }\n    } else {\n      this.failureCount = 0; // reset on success in CLOSED state\n      this.log({ action: 'SUCCESS', state: this.state });\n    }\n  }\n\n  onFailure() {\n    this.failureCount++;\n    this.log({ action: 'FAILURE', count: this.failureCount, state: this.state });\n\n    if (this.state === 'HALF_OPEN' || this.failureCount >= this.failureThreshold) {\n      this.state = 'OPEN';\n      this.nextAttempt = Date.now() + this.timeout;\n      this.successCount = 0;\n      this.log({ action: 'TRANSITION', to: 'OPEN', retryAt: new Date(this.nextAttempt).toISOString() });\n    }\n  }\n\n  getState() {\n    return {\n      state: this.state,\n      failureCount: this.failureCount,\n      successCount: this.successCount,\n    };\n  }\n}\n\n// Usage example\nconst breaker = new CircuitBreaker({ failureThreshold: 3, successThreshold: 2, timeout: 5000 });\n\nasync function callService() {\n  return breaker.call(async () => {\n    // Simulate an unreliable service call\n    if (Math.random() < 0.6) throw new Error('Service unavailable');\n    return { status: 'ok', data: 'response payload' };\n  });\n}\n\n// Run several calls\n(async () => {\n  for (let i = 0; i < 8; i++) {\n    try {\n      const result = await callService();\n      console.log(`Call ${i + 1}: SUCCESS`, result);\n    } catch (e) {\n      console.log(`Call ${i + 1}: FAILED —`, e.message);\n    }\n    console.log('  State:', breaker.getState());\n  }\n})();",
  "example": "// Saga Orchestrator Pattern — Order Flow\n// Steps: Reserve Inventory → Charge Payment → Confirm Order\n// Each step has a compensating action for rollback on failure\n\nclass SagaOrchestrator {\n  constructor(name) {\n    this.name = name;\n    this.steps = [];\n    this.completedSteps = [];\n  }\n\n  addStep(name, execute, compensate) {\n    this.steps.push({ name, execute, compensate });\n    return this; // fluent API\n  }\n\n  async run(context) {\n    console.log(`[Saga:${this.name}] Starting...`);\n    for (const step of this.steps) {\n      try {\n        console.log(`  → Executing: ${step.name}`);\n        const result = await step.execute(context);\n        context = { ...context, ...result };\n        this.completedSteps.push(step);\n        console.log(`  ✓ ${step.name} succeeded`);\n      } catch (error) {\n        console.log(`  ✗ ${step.name} failed: ${error.message}`);\n        await this.rollback(context);\n        return { success: false, failedAt: step.name, error: error.message, context };\n      }\n    }\n    console.log(`[Saga:${this.name}] Completed successfully!`);\n    return { success: true, context };\n  }\n\n  async rollback(context) {\n    console.log(`  ⟲ Rolling back ${this.completedSteps.length} steps...`);\n    for (const step of [...this.completedSteps].reverse()) {\n      try {\n        console.log(`    ↩ Compensating: ${step.name}`);\n        await step.compensate(context);\n        console.log(`    ✓ ${step.name} compensated`);\n      } catch (err) {\n        console.log(`    ✗ Compensation failed for ${step.name}: ${err.message}`);\n        // In production: log to dead letter queue for manual resolution\n      }\n    }\n  }\n}\n\n// Simulated service calls\nconst reserveInventory = async (ctx) => {\n  console.log(`    Reserving ${ctx.quantity}x ${ctx.item}`);\n  return { reservationId: 'RES-' + Date.now() };\n};\nconst releaseInventory = async (ctx) => {\n  console.log(`    Releasing reservation ${ctx.reservationId}`);\n};\n\nconst chargePayment = async (ctx) => {\n  console.log(`    Charging $${ctx.amount} to ${ctx.paymentMethod}`);\n  // Simulate payment failure for amounts over 1000\n  if (ctx.amount > 1000) throw new Error('Payment declined: insufficient funds');\n  return { paymentId: 'PAY-' + Date.now() };\n};\nconst refundPayment = async (ctx) => {\n  console.log(`    Refunding payment ${ctx.paymentId}`);\n};\n\nconst confirmOrder = async (ctx) => {\n  console.log(`    Confirming order for ${ctx.item}`);\n  return { orderId: 'ORD-' + Date.now(), status: 'confirmed' };\n};\nconst cancelOrder = async (ctx) => {\n  console.log(`    Cancelling order ${ctx.orderId}`);\n};\n\n// Build and execute saga\nconst orderSaga = new SagaOrchestrator('CreateOrder')\n  .addStep('ReserveInventory', reserveInventory, releaseInventory)\n  .addStep('ChargePayment', chargePayment, refundPayment)\n  .addStep('ConfirmOrder', confirmOrder, cancelOrder);\n\n// Test 1: Successful order (amount <= 1000)\norderSaga.run({ item: 'Laptop', quantity: 1, amount: 999, paymentMethod: 'credit_card' })\n  .then(r => console.log('Result:', JSON.stringify(r, null, 2)));\n\n// Test 2: Failed order (amount > 1000 triggers payment failure)\n// const failSaga = new SagaOrchestrator('CreateOrder')\n//   .addStep('ReserveInventory', reserveInventory, releaseInventory)\n//   .addStep('ChargePayment', chargePayment, refundPayment)\n//   .addStep('ConfirmOrder', confirmOrder, cancelOrder);\n// failSaga.run({ item: 'Server', quantity: 1, amount: 5000, paymentMethod: 'credit_card' });",
  "useCase": "Microservices patterns are essential for: (1) E-commerce platforms — saga pattern for distributed order processing across inventory, payment, and shipping services; circuit breakers preventing cascading failures during flash sales. (2) Streaming platforms — API gateway aggregating content catalog, recommendations, and user profile services; event-driven communication for real-time view tracking. (3) Financial systems — bulkhead isolation between trading, settlement, and reporting services; retry with backoff for external payment provider calls. (4) SaaS multi-tenant platforms — service mesh for zero-trust networking; strangler fig migration from monolith to microservices. (5) Ride-sharing apps — service discovery for geographically distributed driver matching services; health checks for auto-scaling during demand spikes.",
  "interviewQuestions": [
    {
      "question": "How does service discovery work in a microservices architecture, and what are the two main approaches?",
      "answer": "Service discovery enables services to find each other's network locations dynamically. Client-side discovery: the client queries a service registry (e.g., Consul, Eureka) and selects an instance using a load-balancing algorithm. Server-side discovery: the client sends requests to a load balancer (e.g., AWS ALB, Kubernetes Service), which queries the registry and forwards the request. Client-side gives more control but couples clients to the registry; server-side is simpler for clients but adds a network hop."
    },
    {
      "question": "Explain the circuit breaker pattern and its three states. When would you use it?",
      "answer": "The circuit breaker prevents cascading failures when a downstream service is failing. CLOSED state: requests pass through; failures are counted. When failures exceed a threshold, it transitions to OPEN. OPEN state: all requests are immediately rejected without calling the downstream service, giving it time to recover. After a timeout, it moves to HALF_OPEN. HALF_OPEN state: a limited number of test requests are allowed through. If they succeed (above a success threshold), the circuit closes; if they fail, it reopens. Use it for any remote call that could hang or fail repeatedly — database connections, HTTP calls, third-party APIs."
    },
    {
      "question": "What is the saga pattern, and how do orchestration and choreography differ?",
      "answer": "The saga pattern manages distributed transactions across multiple services by breaking them into a sequence of local transactions, each with a compensating (rollback) action. Orchestration: a central saga coordinator tells each service what to do and handles failures — easier to understand and debug, but creates a single point of coordination. Choreography: each service listens for events and reacts independently — more decoupled and scalable, but harder to trace and debug the overall flow. Choose orchestration for complex flows with many steps; choreography for simpler, highly decoupled systems."
    },
    {
      "question": "What responsibilities does an API gateway handle, and why not let clients call services directly?",
      "answer": "An API gateway handles: request routing, authentication/authorization, rate limiting, response aggregation, protocol translation (REST to gRPC), caching, and SSL termination. Without a gateway, clients would need to know every service's address, handle multiple protocols, and make multiple round trips for composite data. The gateway simplifies client logic, hides internal topology, and provides a single place to enforce security policies. Trade-off: it's a potential single point of failure and bottleneck, so it needs horizontal scaling, health checks, and careful latency budgeting."
    },
    {
      "question": "Compare synchronous (REST/gRPC) and asynchronous (messaging) communication patterns between microservices.",
      "answer": "Synchronous: the caller waits for a response, simpler to implement and reason about, works well for queries and operations needing immediate confirmation. Downsides: temporal coupling (both services must be available), cascading latency, tight coupling. Asynchronous: services communicate via a message broker (Kafka, RabbitMQ), providing temporal decoupling, better fault tolerance, natural load leveling, and enabling event-driven architectures. Downsides: eventual consistency, harder debugging, message ordering challenges, need for idempotent consumers. Best practice: use sync for reads and user-facing operations requiring immediate feedback; async for writes, notifications, and cross-service data propagation."
    },
    {
      "question": "How do you maintain data consistency across microservices that each own their own database?",
      "answer": "Each microservice owns its data (Database per Service pattern), so you can't use traditional ACID transactions. Strategies: (1) Saga pattern for distributed transactions with compensation. (2) Event sourcing — store state changes as immutable events; other services consume and build their own projections. (3) Change Data Capture (CDC) — capture database changes via WAL (e.g., Debezium) and publish to a message broker. (4) Outbox pattern — write events to an outbox table in the same transaction as the data change, then publish asynchronously. (5) CQRS — separate read and write models, with eventual consistency between them. The key principle: embrace eventual consistency and design compensating actions for failure cases."
    },
    {
      "question": "What is a service mesh, and what problems does it solve?",
      "answer": "A service mesh (Istio, Linkerd, Consul Connect) is an infrastructure layer that manages service-to-service communication. It deploys a sidecar proxy alongside each service instance to handle: mutual TLS (encryption and identity), traffic management (canary deployments, traffic splitting), observability (distributed tracing, metrics, access logs), resilience (retries, circuit breaking, timeouts), and policy enforcement. It solves the problem of implementing these cross-cutting concerns in every service's code. Trade-offs: added operational complexity, increased latency from the proxy hop (~1ms), resource overhead per sidecar, and a steep learning curve."
    },
    {
      "question": "How do you decompose a monolith into microservices? What boundaries guide the split?",
      "answer": "Decomposition strategies: (1) Domain-Driven Design (DDD): identify bounded contexts — each becomes a service. (2) Business capability: align services with business functions (orders, payments, inventory). (3) Subdomain: core domain gets the most investment; supporting and generic subdomains can be simpler services or third-party. Start with the Strangler Fig pattern: intercept requests at the edge, route new features to microservices, and gradually migrate existing functionality. Avoid splitting by technical layer (UI service, DB service) — this creates chatty services. Key signals for a good boundary: independent deployability, single team ownership, minimal cross-service transactions, and cohesive data ownership."
    },
    {
      "question": "How does distributed tracing work, and why is it critical for microservices?",
      "answer": "Distributed tracing tracks a request as it flows through multiple services by propagating a correlation/trace ID (e.g., via HTTP headers like X-Request-Id or W3C traceparent). Each service creates spans with timing data, parent-child relationships, and metadata. Tools like Jaeger, Zipkin, or OpenTelemetry collect and visualize these traces. It is critical because: debugging in microservices is hard — a single user request may touch 10+ services. Tracing reveals latency bottlenecks, error sources, and dependency chains. Without it, you're flying blind. Implementation: instrument at the API gateway (inject trace ID), propagate through all service calls, and export spans to a tracing backend."
    },
    {
      "question": "What deployment strategies work best for microservices, and how do you handle rollbacks?",
      "answer": "Strategies: (1) Blue-green: run two identical environments; switch traffic atomically — instant rollback by switching back. (2) Canary: route a small percentage (1-5%) of traffic to the new version; monitor error rates and latency before full rollout. (3) Rolling update: gradually replace instances (Kubernetes default) — zero downtime but harder to rollback mid-way. (4) Feature flags: deploy code but toggle features independently of deployment. Rollback approach: keep the previous version's containers/artifacts ready, use database migrations that are backward-compatible (expand-then-contract), and ensure APIs are versioned so old and new versions can coexist. In Kubernetes, rollback with `kubectl rollout undo`. Critical: never deploy breaking database schema changes and new code in the same release."
    }
  ],
  "exercises": [
    {
      "type": "design",
      "question": "Design a microservices architecture for an e-commerce checkout flow. Identify the services, communication patterns, and how you'd handle a payment failure after inventory has been reserved.",
      "answer": "Services: Cart Service, Inventory Service, Payment Service, Order Service, Notification Service. Use a saga orchestrator in Order Service: Step 1 — reserve inventory (sync gRPC call), Step 2 — charge payment (sync), Step 3 — confirm order (local). On payment failure: saga triggers compensation — call Inventory Service to release reservation. Communication: sync REST/gRPC for the checkout flow (user waiting), async events (Kafka) for notifications and analytics. Each service owns its DB. The Order Service maintains saga state in a saga_log table for crash recovery."
    },
    {
      "type": "scenario",
      "question": "Your circuit breaker is tripping frequently for a downstream service that has intermittent 500 errors (5% of requests). Users are seeing errors even though 95% of calls would succeed. How do you tune it?",
      "answer": "Increase the failure threshold (e.g., from 5 to 20 consecutive failures) so intermittent errors don't trip the breaker. Use a sliding window (time-based or count-based) instead of a simple counter — e.g., open only if >30% of requests in the last 60 seconds fail. Reduce the OPEN timeout so recovery is tested faster. Add retry with backoff before the circuit breaker layer so transient errors are retried without counting as breaker failures. Consider a half-open strategy that allows more test requests (e.g., 10% traffic) rather than a single probe."
    },
    {
      "type": "tricky",
      "question": "In a choreography-based saga, Service A publishes 'OrderCreated', Service B processes it and publishes 'PaymentCharged', but Service B crashes right after committing to its DB but before publishing the event. What happens and how do you fix it?",
      "answer": "The payment is charged but no event is published — causing the saga to hang. This is the dual-write problem. Fix with the Transactional Outbox pattern: Service B writes the event to an 'outbox' table in the same database transaction as the payment. A separate process (CDC via Debezium, or a polling publisher) reads the outbox and publishes to the message broker. This guarantees at-least-once delivery. Consumers must be idempotent — use the event's unique ID to deduplicate. Alternative: use event sourcing where the event IS the source of truth."
    },
    {
      "type": "estimation",
      "question": "You have 50 microservices, each making an average of 3 downstream calls per request. The system handles 10,000 requests/sec at the API gateway. Estimate the total internal service-to-service calls per second and the number of distributed traces generated per minute.",
      "answer": "First hop: 10,000 req/s to the gateway. Each request fans out to ~3 downstream calls. If each downstream service also makes 1-2 further calls, the total internal call volume is approximately: 10,000 × (3 + 3×1.5) ≈ 75,000 internal calls/sec. Distributed traces: 1 trace per inbound request = 10,000 traces/sec = 600,000 traces/min. Each trace contains ~7-10 spans (3 + downstream calls). At 1KB per span, tracing data ≈ 10,000 × 10 × 1KB = 100MB/sec = ~8.6TB/day. You'll need sampling (e.g., 10%) in production to keep this manageable."
    },
    {
      "type": "debug",
      "question": "Your API gateway returns 504 Gateway Timeout for 20% of requests to the /orders endpoint. The Order Service logs show normal response times (<100ms). What steps would you take to diagnose and fix this?",
      "answer": "Step 1: Check the gateway's timeout configuration — it may be too low. Step 2: Check connection pool exhaustion at the gateway — if the pool to Order Service is full, new requests queue and timeout. Step 3: Look at DNS resolution — the gateway may be resolving to stale/dead instances (service discovery issue). Step 4: Check network — look for packet loss or latency between gateway and Order Service (different AZ/region?). Step 5: Inspect gateway access logs — confirm the requests are actually being forwarded. Step 6: Check if Order Service has a readiness probe failing, causing the load balancer to have fewer healthy targets. Fix: increase connection pool size, set proper timeouts with retries, ensure health checks are correctly configured, and add circuit breaker at the gateway."
    },
    {
      "type": "framework",
      "question": "List a decision framework for choosing between synchronous and asynchronous communication for 5 common microservice interaction scenarios.",
      "answer": "1) User login/authentication → Sync: user is waiting, needs immediate response. 2) Order placement notification to warehouse → Async: warehouse can process when ready, no user waiting. 3) Fetching product details for display → Sync: user needs data immediately to render page. 4) Updating search index after product change → Async: eventual consistency is acceptable, decouple from write path. 5) Payment processing → Sync for initial charge (user waiting for confirmation), then Async for receipt email and accounting ledger update. Framework: If the user is waiting AND needs the result to proceed, use sync. If it's a side effect, background process, or can tolerate delay, use async."
    },
    {
      "type": "scenario",
      "question": "You deploy a new version of the Payment Service that changes the response schema. Requests from the Order Service start failing with deserialization errors. How do you prevent this in the future?",
      "answer": "Use contract-first API development with schema versioning. Strategies: (1) API versioning (URL: /v1/payments, /v2/payments) — old consumers keep calling v1. (2) Backward-compatible changes only: add new fields with defaults, never remove or rename fields without deprecation. (3) Consumer-driven contract testing (Pact): consumers define expected schemas, provider CI validates against all consumer contracts before deployment. (4) Schema registry (for async events): enforce backward/forward compatibility checks on publish. (5) Tolerant reader pattern: consumers ignore unknown fields. Immediate fix: roll back Payment Service, then deploy schema changes following the expand-contract pattern."
    },
    {
      "type": "design",
      "question": "Design a health check system for 100 microservices running in Kubernetes. Include liveness, readiness, and startup probes with meaningful checks.",
      "answer": "Liveness probe (/healthz): checks if the process is alive and not deadlocked. Simple — return 200 if event loop is responsive. Fails → Kubernetes restarts the pod. Readiness probe (/readyz): checks if the service can handle traffic — verifies DB connection pool, cache connection, and that configuration is loaded. Fails → pod removed from Service load balancer (no restart). Startup probe (/startupz): used for slow-starting services — gives extra time before liveness kicks in (e.g., 300s for ML model loading). Implementation: each service exposes HTTP endpoints with JSON status. Aggregate dashboard: a central health aggregator polls all services every 30s, stores results in a time-series DB (Prometheus), and visualizes in Grafana with alerts for degraded services."
    },
    {
      "type": "output",
      "question": "A bulkhead partitions a thread pool into 3 pools: ServiceA (10 threads), ServiceB (5 threads), ServiceC (5 threads). ServiceB receives 20 concurrent requests. How many ServiceB requests are rejected, and does ServiceA's availability change?",
      "answer": "ServiceB has a pool of 5 threads. With 20 concurrent requests, 5 execute immediately and 15 are rejected (or queued, depending on config — if no queue, all 15 rejected immediately). ServiceA's availability is completely unaffected — its 10-thread pool is independent. This is exactly the bulkhead's purpose: ServiceB's overload cannot starve ServiceA or ServiceC of resources. Without the bulkhead, all 20 threads might come from a shared pool of 20, potentially consuming all threads and making ServiceA and ServiceC unavailable too."
    },
    {
      "type": "estimation",
      "question": "Estimate the operational overhead of adding a service mesh (Istio) sidecar to each of your 200 microservice pods, each running 3 replicas.",
      "answer": "Total sidecar proxies: 200 services × 3 replicas = 600 Envoy sidecars. Each Envoy proxy uses ~50MB RAM and ~0.1 CPU cores at moderate traffic. Total overhead: 600 × 50MB = 30GB RAM, 600 × 0.1 = 60 CPU cores. Added latency: ~1-2ms per hop for proxy-to-proxy communication. If a request traverses 4 services, that's 4-8ms added latency. Control plane (istiod): ~2-4 instances, 1GB RAM each. Total cost: roughly 3-4 additional nodes (8 CPU, 32GB each) just for the mesh infrastructure. This is ~5-10% overhead. Worth it if you need mTLS, advanced traffic management, and unified observability at scale."
    }
  ],
  "programExercises": [
    {
      "question": "Program 1: Circuit Breaker with state transitions and failure counting",
      "code": "class CircuitBreaker {\n  constructor(threshold, timeout) {\n    this.state = 'CLOSED';\n    this.failures = 0;\n    this.threshold = threshold;\n    this.timeout = timeout;\n    this.openedAt = null;\n    this.log = [];\n  }\n\n  call(fn) {\n    if (this.state === 'OPEN') {\n      if (Date.now() - this.openedAt >= this.timeout) {\n        this.state = 'HALF_OPEN';\n        this.log.push('OPEN -> HALF_OPEN');\n      } else {\n        this.log.push('BLOCKED (OPEN)');\n        return 'BLOCKED';\n      }\n    }\n    try {\n      const result = fn();\n      if (this.state === 'HALF_OPEN') {\n        this.state = 'CLOSED';\n        this.failures = 0;\n        this.log.push('HALF_OPEN -> CLOSED');\n      }\n      return result;\n    } catch (e) {\n      this.failures++;\n      if (this.failures >= this.threshold || this.state === 'HALF_OPEN') {\n        this.state = 'OPEN';\n        this.openedAt = Date.now();\n        this.log.push(`OPENED (failures=${this.failures})`);\n      }\n      return 'FAILED';\n    }\n  }\n}\n\nconst cb = new CircuitBreaker(3, 100);\nconst fail = () => { throw new Error('err'); };\nconst ok = () => 'OK';\n\nconsole.log(cb.call(fail));   // FAILED\nconsole.log(cb.call(fail));   // FAILED\nconsole.log(cb.call(fail));   // FAILED -> opens\nconsole.log(cb.call(ok));     // BLOCKED\nconsole.log('State:', cb.state);\nconsole.log('Log:', cb.log);",
      "output": "FAILED\nFAILED\nFAILED\nBLOCKED\nState: OPEN\nLog: [ 'OPENED (failures=3)', 'BLOCKED (OPEN)' ]"
    },
    {
      "question": "Program 2: Retry with exponential backoff and jitter",
      "code": "function retryWithBackoff(fn, maxRetries, baseDelay) {\n  const attempts = [];\n  for (let i = 0; i <= maxRetries; i++) {\n    const delay = i === 0 ? 0 : baseDelay * Math.pow(2, i - 1);\n    const jitter = Math.floor(delay * 0.1); // deterministic for demo\n    const totalDelay = delay + jitter;\n    try {\n      const result = fn(i);\n      attempts.push({ attempt: i + 1, delay: totalDelay, status: 'SUCCESS' });\n      return { result, attempts };\n    } catch (e) {\n      attempts.push({ attempt: i + 1, delay: totalDelay, status: 'FAILED', error: e.message });\n    }\n  }\n  return { result: null, attempts };\n}\n\nlet callCount = 0;\nconst unreliable = (attempt) => {\n  callCount++;\n  if (callCount < 4) throw new Error(`Transient failure #${callCount}`);\n  return { data: 'success on attempt ' + (attempt + 1) };\n};\n\nconst result = retryWithBackoff(unreliable, 5, 100);\nconsole.log('Result:', result.result);\nconsole.log('Attempts:');\nresult.attempts.forEach(a => console.log(`  #${a.attempt}: ${a.status} (delay: ${a.delay}ms)`));",
      "output": "Result: { data: 'success on attempt 4' }\nAttempts:\n  #1: FAILED (delay: 0ms)\n  #2: FAILED (delay: 110ms)\n  #3: FAILED (delay: 220ms)\n  #4: SUCCESS (delay: 440ms)"
    },
    {
      "question": "Program 3: Service Registry with registration, deregistration, and lookup",
      "code": "class ServiceRegistry {\n  constructor() {\n    this.services = new Map();\n  }\n\n  register(name, instance) {\n    if (!this.services.has(name)) this.services.set(name, []);\n    this.services.get(name).push({ ...instance, registeredAt: Date.now() });\n    return `Registered ${instance.id} for ${name}`;\n  }\n\n  deregister(name, instanceId) {\n    if (!this.services.has(name)) return 'Service not found';\n    const instances = this.services.get(name).filter(i => i.id !== instanceId);\n    this.services.set(name, instances);\n    return `Deregistered ${instanceId} from ${name}`;\n  }\n\n  discover(name) {\n    const instances = this.services.get(name) || [];\n    if (instances.length === 0) return null;\n    // Round-robin selection\n    const idx = Math.floor(Date.now() % instances.length);\n    return instances[idx];\n  }\n\n  listAll() {\n    const result = {};\n    for (const [name, instances] of this.services) {\n      result[name] = instances.map(i => i.id);\n    }\n    return result;\n  }\n}\n\nconst registry = new ServiceRegistry();\nconsole.log(registry.register('user-service', { id: 'us-1', host: '10.0.0.1', port: 3001 }));\nconsole.log(registry.register('user-service', { id: 'us-2', host: '10.0.0.2', port: 3001 }));\nconsole.log(registry.register('order-service', { id: 'os-1', host: '10.0.0.3', port: 3002 }));\nconsole.log('All services:', registry.listAll());\nconsole.log(registry.deregister('user-service', 'us-1'));\nconsole.log('After deregister:', registry.listAll());",
      "output": "Registered us-1 for user-service\nRegistered us-2 for user-service\nRegistered os-1 for order-service\nAll services: { 'user-service': [ 'us-1', 'us-2' ], 'order-service': [ 'os-1' ] }\nDeregistered us-1 from user-service\nAfter deregister: { 'user-service': [ 'us-2' ], 'order-service': [ 'os-1' ] }"
    },
    {
      "question": "Program 4: Health Checker with liveness and readiness probes",
      "code": "class HealthChecker {\n  constructor(serviceName) {\n    this.serviceName = serviceName;\n    this.checks = [];\n  }\n\n  addCheck(name, type, checkFn) {\n    this.checks.push({ name, type, checkFn });\n    return this;\n  }\n\n  async runChecks() {\n    const results = [];\n    for (const check of this.checks) {\n      try {\n        const ok = await check.checkFn();\n        results.push({ name: check.name, type: check.type, status: ok ? 'UP' : 'DOWN' });\n      } catch (e) {\n        results.push({ name: check.name, type: check.type, status: 'DOWN', error: e.message });\n      }\n    }\n    const liveness = results.filter(r => r.type === 'liveness').every(r => r.status === 'UP');\n    const readiness = results.filter(r => r.type === 'readiness').every(r => r.status === 'UP');\n    return {\n      service: this.serviceName,\n      alive: liveness,\n      ready: readiness,\n      checks: results,\n    };\n  }\n}\n\nconst checker = new HealthChecker('order-service')\n  .addCheck('event-loop', 'liveness', () => true)\n  .addCheck('memory', 'liveness', () => process.memoryUsage().heapUsed < 500 * 1024 * 1024)\n  .addCheck('database', 'readiness', () => true)  // simulate DB connected\n  .addCheck('cache', 'readiness', () => false);    // simulate cache down\n\nchecker.runChecks().then(r => console.log(JSON.stringify(r, null, 2)));",
      "output": "{\n  \"service\": \"order-service\",\n  \"alive\": true,\n  \"ready\": false,\n  \"checks\": [\n    { \"name\": \"event-loop\", \"type\": \"liveness\", \"status\": \"UP\" },\n    { \"name\": \"memory\", \"type\": \"liveness\", \"status\": \"UP\" },\n    { \"name\": \"database\", \"type\": \"readiness\", \"status\": \"UP\" },\n    { \"name\": \"cache\", \"type\": \"readiness\", \"status\": \"DOWN\" }\n  ]\n}"
    },
    {
      "question": "Program 5: API Gateway Router with path matching, middleware, and request forwarding",
      "code": "class APIGateway {\n  constructor() {\n    this.routes = [];\n    this.middleware = [];\n  }\n\n  use(fn) { this.middleware.push(fn); }\n\n  route(path, service) {\n    this.routes.push({ path, service });\n  }\n\n  handle(request) {\n    const log = [];\n    // Run middleware\n    for (const mw of this.middleware) {\n      const result = mw(request);\n      if (result.blocked) {\n        log.push(`Middleware blocked: ${result.reason}`);\n        return { status: 403, body: result.reason, log };\n      }\n      log.push(`Middleware passed: ${result.name}`);\n    }\n    // Find route\n    const match = this.routes.find(r => request.path.startsWith(r.path));\n    if (!match) {\n      log.push('No route matched');\n      return { status: 404, body: 'Not Found', log };\n    }\n    log.push(`Routed to: ${match.service}`);\n    return { status: 200, body: `Response from ${match.service}`, log };\n  }\n}\n\nconst gw = new APIGateway();\ngw.use(req => ({ name: 'auth', blocked: !req.token, reason: 'Unauthorized' }));\ngw.use(req => ({ name: 'rateLimit', blocked: false }));\ngw.route('/api/users', 'user-service:3001');\ngw.route('/api/orders', 'order-service:3002');\ngw.route('/api/products', 'product-service:3003');\n\nconsole.log('Req 1:', gw.handle({ path: '/api/orders/123', token: 'abc' }));\nconsole.log('Req 2:', gw.handle({ path: '/api/unknown', token: 'abc' }));\nconsole.log('Req 3:', gw.handle({ path: '/api/users', token: null }));",
      "output": "Req 1: { status: 200, body: 'Response from order-service:3002', log: [ 'Middleware passed: auth', 'Middleware passed: rateLimit', 'Routed to: order-service:3002' ] }\nReq 2: { status: 404, body: 'Not Found', log: [ 'Middleware passed: auth', 'Middleware passed: rateLimit', 'No route matched' ] }\nReq 3: { status: 403, body: 'Unauthorized', log: [ 'Middleware blocked: Unauthorized' ] }"
    },
    {
      "question": "Program 6: Saga Coordinator with step execution and compensation on failure",
      "code": "class SagaCoordinator {\n  constructor() {\n    this.steps = [];\n    this.executed = [];\n  }\n\n  step(name, action, compensate) {\n    this.steps.push({ name, action, compensate });\n    return this;\n  }\n\n  run(ctx) {\n    const results = [];\n    for (const s of this.steps) {\n      const result = s.action(ctx);\n      if (result.error) {\n        results.push({ step: s.name, status: 'FAILED', error: result.error });\n        // Compensate completed steps in reverse\n        const compensations = [];\n        for (const done of [...this.executed].reverse()) {\n          done.compensate(ctx);\n          compensations.push(done.name);\n        }\n        return { success: false, results, compensated: compensations };\n      }\n      ctx = { ...ctx, ...result.data };\n      this.executed.push(s);\n      results.push({ step: s.name, status: 'OK' });\n    }\n    return { success: true, results, finalContext: ctx };\n  }\n}\n\nconst saga = new SagaCoordinator()\n  .step('ReserveStock',\n    (ctx) => ({ data: { reservationId: 'R100' } }),\n    (ctx) => console.log('  Compensate: released stock'))\n  .step('ChargePayment',\n    (ctx) => ({ error: 'Card declined' }),  // simulate failure\n    (ctx) => console.log('  Compensate: refunded payment'))\n  .step('ShipOrder',\n    (ctx) => ({ data: { shipmentId: 'S200' } }),\n    (ctx) => console.log('  Compensate: cancelled shipment'));\n\nconst result = saga.run({ orderId: 'ORD-1', amount: 59.99 });\nconsole.log(JSON.stringify(result, null, 2));",
      "output": "  Compensate: released stock\n{\n  \"success\": false,\n  \"results\": [\n    { \"step\": \"ReserveStock\", \"status\": \"OK\" },\n    { \"step\": \"ChargePayment\", \"status\": \"FAILED\", \"error\": \"Card declined\" }\n  ],\n  \"compensated\": [ \"ReserveStock\" ]\n}"
    },
    {
      "question": "Program 7: Bulkhead pattern with isolated resource pools per service",
      "code": "class Bulkhead {\n  constructor(partitions) {\n    this.pools = {};\n    for (const [name, max] of Object.entries(partitions)) {\n      this.pools[name] = { max, active: 0, rejected: 0 };\n    }\n  }\n\n  acquire(service) {\n    const pool = this.pools[service];\n    if (!pool) return { allowed: false, reason: 'Unknown service' };\n    if (pool.active >= pool.max) {\n      pool.rejected++;\n      return { allowed: false, reason: `Pool full (${pool.active}/${pool.max})` };\n    }\n    pool.active++;\n    return { allowed: true, active: pool.active };\n  }\n\n  release(service) {\n    const pool = this.pools[service];\n    if (pool && pool.active > 0) pool.active--;\n  }\n\n  status() {\n    const result = {};\n    for (const [name, pool] of Object.entries(this.pools)) {\n      result[name] = { active: pool.active, max: pool.max, rejected: pool.rejected };\n    }\n    return result;\n  }\n}\n\nconst bh = new Bulkhead({ 'payment-svc': 3, 'inventory-svc': 5, 'email-svc': 2 });\n\n// Simulate overloading payment-svc\nfor (let i = 0; i < 5; i++) {\n  const r = bh.acquire('payment-svc');\n  console.log(`payment-svc acquire #${i + 1}:`, r.allowed ? 'OK' : r.reason);\n}\n\n// inventory-svc is unaffected\nconsole.log('inventory-svc acquire:', bh.acquire('inventory-svc'));\nconsole.log('\\nStatus:', JSON.stringify(bh.status()));",
      "output": "payment-svc acquire #1: OK\npayment-svc acquire #2: OK\npayment-svc acquire #3: OK\npayment-svc acquire #4: Pool full (3/3)\npayment-svc acquire #5: Pool full (3/3)\ninventory-svc acquire: { allowed: true, active: 1 }\n\nStatus: {\"payment-svc\":{\"active\":3,\"max\":3,\"rejected\":2},\"inventory-svc\":{\"active\":1,\"max\":5,\"rejected\":0},\"email-svc\":{\"active\":0,\"max\":2,\"rejected\":0}}"
    },
    {
      "question": "Program 8: Rate Limiter per service using token bucket algorithm",
      "code": "class PerServiceRateLimiter {\n  constructor() {\n    this.buckets = new Map();\n  }\n\n  configure(service, maxTokens, refillRate) {\n    this.buckets.set(service, {\n      tokens: maxTokens,\n      maxTokens,\n      refillRate, // tokens per second\n      lastRefill: Date.now(),\n    });\n  }\n\n  refill(bucket) {\n    const now = Date.now();\n    const elapsed = (now - bucket.lastRefill) / 1000;\n    bucket.tokens = Math.min(bucket.maxTokens, bucket.tokens + elapsed * bucket.refillRate);\n    bucket.lastRefill = now;\n  }\n\n  allow(service) {\n    const bucket = this.buckets.get(service);\n    if (!bucket) return { allowed: false, reason: 'No config' };\n    this.refill(bucket);\n    if (bucket.tokens >= 1) {\n      bucket.tokens--;\n      return { allowed: true, remaining: Math.floor(bucket.tokens) };\n    }\n    return { allowed: false, remaining: 0, retryAfter: Math.ceil(1 / bucket.refillRate) + 's' };\n  }\n\n  status() {\n    const result = {};\n    for (const [svc, b] of this.buckets) {\n      result[svc] = { tokens: Math.floor(b.tokens), max: b.maxTokens };\n    }\n    return result;\n  }\n}\n\nconst limiter = new PerServiceRateLimiter();\nlimiter.configure('auth-api', 5, 2);    // 5 burst, 2/sec refill\nlimiter.configure('search-api', 3, 1);  // 3 burst, 1/sec refill\n\n// Exhaust auth-api tokens\nfor (let i = 0; i < 7; i++) {\n  console.log(`auth-api #${i + 1}:`, limiter.allow('auth-api'));\n}\nconsole.log('search-api:', limiter.allow('search-api'));\nconsole.log('Status:', limiter.status());",
      "output": "auth-api #1: { allowed: true, remaining: 4 }\nauth-api #2: { allowed: true, remaining: 3 }\nauth-api #3: { allowed: true, remaining: 2 }\nauth-api #4: { allowed: true, remaining: 1 }\nauth-api #5: { allowed: true, remaining: 0 }\nauth-api #6: { allowed: false, remaining: 0, retryAfter: '1s' }\nauth-api #7: { allowed: false, remaining: 0, retryAfter: '1s' }\nsearch-api: { allowed: true, remaining: 2 }\nStatus: { 'auth-api': { tokens: 0, max: 5 }, 'search-api': { tokens: 2, max: 3 } }"
    },
    {
      "question": "Program 9: Distributed Config Store with versioning and change notification",
      "code": "class ConfigStore {\n  constructor() {\n    this.configs = new Map();\n    this.listeners = [];\n    this.version = 0;\n  }\n\n  set(key, value) {\n    this.version++;\n    const entry = { value, version: this.version, updatedAt: new Date().toISOString() };\n    this.configs.set(key, entry);\n    this.notify(key, value, this.version);\n    return entry;\n  }\n\n  get(key) {\n    return this.configs.get(key) || null;\n  }\n\n  getForService(servicePrefix) {\n    const result = {};\n    for (const [key, entry] of this.configs) {\n      if (key.startsWith(servicePrefix)) {\n        result[key] = entry.value;\n      }\n    }\n    return result;\n  }\n\n  onChange(callback) {\n    this.listeners.push(callback);\n  }\n\n  notify(key, value, version) {\n    for (const cb of this.listeners) {\n      cb({ key, value, version });\n    }\n  }\n}\n\nconst config = new ConfigStore();\nconst changes = [];\nconfig.onChange(e => changes.push(`v${e.version}: ${e.key}=${e.value}`));\n\nconfig.set('order-svc/timeout', 3000);\nconfig.set('order-svc/retries', 3);\nconfig.set('payment-svc/timeout', 5000);\nconfig.set('order-svc/timeout', 5000); // update\n\nconsole.log('order-svc config:', config.getForService('order-svc'));\nconsole.log('payment-svc timeout:', config.get('payment-svc/timeout'));\nconsole.log('Change log:', changes);",
      "output": "order-svc config: { 'order-svc/timeout': 5000, 'order-svc/retries': 3 }\npayment-svc timeout: { value: 5000, version: 3, updatedAt: '...' }\nChange log: [ 'v1: order-svc/timeout=3000', 'v2: order-svc/retries=3', 'v3: payment-svc/timeout=5000', 'v4: order-svc/timeout=5000' ]"
    },
    {
      "question": "Program 10: Request Correlation ID propagation across service calls",
      "code": "const crypto = require('crypto');\n\nclass CorrelationContext {\n  static generate() {\n    return 'req-' + crypto.randomBytes(4).toString('hex');\n  }\n}\n\nclass ServiceCall {\n  constructor(name) {\n    this.name = name;\n    this.logs = [];\n  }\n\n  handle(request) {\n    const correlationId = request.headers['x-correlation-id'] || CorrelationContext.generate();\n    this.logs.push({ service: this.name, correlationId, action: 'received' });\n\n    // Simulate downstream call — propagate correlation ID\n    const downstreamReq = {\n      headers: { 'x-correlation-id': correlationId },\n      from: this.name,\n    };\n    this.logs.push({ service: this.name, correlationId, action: 'calling-downstream' });\n    return { correlationId, downstream: downstreamReq, logs: this.logs };\n  }\n}\n\n// Simulate 3-service call chain\nconst gateway = new ServiceCall('api-gateway');\nconst orderSvc = new ServiceCall('order-service');\nconst paymentSvc = new ServiceCall('payment-service');\n\n// Start at gateway\nconst corrId = CorrelationContext.generate();\nconst r1 = gateway.handle({ headers: { 'x-correlation-id': corrId } });\nconst r2 = orderSvc.handle(r1.downstream);\nconst r3 = paymentSvc.handle(r2.downstream);\n\nconst allLogs = [...r1.logs, ...r2.logs, ...r3.logs];\nconsole.log('Correlation ID:', corrId);\nconsole.log('Trace:');\nallLogs.forEach(l => console.log(`  [${l.service}] ${l.action} (${l.correlationId})`));\nconsole.log('All same ID:', allLogs.every(l => l.correlationId === corrId));",
      "output": "Correlation ID: req-a1b2c3d4\nTrace:\n  [api-gateway] received (req-a1b2c3d4)\n  [api-gateway] calling-downstream (req-a1b2c3d4)\n  [order-service] received (req-a1b2c3d4)\n  [order-service] calling-downstream (req-a1b2c3d4)\n  [payment-service] received (req-a1b2c3d4)\n  [payment-service] calling-downstream (req-a1b2c3d4)\nAll same ID: true"
    }
  ]
}
